{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n## Objective: Improve baseline performance by injecting more regularization\n\n## Findings: High regularization improved baseline performance\n\n## Model Details\n* Backbone: MobileNetV3-Large (alpha=1.0, extract output from layer 142)\n* Output Regularization: L1 (1E-1)\n* Spatial Dropout: 15%, Regular Dropout = 15%\n* Total params: 203,289\n* Trainable params: 197,193\n* Non-trainable params: 6,096\n* Batch Size: 4\n* Training Epochs: 25\n\n## Preprocessing Details\n* Dataset: EyePACS-AIROGS-light\n* Image Dimension: 256x256\n* HFLIP: True\n* VFLIP: True\n* Rotation: False\n* Brightness: True (+/- 5%)\n* Noise Variance: 10.0\n\n## Best Model by Val Loss: Evaluation Metrics on Test Set\n* loss: 0.2676\n* binary_accuracy: 0.9210\n* auc: 0.9732\n* precision: 0.9136\n* recall: 0.9300\n\n## Epoch 25 Model: Evaluation Metrics on Test Set\n* loss: 0.2776\n* binary_accuracy: 0.9240\n* auc: 0.9704\n* precision: 0.9344\n* recall: 0.9120","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport cv2\nfrom keras.layers import SpatialDropout2D, Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, Conv2D, BatchNormalization, MaxPooling2D, Input, Concatenate, ReLU, AveragePooling2D, UpSampling2D\nfrom tensorflow.keras.applications import DenseNet201, InceptionResNetV2, MobileNetV2, EfficientNetB3, Xception, VGG19, InceptionV3, EfficientNetB0, EfficientNetB2, Xception\nfrom tensorflow.keras import regularizers, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.utils import Sequence\nfrom keras.models import Model\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import SGD, Adam\nimport keras\nimport numpy as np\nimport random\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2023-09-03T13:43:58.292895Z","iopub.execute_input":"2023-09-03T13:43:58.293264Z","iopub.status.idle":"2023-09-03T13:43:58.301053Z","shell.execute_reply.started":"2023-09-03T13:43:58.29323Z","shell.execute_reply":"2023-09-03T13:43:58.299802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a custom function that injects random noise into the image. Create unique generators using seeds and combine","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\ntarget = 256\n\n# inject noise but keep dark parts black\ndef addNoise(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY).astype(np.uint8)\n    ret, mask = cv2.threshold(gray, 5, 255, cv2.THRESH_BINARY)\n\n    randStd = random.uniform(0, 10.0) # 15\n    gaussian = np.random.normal(randStd*-1, randStd, (target, target,3))\n    noisy_image = image + gaussian\n    image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n\n    image[mask == 0] = [0,0,0]\n    image = preprocess_input(image)\n    return image\n\ndataPath = '/kaggle/input/eyepacs-airogs-light/release-crop/release-crop/'\n\n# combine two unique generators using noise injection\nbatchSize = 4\ntrainDataGen = ImageDataGenerator(preprocessing_function=addNoise, horizontal_flip=True,vertical_flip=True,rotation_range=0,brightness_range=(0.95, 1.05))\ntrainGen1 = trainDataGen.flow_from_directory(batch_size = batchSize, shuffle=True,  class_mode=\"binary\", target_size=(target, target), directory=dataPath + 'train', color_mode='rgb', seed=0)\ntrainGen2 = trainDataGen.flow_from_directory(batch_size = batchSize, shuffle=True,  class_mode=\"binary\", target_size=(target, target), directory=dataPath + 'train', color_mode='rgb', seed=1)\n\ndef combine_gen(*gens):\n    while True:\n        for g in gens:\n            yield next(g)\n\ntrainGen = combine_gen(trainGen1, trainGen2)\n\nvalDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\nvalGen = valDataGen.flow_from_directory(batch_size = 1, class_mode=\"binary\", target_size=(target, target), directory=dataPath + 'validation', color_mode='rgb')\n\ntestDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntestGen = testDataGen.flow_from_directory(batch_size = 1, class_mode=\"binary\", target_size=(target, target), directory=dataPath + 'test', color_mode='rgb')","metadata":{"execution":{"iopub.status.busy":"2023-09-03T13:43:29.800728Z","iopub.execute_input":"2023-09-03T13:43:29.801677Z","iopub.status.idle":"2023-09-03T13:43:35.074423Z","shell.execute_reply.started":"2023-09-03T13:43:29.801632Z","shell.execute_reply":"2023-09-03T13:43:35.073519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"from keras.layers.pooling.global_max_pooling2d import GlobalMaxPool2D\n\n# simple model that uses mobilenet background\ndef getModel(image_size, num_classes):\n    model_input = keras.Input(shape=(image_size, image_size, 3))\n    \n    transfer = keras.applications.MobileNetV3Large(\n        weights='imagenet', include_top=False, input_tensor=model_input\n    )\n    x = transfer.get_layer(index=142).output\n    \n    x = SpatialDropout2D(0.15)(x)\n    x = Conv2D(filters=64, kernel_size=1, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-1))(x)\n    x = GlobalMaxPool2D()(x)\n    x = Dropout(0.15)(x)\n\n    model_output = Dense(1, activation='sigmoid') (x)\n\n    return keras.Model(inputs=model_input, outputs=model_output)\n\nmodel = getModel(image_size=target, num_classes=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=2, min_lr=1e-5) # factor=0.85\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath='val-best.h5',\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-03T13:44:12.070262Z","iopub.execute_input":"2023-09-03T13:44:12.071286Z","iopub.status.idle":"2023-09-03T13:44:13.686715Z","shell.execute_reply.started":"2023-09-03T13:44:12.07124Z","shell.execute_reply":"2023-09-03T13:44:13.685721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:55:28.919956Z","iopub.execute_input":"2023-09-03T14:55:28.920371Z","iopub.status.idle":"2023-09-03T14:55:29.216054Z","shell.execute_reply.started":"2023-09-03T14:55:28.920325Z","shell.execute_reply":"2023-09-03T14:55:29.215281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['binary_accuracy',keras.metrics.AUC(),keras.metrics.Precision(), keras.metrics.Recall()])\nhistory = model.fit(trainGen, steps_per_epoch = len(trainGen1)*2,validation_data=valGen, validation_steps=len(valGen), epochs=25, callbacks=[reduce_lr, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-09-03T13:44:13.68895Z","iopub.execute_input":"2023-09-03T13:44:13.689303Z","iopub.status.idle":"2023-09-03T14:48:40.648789Z","shell.execute_reply.started":"2023-09-03T13:44:13.689271Z","shell.execute_reply":"2023-09-03T14:48:40.647802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model Loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:48:40.652076Z","iopub.execute_input":"2023-09-03T14:48:40.652502Z","iopub.status.idle":"2023-09-03T14:48:40.889185Z","shell.execute_reply.started":"2023-09-03T14:48:40.652471Z","shell.execute_reply":"2023-09-03T14:48:40.888265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"binary_accuracy\"])\nplt.plot(history.history[\"val_binary_accuracy\"])\nplt.title(\"Model Accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epoch\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:48:40.890418Z","iopub.execute_input":"2023-09-03T14:48:40.890859Z","iopub.status.idle":"2023-09-03T14:48:41.139933Z","shell.execute_reply.started":"2023-09-03T14:48:40.890825Z","shell.execute_reply":"2023-09-03T14:48:41.138972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(testGen)","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:48:41.142248Z","iopub.execute_input":"2023-09-03T14:48:41.142682Z","iopub.status.idle":"2023-09-03T14:48:52.60354Z","shell.execute_reply.started":"2023-09-03T14:48:41.142648Z","shell.execute_reply":"2023-09-03T14:48:52.602649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('/kaggle/working/val-best.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:48:52.606806Z","iopub.execute_input":"2023-09-03T14:48:52.607097Z","iopub.status.idle":"2023-09-03T14:48:52.779254Z","shell.execute_reply.started":"2023-09-03T14:48:52.607065Z","shell.execute_reply":"2023-09-03T14:48:52.778333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(testGen)","metadata":{"execution":{"iopub.status.busy":"2023-09-03T14:48:52.780708Z","iopub.execute_input":"2023-09-03T14:48:52.781042Z","iopub.status.idle":"2023-09-03T14:49:01.628372Z","shell.execute_reply.started":"2023-09-03T14:48:52.781009Z","shell.execute_reply":"2023-09-03T14:49:01.627259Z"},"trusted":true},"execution_count":null,"outputs":[]}]}