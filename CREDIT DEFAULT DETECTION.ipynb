{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53332,"databundleVersionId":5936090,"sourceType":"competition"}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom IPython.display import clear_output\n\nfrom six.moves import urllib\nimport tensorflow.compat.v2.feature_column as fc\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"e823aa20-6261-4bc4-a9ee-84213ce4f395","_cell_guid":"0c136a19-1b55-40c2-9c5a-b68d4cee17ec","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:10.956783Z","iopub.execute_input":"2023-06-09T20:46:10.957209Z","iopub.status.idle":"2023-06-09T20:46:10.963892Z","shell.execute_reply.started":"2023-06-09T20:46:10.957174Z","shell.execute_reply":"2023-06-09T20:46:10.962596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### READ THE TRAIN AND TEST DATASET","metadata":{"_uuid":"fce4fb5b-ddc5-4a1a-ab8e-f739bef40786","_cell_guid":"84c1c240-3368-4cc3-b3de-4a51a5406cfb","trusted":true}},{"cell_type":"code","source":"test_fp = '/kaggle/input/ncair-2023-ai-competition/test.csv' \ntrain_fp = '/kaggle/input/ncair-2023-ai-competition/train.csv'\n\n#LAODING THE DATASET FOR TRAINING AND TESTING\ndf_train = pd.read_csv(train_fp,index_col='BVN') # training data\ndf_test = pd.read_csv(test_fp,index_col='BVN') # testing data","metadata":{"_uuid":"7f550d7c-902e-4938-a5e6-cf3915c0508d","_cell_guid":"f1ec4a2a-8f3b-47f6-a6b3-c023da9073ef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:10.965818Z","iopub.execute_input":"2023-06-09T20:46:10.966246Z","iopub.status.idle":"2023-06-09T20:46:11.060021Z","shell.execute_reply.started":"2023-06-09T20:46:10.966216Z","shell.execute_reply":"2023-06-09T20:46:11.058890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 2:** Check the general info about your data\n\n- This will also tell you the number of data entries you have\n\n- Use the information to note down the categorical columns (features), you might need it later","metadata":{"_uuid":"8440a601-751f-4927-82fc-2973b8ae1bdb","_cell_guid":"454986f2-25fa-4862-a4d1-8ddfb84aa650","trusted":true}},{"cell_type":"code","source":"df_train.info()","metadata":{"_uuid":"d63167e0-d877-441e-baa0-19b55ded2b41","_cell_guid":"28c319ee-a95f-4d9c-8490-ea0155298458","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.062128Z","iopub.execute_input":"2023-06-09T20:46:11.062682Z","iopub.status.idle":"2023-06-09T20:46:11.091359Z","shell.execute_reply.started":"2023-06-09T20:46:11.062643Z","shell.execute_reply":"2023-06-09T20:46:11.090041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECK THE STATS OF THE DATASET","metadata":{"_uuid":"5f156a9c-9717-48b7-a3ba-e3622e8ad758","_cell_guid":"ecb76f47-1212-4597-91c9-e976556233c3","trusted":true}},{"cell_type":"code","source":"df_train.describe()","metadata":{"_uuid":"645149ac-1bfa-4670-894b-f3054b0f2503","_cell_guid":"2f7250cc-f2b1-4f26-8b37-9733f91356a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.094916Z","iopub.execute_input":"2023-06-09T20:46:11.095265Z","iopub.status.idle":"2023-06-09T20:46:11.178847Z","shell.execute_reply.started":"2023-06-09T20:46:11.095238Z","shell.execute_reply":"2023-06-09T20:46:11.177532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WORK ON EDUCTION MISSING VALUE","metadata":{"_uuid":"fd04c82e-97a5-483e-9bab-315e8e97e457","_cell_guid":"f2d28c6d-1fd0-400c-b72e-508baa94ac16","trusted":true}},{"cell_type":"code","source":"# Calculate the mode of the column\nmode_value = df_train[\"EDUCATION\"].mode()[0]\n# Replace missing values with the mode\ndf_train['EDUCATION'].fillna(mode_value, inplace=True)\n\n# Calculate the mode of the column\nmode_value = df_test[\"EDUCATION\"].mode()[0]\n# Replace missing values with the mode\ndf_train['EDUCATION'].fillna(mode_value, inplace=True)\n","metadata":{"_uuid":"d1315146-ad61-4475-b791-d21a449a2589","_cell_guid":"013139bf-4df6-47b1-b46f-2c0db87366c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.181555Z","iopub.execute_input":"2023-06-09T20:46:11.182029Z","iopub.status.idle":"2023-06-09T20:46:11.193497Z","shell.execute_reply.started":"2023-06-09T20:46:11.181987Z","shell.execute_reply":"2023-06-09T20:46:11.192062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECK FOR  ZEROS IN CATEGORICAL COLUMNS AND REPLACE WITH THE MODE","metadata":{"_uuid":"12cb6992-4293-48bc-b870-707bcb192350","_cell_guid":"46acf311-69d0-40d2-bda8-fbb380cd5484","trusted":true}},{"cell_type":"code","source":"#CHECK FOR ZEROS IN CATEGORICAL COLUMNS\ndf_train['MARRIAGE'].value_counts()","metadata":{"_uuid":"f84de004-88b5-447f-92b9-049e754e6421","_cell_guid":"8ae59bb3-437e-43bf-a7c5-962b91bc1619","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.194836Z","iopub.execute_input":"2023-06-09T20:46:11.195172Z","iopub.status.idle":"2023-06-09T20:46:11.209612Z","shell.execute_reply.started":"2023-06-09T20:46:11.195144Z","shell.execute_reply":"2023-06-09T20:46:11.208517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of categorical columns\ncategorical_columns = ['SEX', 'MARRIAGE', 'EDUCATION', 'Nationality']\n\n# Iterate over the categorical columns\nfor col in categorical_columns:\n    # Find rows with zero values\n    zero_rows = df_train[col] == 0\n    zero_rows_t = df_test[col] == 0\n    \n    # Calculate the mode of the column\n    mode_value = df_train[col].mode()[0]\n    mode_value_t = df_test[col].mode()[0]\n    \n    \n    # Replace zero values with the mode\n    df_train.loc[zero_rows, col] = mode_value\n    df_test.loc[zero_rows_t, col] = mode_value_t\n\n\n\n# Print the updated DataFrame\ndf_train.head()","metadata":{"_uuid":"35f23109-8bc7-4cfa-82d2-6a3ba15dd031","_cell_guid":"cf9ed54d-d0f8-439c-9c5b-1242dde64379","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.210984Z","iopub.execute_input":"2023-06-09T20:46:11.211356Z","iopub.status.idle":"2023-06-09T20:46:11.254670Z","shell.execute_reply.started":"2023-06-09T20:46:11.211327Z","shell.execute_reply":"2023-06-09T20:46:11.253749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = df_train['MARRIAGE'].mode()[0]\ndf_train['MARRIAGE'] = df_train['MARRIAGE'].replace('0', mode)\n\nmode = df_test['MARRIAGE'].mode()[0]\ndf_test['MARRIAGE'] = df_test['MARRIAGE'].replace('0', mode)","metadata":{"_uuid":"2adaa8df-06fe-47af-88f1-697e3e05b2d5","_cell_guid":"5c0e05c8-2525-4240-974e-d6a726dcdee4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.256009Z","iopub.execute_input":"2023-06-09T20:46:11.256595Z","iopub.status.idle":"2023-06-09T20:46:11.267337Z","shell.execute_reply.started":"2023-06-09T20:46:11.256565Z","shell.execute_reply":"2023-06-09T20:46:11.266059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECK FOR ZEROS IN CATEGORICAL DATA","metadata":{"_uuid":"ceefd944-6a83-4719-901a-f93fafb01809","_cell_guid":"f3e60e7b-8b9f-4ffa-b423-a1e4ed137e43","trusted":true}},{"cell_type":"code","source":"f= ['SEX', 'MARRIAGE', 'EDUCATION', 'Nationality']\ndf_train.index.value_counts()","metadata":{"_uuid":"d4d7ba6b-f69f-4722-828c-8e60c48c4f28","_cell_guid":"973df639-d55c-4686-a88b-33e37cb797a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.499610Z","iopub.execute_input":"2023-06-09T20:46:11.500715Z","iopub.status.idle":"2023-06-09T20:46:11.512108Z","shell.execute_reply.started":"2023-06-09T20:46:11.500672Z","shell.execute_reply":"2023-06-09T20:46:11.510902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"61b18dd7-0955-40c3-859b-7fac63c3b4c5","_cell_guid":"78243e6f-cffd-4179-984a-a75c8fe328bd","trusted":true}},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"_uuid":"be300d60-84a9-452b-959d-ff9d3fcba837","_cell_guid":"19b9c74d-7431-464d-bf99-701e85a8e96f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.513796Z","iopub.execute_input":"2023-06-09T20:46:11.514651Z","iopub.status.idle":"2023-06-09T20:46:11.536782Z","shell.execute_reply.started":"2023-06-09T20:46:11.514620Z","shell.execute_reply":"2023-06-09T20:46:11.535500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING THE DOB COLUMN","metadata":{"_uuid":"22ac23ad-5ba6-42c7-a1de-ff6e146e987d","_cell_guid":"bfebe931-6942-4ed5-b8cf-0b0303ef7fcf","trusted":true}},{"cell_type":"code","source":"from datetime import datetime\n\ncurrent_year = datetime.now().year\ndf_train['DOB'] = pd.to_datetime(df_train['DOB'], format='%m/%d/%Y', errors='coerce')\ndf_train['Age'] = current_year - df_train['DOB'].dt.year\n\nmean = df_train['Age'].mean()\ndf_train['Age'].fillna(mean, inplace=True)\ndf_train['Age'].fillna(0, inplace=True)  # Fill remaining NaN values with 0\n\ndf_train.drop('DOB', axis=1, inplace=True)\n\n\n#############################TEST##################################\nfrom datetime import datetime\n\ncurrent_year = datetime.now().year\ndf_test['DOB'] = pd.to_datetime(df_test['DOB'], format='%m/%d/%Y',errors='coerce')\ndf_test['Age'] = current_year - df_test['DOB'].dt.year\n\nmean = df_test['Age'].mean()\ndf_test['Age'].fillna(mean, inplace=True)\ndf_test['Age'].fillna(0, inplace=True)  # Fill remaining NaN values with 0\n\ndf_test.drop('DOB', axis=1, inplace=True)\n\n\n","metadata":{"_uuid":"1163b392-4b0d-4fa5-8ab7-f2fede5efc69","_cell_guid":"f3c07e3c-d09c-4680-91eb-b53df05d5117","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:11.537943Z","iopub.execute_input":"2023-06-09T20:46:11.538475Z","iopub.status.idle":"2023-06-09T20:46:11.627076Z","shell.execute_reply.started":"2023-06-09T20:46:11.538382Z","shell.execute_reply":"2023-06-09T20:46:11.626148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ad8a9531-c444-4315-8f1a-72cf5d5a54b2","_cell_guid":"af1e4870-3df8-46b3-8f48-d9633b11b98c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECKING NORMAL DISTRIBUTION","metadata":{"_uuid":"2b4c1feb-ae93-4fed-9316-45d84731db98","_cell_guid":"d2161dd4-79a6-4bbc-97dd-c064fbf04a90","trusted":true}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a distribution plot of all columns\nfig, axes = plt.subplots(nrows=len(df_train.columns), figsize=(10, 50))\nfor i, column in enumerate(df_train.columns):\n    sns.histplot(data=df_train[column], ax=axes[i], kde=True)\n    axes[i].set_title(column)\n    axes[i].set_xlabel('')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"f531f4e3-6503-445e-b48f-364046912a14","_cell_guid":"1a656b86-7b06-4a89-8f19-f451fa830ba8","execution":{"iopub.status.busy":"2023-06-09T20:46:11.629194Z","iopub.execute_input":"2023-06-09T20:46:11.629712Z","iopub.status.idle":"2023-06-09T20:46:47.017946Z","shell.execute_reply.started":"2023-06-09T20:46:11.629685Z","shell.execute_reply":"2023-06-09T20:46:47.016807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SUMING TO CHECK IF SIMILAR INFO EXISTS","metadata":{"_uuid":"b25f13cd-8e5a-4ddb-93c1-7f01d280897d","_cell_guid":"7b709124-b489-441a-8713-904f8f9ff3f3","trusted":true}},{"cell_type":"code","source":"df_train['Total_Bill_Amount'] = df_train[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)\ndf_train['Total_PAY'] = df_train[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].sum(axis=1)\ndf_train['Total_PAY_AMT'] = df_train[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].sum(axis=1)\ndf_train['Average_PAY'] = df_train['Total_PAY']/6\ndf_train['Average_Bill_Amount']=df_train['Total_Bill_Amount'] /6\ndf_train['Average_PAY_AMT']=df_train['Total_PAY_AMT'] /6\n\n\n\n#TEST\ndf_test['Total_Bill_Amount'] = df_test[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)\ndf_test['Total_PAY'] = df_test[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].sum(axis=1)\ndf_test['Total_PAY_AMT'] = df_test[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].sum(axis=1)\ndf_test['Average_PAY'] = df_test['Total_PAY']/6\ndf_test['Average_Bill_Amount']=df_test['Total_Bill_Amount'] /6\ndf_test['Average_PAY_AMT']=df_test['Total_PAY_AMT'] /6","metadata":{"_uuid":"69fae77c-e40c-4668-9cdb-af91ab8f778b","_cell_guid":"1d62a2c4-e081-4646-97cf-fafb30296e05","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:47.019865Z","iopub.execute_input":"2023-06-09T20:46:47.020341Z","iopub.status.idle":"2023-06-09T20:46:47.057355Z","shell.execute_reply.started":"2023-06-09T20:46:47.020297Z","shell.execute_reply":"2023-06-09T20:46:47.056014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING SKEWED COLUMNS","metadata":{"_uuid":"a9e86eed-8cd5-4e03-b775-2928c24da5c3","_cell_guid":"c0c54042-ef83-4ae5-b2d1-8c83452a3b9b","trusted":true}},{"cell_type":"code","source":"\n\n#USING LOG TRANSFORMS\n# Define the columns to transform\ncolumns_to_transform = ['LIMIT_BAL','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n\n# Create a copy of the original DataFrame\ntransformed_data = df_train.copy()\n\n# Apply log transformation to each column\nfor column in columns_to_transform:\n    transformed_column = np.log1p(df_train[column])\n    transformed_data[column] = transformed_column\n\n# Replace the original columns with the transformed columns in the original DataFrame\ndf_train[columns_to_transform] = transformed_data[columns_to_transform]\n\n\n\n##################TEST###############\n#USING LOG TRANSFORMS\n# Define the columns to transform\ncolumns_to_transform = ['LIMIT_BAL','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n\n# Create a copy of the original DataFrame\ntransformed_data = df_test.copy()\n\n# Apply log transformation to each column\nfor column in columns_to_transform:\n    transformed_column = np.log1p(df_test[column])\n    transformed_data[column] = transformed_column\n\n# Replace the original columns with the transformed columns in the original DataFrame\ndf_test[columns_to_transform] = transformed_data[columns_to_transform]\n\n\n\n#df_train['Total_Bill_Amount'] = df_train[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)\n\n# import scipy.stats as stats\n\n# # Shift the values to make them positive\n# shift_constant = abs(df_train[['Total_PAY_AMT', 'Total_Bill_Amount']].min().min()) + 1  # Add 1 to avoid zero values\n# shifted_total_pay_amt = df_train['Total_PAY_AMT'] + shift_constant\n# shifted_total_bill_amt = df_train['Total_Bill_Amount'] + shift_constant\n\n# # Apply Box-Cox transformation\n# transformed_total_pay_amt, _ = stats.boxcox(shifted_total_pay_amt)\n# transformed_total_bill_amt, _ = stats.boxcox(shifted_total_bill_amt)\n\n# # Replace the original columns with the transformed columns in the DataFrame\n# df_train['Total_PAY_AMT'] = transformed_total_pay_amt\n# df_train['Total_Bill_Amount'] = transformed_total_bill_amt","metadata":{"_uuid":"6aeab961-24c1-4d3c-92a1-2cfa3d575d05","_cell_guid":"8aeafef6-fe2b-4523-852e-29b2371897bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:47.059098Z","iopub.execute_input":"2023-06-09T20:46:47.060025Z","iopub.status.idle":"2023-06-09T20:46:47.100970Z","shell.execute_reply.started":"2023-06-09T20:46:47.059985Z","shell.execute_reply":"2023-06-09T20:46:47.099822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a distribution plot of all columns\nfig, axes = plt.subplots(nrows=len(df_train.columns), figsize=(10, 50))\nfor i, column in enumerate(df_train.columns):\n    sns.histplot(data=df_train[column], ax=axes[i], kde=True)\n    axes[i].set_title(column)\n    axes[i].set_xlabel('')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"a8c64ee1-6dfd-4798-8beb-71ee0c586522","_cell_guid":"99002301-eaeb-4004-b18e-12b1c66d66dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:46:47.102559Z","iopub.execute_input":"2023-06-09T20:46:47.103341Z","iopub.status.idle":"2023-06-09T20:47:07.678118Z","shell.execute_reply.started":"2023-06-09T20:46:47.103299Z","shell.execute_reply":"2023-06-09T20:47:07.676890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"_uuid":"14502432-6e4d-432f-8a08-793f0f823eee","_cell_guid":"a0ce45e7-77bc-4849-b25d-31db24e5ccac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:07.679792Z","iopub.execute_input":"2023-06-09T20:47:07.680233Z","iopub.status.idle":"2023-06-09T20:47:07.709378Z","shell.execute_reply.started":"2023-06-09T20:47:07.680192Z","shell.execute_reply":"2023-06-09T20:47:07.708155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a box plot of all columns\ndf_train.boxplot(figsize=(12, 8))\nplt.title('Box Plot of Columns')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"_uuid":"228247a8-041d-47a4-941c-71bbab74dfce","_cell_guid":"46cfd520-7225-4817-9f18-22e781be7364","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:07.711184Z","iopub.execute_input":"2023-06-09T20:47:07.711703Z","iopub.status.idle":"2023-06-09T20:47:08.631128Z","shell.execute_reply.started":"2023-06-09T20:47:07.711660Z","shell.execute_reply":"2023-06-09T20:47:08.629975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"_uuid":"b8ece8c4-2d80-4490-8fd1-978f40975b60","_cell_guid":"00b9acdc-4449-42bf-8297-b36b0a3e8e8d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:08.634675Z","iopub.execute_input":"2023-06-09T20:47:08.635028Z","iopub.status.idle":"2023-06-09T20:47:08.738611Z","shell.execute_reply.started":"2023-06-09T20:47:08.634998Z","shell.execute_reply":"2023-06-09T20:47:08.737490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ENCODE CATEGORICAL DATA","metadata":{"_uuid":"462fe814-8bc8-4e3e-9b74-faec203c77e1","_cell_guid":"41b66ac8-e072-4a55-8894-98e37913e70c","trusted":true}},{"cell_type":"code","source":"categorical_columns = ['SEX', 'MARRIAGE', 'EDUCATION', 'Nationality']\n\nlist(df_train.EDUCATION.unique())","metadata":{"_uuid":"6371a79f-1b19-4c0f-b527-ebed5aa5b3ba","_cell_guid":"7936e66f-40aa-4968-8c63-1901779796d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:08.740299Z","iopub.execute_input":"2023-06-09T20:47:08.740975Z","iopub.status.idle":"2023-06-09T20:47:08.749024Z","shell.execute_reply.started":"2023-06-09T20:47:08.740936Z","shell.execute_reply":"2023-06-09T20:47:08.748183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PERFORM ONEHOT FOR SEX\n# Perform one-hot encoding\nencoded_df = pd.get_dummies(df_train['SEX'], prefix='SEX')\n\n# Concatenate the encoded columns with the original DataFrame\ndf_train = pd.concat([df_train, encoded_df], axis=1)\n\n# Drop the original 'SEX' column\ndf_train.drop('SEX', axis=1, inplace=True)\n\n\n\n\n#PERFORM ORDINAL ENCODING TO MARRIAGE\nimport pandas as pd\n# Define the mapping of categories to numerical values\nmapping = {'Single': 0, 'Divorce': 1, 'Married': 2}\n# Perform ordinal encoding\ndf_train['MARRIAGE'] = df_train['MARRIAGE'].replace(mapping)\n\n##############################################################################\n\n#PERFORM ORDINAL ENCODING TO EDUCATION\neducation_mapping = {\n    'Uneducated': 0,\n    'Undergraduate': 1,\n    'Graduate': 2,\n    'Masters': 3,\n    'Doctorate': 4\n}\n\n# Perform ordinal encoding\ndf_train['EDUCATION'] = df_train['EDUCATION'].replace(education_mapping)\n\n#####################################################################################\n\n\n#PERFORM ORDINAL ENCODING TO  NATIONALITY\nnationality_mapping={'Nigeria':0}\n\ndf_train['Nationality'] = df_train['Nationality'].replace(nationality_mapping)\n\n\n\n\n\n\n################TEST\n\n\n# PERFORM ONEHOT FOR SEX\n# Perform one-hot encoding\nencoded_df = pd.get_dummies(df_test['SEX'], prefix='SEX')\n\n# Concatenate the encoded columns with the original DataFrame\ndf_test = pd.concat([df_test, encoded_df], axis=1)\n\n# Drop the original 'SEX' column\ndf_test.drop('SEX', axis=1, inplace=True)\n\n\n\n#PERFORM ORDINAL ENCODING TO MARRIAGE\nimport pandas as pd\n# Define the mapping of categories to numerical values\nmapping = {'Single': 0, 'Divorce': 1, 'Married': 2}\n# Perform ordinal encoding\ndf_test['MARRIAGE'] = df_test['MARRIAGE'].replace(mapping)\n\n##############################################################################\n\n#PERFORM ORDINAL ENCODING TO EDUCATION\neducation_mapping = {\n    'Uneducated': 0,\n    'Undergraduate': 1,\n    'Graduate': 2,\n    'Masters': 3,\n    'Doctorate': 4\n}\n\n# Perform ordinal encoding\ndf_test['EDUCATION'] = df_test['EDUCATION'].replace(education_mapping)\n\n#####################################################################################\n\n\n#PERFORM ORDINAL ENCODING TO  NATIONALITY\nnationality_mapping={'Nigeria':0}\n\ndf_test['Nationality'] = df_test['Nationality'].replace(nationality_mapping)\n\n","metadata":{"_uuid":"74d30772-1e2f-45ef-b204-176b154ea519","_cell_guid":"30c2ed57-39c5-4de3-be15-d0271ef7c818","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:08.750526Z","iopub.execute_input":"2023-06-09T20:47:08.751182Z","iopub.status.idle":"2023-06-09T20:47:08.810900Z","shell.execute_reply.started":"2023-06-09T20:47:08.751130Z","shell.execute_reply":"2023-06-09T20:47:08.809803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"_uuid":"263bab80-f78e-4c7d-bf88-7045f0755491","_cell_guid":"b1bfe85f-e94e-44d9-bf1e-7233fce8d135","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:08.812250Z","iopub.execute_input":"2023-06-09T20:47:08.812613Z","iopub.status.idle":"2023-06-09T20:47:08.836484Z","shell.execute_reply.started":"2023-06-09T20:47:08.812583Z","shell.execute_reply":"2023-06-09T20:47:08.835141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECKING FOR MUTUAL INFORMATION","metadata":{"_uuid":"95ec1ef4-35b1-4b15-a44e-e315843bd90c","_cell_guid":"c435b03b-ea28-47c5-8b37-af220093e371","trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\nX = df_train.drop('default', axis=1)  # Drop the target variable from the feature matrix\ny = df_train['default']  # Target variable\n\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y)","metadata":{"_uuid":"08b7bdae-94e4-46c2-a691-e46a3d92662b","_cell_guid":"35fadd38-2a31-48b3-a8da-52740f68d22c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:08.837855Z","iopub.execute_input":"2023-06-09T20:47:08.838269Z","iopub.status.idle":"2023-06-09T20:47:12.368367Z","shell.execute_reply.started":"2023-06-09T20:47:08.838239Z","shell.execute_reply":"2023-06-09T20:47:12.367449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_scores","metadata":{"_uuid":"4a3dad09-5243-4f40-8acd-b69bccb78020","_cell_guid":"354c52f0-9cb8-4a37-9555-1e41b51080cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:12.371538Z","iopub.execute_input":"2023-06-09T20:47:12.372541Z","iopub.status.idle":"2023-06-09T20:47:12.381580Z","shell.execute_reply.started":"2023-06-09T20:47:12.372508Z","shell.execute_reply":"2023-06-09T20:47:12.380500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df_train contains your dataset\ncolumns_to_drop = ['Nationality','SEX_Male','SEX_Female','MARRIAGE']\n\n# Drop the specified columns from the dataframe\ndf_train.drop(columns_to_drop, axis=1, inplace=True)\ndf_test.drop(columns_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:12.383410Z","iopub.execute_input":"2023-06-09T20:47:12.384246Z","iopub.status.idle":"2023-06-09T20:47:12.395552Z","shell.execute_reply.started":"2023-06-09T20:47:12.384209Z","shell.execute_reply":"2023-06-09T20:47:12.394547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### USING K MEANS TO CLUSTER","metadata":{"_uuid":"faecd9b5-c885-4118-a9dc-4342ba9a20e3","_cell_guid":"87ca1a23-cb5b-46b3-86af-3177c292b9ac","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\n\n# Assuming df_train contains your dataset with the features mentioned\nfeatures = ['Total_PAY', 'PAY_1', 'PAY_5', 'PAY_3', 'PAY_2', 'PAY_4', 'PAY_AMT3', 'PAY_6', 'Total_PAY_AMT', 'PAY_AMT1',\n            'PAY_AMT2', 'PAY_AMT6', 'BILL_AMT5', 'PAY_AMT4', 'PAY_AMT5', 'BILL_AMT1', 'Total_Bill_Amount', 'BILL_AMT6',\n            'BILL_AMT4', 'LIMIT_BAL', 'BILL_AMT2', 'BILL_AMT3']\n\n# Create a subset of the dataframe with the selected features\nX = df_train[features]\nG = df_test[features]\n\n# Specify the number of clusters (K)\nnum_clusters = 5\n\n# Create an instance of the KMeans class\nkmeans = KMeans(n_clusters=num_clusters, random_state=0)\nkmeans_t = KMeans(n_clusters=num_clusters, random_state=0)\n\n# Fit the K-means model to the data\nkmeans.fit(X)\nkmeans_t.fit(G)\n\n# Get the cluster labels assigned to each data point\ncluster_labels = kmeans.labels_\ncluster_labels_t = kmeans_t.labels_\n\n# Add the cluster labels to the original dataframe\ndf_train['Cluster'] = cluster_labels\ndf_test['Cluster'] = cluster_labels_t\n\n# Create a scatter plot using relplot\nsns.relplot(data=df_train, x='Total_PAY', y='PAY_1', hue='Cluster')\nplt.xlabel('Total_PAY')\nplt.ylabel('PAY_1')\nplt.title('K-means Clustering')\nplt.show()","metadata":{"_uuid":"87ed15b7-6264-4353-86a6-694fee44948a","_cell_guid":"256e2e34-c61c-4a85-89cb-1ab4e354046a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:12.396877Z","iopub.execute_input":"2023-06-09T20:47:12.397431Z","iopub.status.idle":"2023-06-09T20:47:17.401429Z","shell.execute_reply.started":"2023-06-09T20:47:12.397399Z","shell.execute_reply":"2023-06-09T20:47:17.400332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SPLITTING AND STANDARDIZATION","metadata":{"_uuid":"de568ee3-8442-4700-ada4-21038d4a5a68","_cell_guid":"3623dc4d-c229-4b36-b0db-b7b326b87a7f","trusted":true}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#SPLIT INTO X AND Y\ny=df_train['default']\nx=df_train.drop(columns=['default'],axis=1,inplace=True)","metadata":{"_uuid":"a8010321-4b74-41bf-9fbb-28f49ab65cd2","_cell_guid":"4ef8b720-3736-406d-90ab-bbff4fdbe048","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:17.403001Z","iopub.execute_input":"2023-06-09T20:47:17.403883Z","iopub.status.idle":"2023-06-09T20:47:17.411594Z","shell.execute_reply.started":"2023-06-09T20:47:17.403849Z","shell.execute_reply":"2023-06-09T20:47:17.410417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list(df_train.columns))\nlen(list(df_test.columns))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:17.413389Z","iopub.execute_input":"2023-06-09T20:47:17.413855Z","iopub.status.idle":"2023-06-09T20:47:17.427761Z","shell.execute_reply.started":"2023-06-09T20:47:17.413813Z","shell.execute_reply":"2023-06-09T20:47:17.426668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Exclude the 'index' column from the DataFrame\ndf_train_without_index = df_train.reset_index(drop=True)\nBVN=df_test.index\ndf_test_without_index = df_test.reset_index(drop=True)\n\n# Apply SimpleImputer to handle missing values\nimputer = SimpleImputer()\nimputed_data = imputer.fit_transform(df_train_without_index)\nimputed_data_t = imputer.fit_transform(df_test_without_index)\n\n\n\n# Apply StandardScaler to the imputed data\nscaler = StandardScaler()\nx_scaled = pd.DataFrame(scaler.fit_transform(imputed_data), columns=df_train_without_index.columns)\nx_scaled_t = pd.DataFrame(scaler.fit_transform(imputed_data_t), columns=df_test_without_index.columns)","metadata":{"_uuid":"7a7f66c2-dadb-4ee3-99e8-25abca73a9f8","_cell_guid":"d8e6b807-145e-4840-81df-c4d3d6281e6f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:17.429191Z","iopub.execute_input":"2023-06-09T20:47:17.429964Z","iopub.status.idle":"2023-06-09T20:47:17.480907Z","shell.execute_reply.started":"2023-06-09T20:47:17.429931Z","shell.execute_reply":"2023-06-09T20:47:17.480054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import check_array\n\n# Assuming you have your features (X) and target variable (y) DataFrame\n\n# Validate and reshape input data\nX = check_array(x_scaled, ensure_2d=True)\ny = check_array(y, ensure_2d=False)\n\n# Create an instance of SMOTE\nsmote = SMOTE()\n\n# Perform oversampling on the dataset\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Convert the resampled arrays back to DataFrames\nX_resampled = pd.DataFrame(X_resampled, columns=df_train_without_index.columns)\ny_resampled = pd.DataFrame(y_resampled, columns=['default'])\n\n# Check the class distribution after oversampling\nclass_counts = y_resampled['default'].value_counts()\nprint(\"Class Distribution after Oversampling:\\n\", class_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:17.482039Z","iopub.execute_input":"2023-06-09T20:47:17.482991Z","iopub.status.idle":"2023-06-09T20:47:17.737537Z","shell.execute_reply.started":"2023-06-09T20:47:17.482961Z","shell.execute_reply":"2023-06-09T20:47:17.736240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_resampled.shape","metadata":{"_uuid":"f0be90f4-7473-484e-9cb6-b43184bb08f8","_cell_guid":"119baeb5-395e-4eb0-88cd-dcaf201e13d9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:17.738943Z","iopub.execute_input":"2023-06-09T20:47:17.739436Z","iopub.status.idle":"2023-06-09T20:47:17.747221Z","shell.execute_reply.started":"2023-06-09T20:47:17.739396Z","shell.execute_reply":"2023-06-09T20:47:17.746049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CHECK FOR COLLINEARITY IN THE STANDARDIZED DATA","metadata":{"_uuid":"a9490608-d3b6-42f9-9d54-43070ef3aff9","_cell_guid":"9765c65f-9d12-4977-9a32-34f029bd5c7b","trusted":true}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate the correlation matrix\ncorrelation_matrix = x_scaled.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix Heatmap')\nplt.show()\n\ncorrelation_matrix","metadata":{"_uuid":"ad666ca3-2895-4407-bd3c-45bb881b32fc","_cell_guid":"175d954c-8b7f-45ed-a9da-449fc4b117a1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:17.749242Z","iopub.execute_input":"2023-06-09T20:47:17.749732Z","iopub.status.idle":"2023-06-09T20:47:19.953180Z","shell.execute_reply.started":"2023-06-09T20:47:17.749692Z","shell.execute_reply":"2023-06-09T20:47:19.951809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DEALING WITH MULTICOLLINEARITY","metadata":{"_uuid":"8601cf05-0bdd-47d3-b99f-a9691edaf708","_cell_guid":"e5faac43-cbd7-474d-98df-8025bdfcc86d","trusted":true}},{"cell_type":"code","source":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# # Calculate the correlation matrix\n# correlation_matrix = df_scaled_reconstructed.corr()\n\n# # Plot the heatmap\n# plt.figure(figsize=(12, 8))\n# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n# plt.title('Correlation Matrix Heatmap')\n# plt.show()","metadata":{"_uuid":"0674597f-02a9-47e7-9779-827f85f91a48","_cell_guid":"75aeb7f3-a682-4e0c-b04f-a3ae7ce8348c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:19.954759Z","iopub.execute_input":"2023-06-09T20:47:19.955103Z","iopub.status.idle":"2023-06-09T20:47:19.959808Z","shell.execute_reply.started":"2023-06-09T20:47:19.955074Z","shell.execute_reply":"2023-06-09T20:47:19.958728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #y.value_counts()","metadata":{"_uuid":"f42b5549-d6f6-4d29-b56e-02162ac4e709","_cell_guid":"0ff98fa6-c7d9-4b2e-aa79-90ca9fef5932","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:19.961110Z","iopub.execute_input":"2023-06-09T20:47:19.961427Z","iopub.status.idle":"2023-06-09T20:47:19.976972Z","shell.execute_reply.started":"2023-06-09T20:47:19.961401Z","shell.execute_reply":"2023-06-09T20:47:19.975727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine x_scaled and y into a single DataFrame\ndf_scaled = X_resampled\ndf_tests = x_scaled_t\ndf_scaled.head()","metadata":{"_uuid":"06ebf703-cac7-4f9d-ba21-98504e80b451","_cell_guid":"7f955d1e-f590-4d63-be5f-a5c447b7a8b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:19.978582Z","iopub.execute_input":"2023-06-09T20:47:19.979066Z","iopub.status.idle":"2023-06-09T20:47:20.006753Z","shell.execute_reply.started":"2023-06-09T20:47:19.979015Z","shell.execute_reply":"2023-06-09T20:47:20.005367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_resampled.reset_index(drop=True,inplace=True)\nx_scaled_t.reset_index(drop=True,inplace=True)","metadata":{"_uuid":"96ac35ad-6d93-4134-a4e2-406c39db57e6","_cell_guid":"750ad9d4-59d0-4d61-9573-c0a08f297729","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:20.015188Z","iopub.execute_input":"2023-06-09T20:47:20.015604Z","iopub.status.idle":"2023-06-09T20:47:20.025217Z","shell.execute_reply.started":"2023-06-09T20:47:20.015571Z","shell.execute_reply":"2023-06-09T20:47:20.024045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(df_scaled,y_resampled, test_size=0.2,random_state=50)","metadata":{"_uuid":"772dd940-cfb6-4107-b49c-c40d1066af86","_cell_guid":"a35f3b12-c095-4fe0-9a09-5e32cd947175","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:20.026497Z","iopub.execute_input":"2023-06-09T20:47:20.026906Z","iopub.status.idle":"2023-06-09T20:47:20.039696Z","shell.execute_reply.started":"2023-06-09T20:47:20.026879Z","shell.execute_reply":"2023-06-09T20:47:20.038240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner import Objective, BayesianOptimization\n\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Dense(units=hp.Int('units_1', min_value=0, max_value=20, step=2), activation='relu', input_shape=(28,)))\n    model.add(layers.Dense(units=hp.Int('units_2', min_value=0, max_value=20, step=2), activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n                  loss='binary_crossentropy',\n                  metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n\n    return model\n\ndef search_hyperparameters(x_train, y_train, x_val, y_val, max_trials=20, epochs=10):\n    objective = Objective('val_auc', direction='max')\n    tuner = BayesianOptimization(\n        build_model,\n        objective=objective,\n        max_trials=max_trials,\n        executions_per_trial=1,\n        directory='hyperparameter_search_2jII',\n        project_name='model_tuning'\n    )\n\n    tuner.search(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val))\n\n    best_model = tuner.get_best_models(num_models=1)[0]\n    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n    return best_model, best_hyperparameters\n\n# Perform hyperparameter search\nbest_model, best_hyperparameters = search_hyperparameters(x_train, y_train, x_val, y_val)","metadata":{"_uuid":"1e946f73-b77a-4142-af8a-1bc5d2cbf7a7","_cell_guid":"a14d8fb5-c358-4d28-b5b1-53325db13578","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:48:21.487338Z","iopub.execute_input":"2023-06-09T20:48:21.487814Z","iopub.status.idle":"2023-06-09T20:53:55.159558Z","shell.execute_reply.started":"2023-06-09T20:48:21.487777Z","shell.execute_reply":"2023-06-09T20:53:55.158485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.summary()\n","metadata":{"_uuid":"5951f600-6c3d-4e63-95f7-c0ea6f80b022","_cell_guid":"d5230469-787b-4f2b-8a36-f2f654cedd46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:55:04.409568Z","iopub.execute_input":"2023-06-09T20:55:04.410023Z","iopub.status.idle":"2023-06-09T20:55:04.431894Z","shell.execute_reply.started":"2023-06-09T20:55:04.409992Z","shell.execute_reply":"2023-06-09T20:55:04.430937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n\n\n\n# Create the MLP model\nmodel = keras.Sequential([\n    keras.layers.Dense(20, activation='relu',input_shape=[28]),\n    #keras.layers.BatchNormalization(),\n    #keras.layers.Dropout(0.3),\n    keras.layers.Dense(20, activation='relu'),\n    #keras.layers.BatchNormalization(),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n              loss='binary_crossentropy',\n              metrics=[ tf.keras.metrics.AUC()])\n\n\nmodel.summary()","metadata":{"_uuid":"18a10fce-2cb2-4e15-b397-dc596f5ad3c1","_cell_guid":"393cb2ae-8ff8-45a6-b34d-1576d2887228","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:57:47.299293Z","iopub.execute_input":"2023-06-09T20:57:47.299711Z","iopub.status.idle":"2023-06-09T20:57:47.379432Z","shell.execute_reply.started":"2023-06-09T20:57:47.299680Z","shell.execute_reply":"2023-06-09T20:57:47.377762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:55:21.711947Z","iopub.execute_input":"2023-06-09T20:55:21.712382Z","iopub.status.idle":"2023-06-09T20:55:21.718172Z","shell.execute_reply.started":"2023-06-09T20:55:21.712348Z","shell.execute_reply":"2023-06-09T20:55:21.716903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_train, y_train, \n          batch_size=12, \n          epochs=20,\n          callbacks=[early_stopping],\n          shuffle=True,\n          validation_data=(x_val, y_val))","metadata":{"_uuid":"6b260583-3a2e-460e-a581-311bc6750b5b","_cell_guid":"ef15ef05-895f-495b-83fd-a7366e03a2b1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:59:05.838305Z","iopub.execute_input":"2023-06-09T20:59:05.839347Z","iopub.status.idle":"2023-06-09T20:59:37.556523Z","shell.execute_reply.started":"2023-06-09T20:59:05.839310Z","shell.execute_reply":"2023-06-09T20:59:37.555676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training and validation losses\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:59:43.529237Z","iopub.execute_input":"2023-06-09T20:59:43.529627Z","iopub.status.idle":"2023-06-09T20:59:43.843689Z","shell.execute_reply.started":"2023-06-09T20:59:43.529595Z","shell.execute_reply":"2023-06-09T20:59:43.842596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Check missing columns\nmissing_columns = set(df_tests.columns).symmetric_difference(set(x_train.columns))\n\nif len(missing_columns) > 0:\n    print(\"Missing columns:\")\n    for column in missing_columns:\n        if column in df_tests.columns and column not in x_train.columns:\n            print(f\"- Column '{column}' is missing in x_train.\")\n        elif column in x_train.columns and column not in df_tests.columns:\n            print(f\"- Column '{column}' is missing in df_tests.\")\nelse:\n    print(\"No missing columns found.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:56:53.970097Z","iopub.execute_input":"2023-06-09T20:56:53.971272Z","iopub.status.idle":"2023-06-09T20:56:53.979484Z","shell.execute_reply.started":"2023-06-09T20:56:53.971232Z","shell.execute_reply":"2023-06-09T20:56:53.978320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.705945Z","iopub.status.idle":"2023-06-09T20:47:24.706514Z","shell.execute_reply.started":"2023-06-09T20:47:24.706224Z","shell.execute_reply":"2023-06-09T20:47:24.706248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Access the accuracy and loss values from the history object\naccuracy = history.history[tf.keras.metrics.AUC()]\nval_accuracy = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Plot the accuracy curve\nplt.plot(accuracy, label='Training Accuracy')\nplt.plot(val_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the loss curve\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"_uuid":"b9d2528f-fbd7-44b5-9e7e-858bebd67a29","_cell_guid":"dcff6f69-6aa2-4afb-af6f-c83d3a016424","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:24.708136Z","iopub.status.idle":"2023-06-09T20:47:24.708555Z","shell.execute_reply.started":"2023-06-09T20:47:24.708350Z","shell.execute_reply":"2023-06-09T20:47:24.708368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBCLASSIFIER","metadata":{"_uuid":"30255629-25e7-47a6-b0c2-d765ca6b40c8","_cell_guid":"e84ca2e4-6921-430d-9779-f4742b9f3cf3","trusted":true}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [100, 500, 1000],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.4, 0.6, 0.8]\n}\n\n# Create an XGBoost classifier object\nmodel = xgb.XGBClassifier()\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc')\n\n# Fit the grid search to the data\ngrid_search.fit(x_scaled, y)\n\n# Print the best parameters and best score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best AUC Score:\", grid_search.best_score_)","metadata":{"_uuid":"7246ba16-747d-4b54-949f-ba19c7cb3b8f","_cell_guid":"1759487e-fe5e-4638-93f4-a8918abd24a0","execution":{"iopub.status.busy":"2023-06-09T20:47:24.710604Z","iopub.status.idle":"2023-06-09T20:47:24.711144Z","shell.execute_reply.started":"2023-06-09T20:47:24.710876Z","shell.execute_reply":"2023-06-09T20:47:24.710902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = xgb.XGBClassifier(colsample_bytree=0.4,\n                            learning_rate=0.01,\n                            max_depth=6,\n                            n_estimators=500,\n                            subsample=0.6,\n                            eval_metric='auc')","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.712838Z","iopub.status.idle":"2023-06-09T20:47:24.713372Z","shell.execute_reply.started":"2023-06-09T20:47:24.713095Z","shell.execute_reply":"2023-06-09T20:47:24.713119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.fit(x_scaled, y)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.715240Z","iopub.status.idle":"2023-06-09T20:47:24.715829Z","shell.execute_reply.started":"2023-06-09T20:47:24.715537Z","shell.execute_reply":"2023-06-09T20:47:24.715564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a test dataset named `x_test` for prediction\n\n# Perform predictions\npredictions = model_2.predict(df_tests)\n\n# Print the predicted values\nprint(predictions)\n\n\n# Create a DataFrame with columns \"BVN\" and \"default\" and the predicted values\npredictions_df = pd.DataFrame({'BVN': BVN, 'default': predictions})\n\n# Print the DataFrame\nprint(predictions_df)\n\n\n# Set the \"BVN\" column as the index\npredictions_df.set_index('BVN', inplace=True)\n\n# Print the updated DataFrame\nprint(predictions_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.717624Z","iopub.status.idle":"2023-06-09T20:47:24.718205Z","shell.execute_reply.started":"2023-06-09T20:47:24.717905Z","shell.execute_reply":"2023-06-09T20:47:24.717932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get the training history from the XGBoost model\ntrain_loss = model_2.evals_result()['validation_0']['logloss']\ntrain_accuracy = model_2.evals_result()['validation_0']['accuracy']\neval_loss = model_2.evals_result()['validation_1']['logloss']\neval_accuracy = model_2.evals_result()['validation_1']['accuracy']\n\n# Create a list of epochs based on the length of the training history\nepochs = range(1, len(train_loss) + 1)\n\n# Plot the training loss and accuracy\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_loss, label='Training Loss')\nplt.plot(epochs, eval_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracy, label='Training Accuracy')\nplt.plot(epochs, eval_accuracy, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"a4f9a3fe-b877-44d3-9909-44a4051bb671","_cell_guid":"4c02295c-9ff3-48cf-9e31-c16a6032c557","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-09T20:47:24.720015Z","iopub.status.idle":"2023-06-09T20:47:24.720587Z","shell.execute_reply.started":"2023-06-09T20:47:24.720286Z","shell.execute_reply":"2023-06-09T20:47:24.720317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Assuming X_train and y_train are your training features and labels\n# Assuming X_test and y_test are your testing features and labels\n\n# Create an instance of LogisticRegression\nlogreg = LogisticRegression()\n\n# Train the logistic regression model\nlogreg.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.722031Z","iopub.status.idle":"2023-06-09T20:47:24.722589Z","shell.execute_reply.started":"2023-06-09T20:47:24.722300Z","shell.execute_reply":"2023-06-09T20:47:24.722327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Make predictions on the testing dataset\ny_pred = logreg.predict(x_val)\nx_val, y_val\n# Evaluate the model\naccuracy = accuracy_score(y_val, y_pred)\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nroc_auc = roc_auc_score(y_valy_test, y_pred)\n\n# Print the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"ROC-AUC:\", roc_auc)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.723825Z","iopub.status.idle":"2023-06-09T20:47:24.724356Z","shell.execute_reply.started":"2023-06-09T20:47:24.724080Z","shell.execute_reply":"2023-06-09T20:47:24.724106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(df_tests)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:00:57.726501Z","iopub.execute_input":"2023-06-09T21:00:57.727638Z","iopub.status.idle":"2023-06-09T21:00:58.174167Z","shell.execute_reply.started":"2023-06-09T21:00:57.727599Z","shell.execute_reply":"2023-06-09T21:00:58.173041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, test_data):\n    # Get the column to use for prediction\n    test_column = test_data\n    \n    # Use the trained model to predict y_train based on the test column\n    predictions = model.predict(test_column)\n    \n    return predictions\n\n\npredictions = make_predictions(model, df_tests)\nprint(predictions)\n\np=predictions\n\nprediction = np.where(p >= 0.5, 1, 0)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:00.236155Z","iopub.execute_input":"2023-06-09T21:01:00.237211Z","iopub.status.idle":"2023-06-09T21:01:00.630883Z","shell.execute_reply.started":"2023-06-09T21:01:00.237173Z","shell.execute_reply":"2023-06-09T21:01:00.629756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf = pd.DataFrame(prediction,df_tests.index)\npf.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:03.653488Z","iopub.execute_input":"2023-06-09T21:01:03.653909Z","iopub.status.idle":"2023-06-09T21:01:03.665681Z","shell.execute_reply.started":"2023-06-09T21:01:03.653869Z","shell.execute_reply":"2023-06-09T21:01:03.664685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming you have a DataFrame called df with existing column names\n# Define a dictionary mapping old column names to new column names\ncolumn_names = {\n    0: 'default',\n}\n\n# Rename the columns using the dictionary\npf = pf.rename(columns=column_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:08.817639Z","iopub.execute_input":"2023-06-09T21:01:08.818036Z","iopub.status.idle":"2023-06-09T21:01:08.824847Z","shell.execute_reply.started":"2023-06-09T21:01:08.818008Z","shell.execute_reply":"2023-06-09T21:01:08.823400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf['BVN']=BVN","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:12.350144Z","iopub.execute_input":"2023-06-09T21:01:12.350573Z","iopub.status.idle":"2023-06-09T21:01:12.356912Z","shell.execute_reply.started":"2023-06-09T21:01:12.350533Z","shell.execute_reply":"2023-06-09T21:01:12.355753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf.set_index('BVN', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:14.273921Z","iopub.execute_input":"2023-06-09T21:01:14.275040Z","iopub.status.idle":"2023-06-09T21:01:14.281170Z","shell.execute_reply.started":"2023-06-09T21:01:14.274991Z","shell.execute_reply":"2023-06-09T21:01:14.280119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf","metadata":{"execution":{"iopub.status.busy":"2023-06-09T21:01:16.692280Z","iopub.execute_input":"2023-06-09T21:01:16.692677Z","iopub.status.idle":"2023-06-09T21:01:16.704015Z","shell.execute_reply.started":"2023-06-09T21:01:16.692649Z","shell.execute_reply":"2023-06-09T21:01:16.702924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf.to_csv('/kaggle/working/prediction_June.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.743664Z","iopub.status.idle":"2023-06-09T20:47:24.744167Z","shell.execute_reply.started":"2023-06-09T20:47:24.743914Z","shell.execute_reply":"2023-06-09T20:47:24.743938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, test_data, column_name):\n    # Get the column to use for prediction\n    test_column = test_data[column_name]\n    \n    # Use the trained model to predict y_train based on the test column\n    predictions = model.predict(test_column)\n    \n    return predictions\n\npredictions = make_predictions(model, df_tests, 'BVN')\npf = pd.DataFrame(predictions,df_tests['BVN'])","metadata":{"execution":{"iopub.status.busy":"2023-06-09T20:47:24.746610Z","iopub.status.idle":"2023-06-09T20:47:24.747018Z","shell.execute_reply.started":"2023-06-09T20:47:24.746829Z","shell.execute_reply":"2023-06-09T20:47:24.746848Z"},"trusted":true},"execution_count":null,"outputs":[]}]}