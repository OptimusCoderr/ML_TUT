{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7863251,"sourceType":"datasetVersion","datasetId":4612939}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## READ TEST, TRAIND AND VALIDATION FILES","metadata":{"_uuid":"fe6c9936-5f1b-4874-b990-a23085a270b8","_cell_guid":"b6a72be2-611b-45d1-b271-25b020328095","execution":{"iopub.status.busy":"2024-03-17T13:33:38.482231Z","iopub.execute_input":"2024-03-17T13:33:38.482673Z","iopub.status.idle":"2024-03-17T13:33:39.059369Z","shell.execute_reply.started":"2024-03-17T13:33:38.482639Z","shell.execute_reply":"2024-03-17T13:33:39.057982Z"},"trusted":true}},{"cell_type":"code","source":"import pandas as pd\n# Read the .tsv file into a Pandas DataFrame\ndf_train = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/train.tsv', sep='\\t')\ndf_valid = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/validation.tsv', sep='\\t')\ndf_test = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/test.tsv', sep='\\t')\n# Print the first few rows of the DataFrame","metadata":{"_uuid":"c5212278-b42d-4921-844f-5d55dd9e7327","_cell_guid":"6ca6417e-a2fa-4d34-be61-f7b68b647210","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:23:57.570129Z","iopub.execute_input":"2024-03-18T15:23:57.570956Z","iopub.status.idle":"2024-03-18T15:23:58.687717Z","shell.execute_reply.started":"2024-03-18T15:23:57.570912Z","shell.execute_reply":"2024-03-18T15:23:58.686106Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"_uuid":"42e2bc6f-4fd1-4ae8-9e52-0e4639383517","_cell_guid":"e1906a18-e1e7-41ef-ab44-058e4b8b18e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:23:58.690404Z","iopub.execute_input":"2024-03-18T15:23:58.690770Z","iopub.status.idle":"2024-03-18T15:23:58.715130Z","shell.execute_reply.started":"2024-03-18T15:23:58.690741Z","shell.execute_reply":"2024-03-18T15:23:58.713774Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   file_name  label  shaming  stereotype  objectification  violence  \\\n0   8716.jpg      1        0           1                1         0   \n1   3066.jpg      1        1           1                0         0   \n2   6038.jpg      0        0           0                0         0   \n3  10861.jpg      1        0           0                1         0   \n4  11198.jpg      0        0           0                0         0   \n\n                                                text  \n0  GETS MARRIED TO GIRL OF HIS DREAMS SHE DOESN'T...  \n1  When your mama don't change yo diaper for 19 y...  \n2  Some people want a big house, a fast car, and ...  \n3     FAP FAP FAP FAP FAP FAP memecenter Meme Center  \n4              I RAISE. A I CALL. I FOLD. I'M ALL IN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>label</th>\n      <th>shaming</th>\n      <th>stereotype</th>\n      <th>objectification</th>\n      <th>violence</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8716.jpg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>GETS MARRIED TO GIRL OF HIS DREAMS SHE DOESN'T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3066.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>When your mama don't change yo diaper for 19 y...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6038.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Some people want a big house, a fast car, and ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10861.jpg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>FAP FAP FAP FAP FAP FAP memecenter Meme Center</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11198.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>I RAISE. A I CALL. I FOLD. I'M ALL IN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Attaching the image file_path","metadata":{"_uuid":"9a107ce0-70f9-426f-abed-07996a4c1e2b","_cell_guid":"c6bc5487-77ec-4aba-9873-b58930cf03b7","trusted":true}},{"cell_type":"code","source":"import pandas as pd\n\n# Read the .tsv file into a Pandas DataFrame\ndf_train = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/train.tsv', sep='\\t')\ndf_valid = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/validation.tsv', sep='\\t')\ndf_test = pd.read_csv('/kaggle/input/multimodal-misogyny-detection-mami-2022/test.tsv', sep='\\t')\n\n# Define the path to the image folders\ntrain_image_folder = '/kaggle/input/multimodal-misogyny-detection-mami-2022/MAMI_2022_images/training_images/'\ntest_image_folder = '/kaggle/input/multimodal-misogyny-detection-mami-2022/MAMI_2022_images/test_images/'\n\n# Function to generate file path using file name\ndef get_file_path(row):\n    if row['file_name'] in df_train['file_name'].values:\n        return train_image_folder + row['file_name']\n    elif row['file_name'] in df_valid['file_name'].values:\n        return train_image_folder + row['file_name']\n    elif row['file_name'] in df_test['file_name'].values:\n        return test_image_folder + row['file_name']\n    else:\n        return None\n\n# Apply the function to generate file path\ndf_train['file_path'] = df_train.apply(get_file_path, axis=1)\ndf_valid['file_path'] = df_valid.apply(get_file_path, axis=1)\ndf_test['file_path'] = df_test.apply(get_file_path, axis=1)","metadata":{"_uuid":"0b382ef7-d559-47e1-9078-08880c736009","_cell_guid":"7beea55c-830d-456b-af57-186ab4c51e43","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:23:58.716685Z","iopub.execute_input":"2024-03-18T15:23:58.717736Z","iopub.status.idle":"2024-03-18T15:24:04.856341Z","shell.execute_reply.started":"2024-03-18T15:23:58.717697Z","shell.execute_reply":"2024-03-18T15:24:04.855194Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_valid.file_path[0]","metadata":{"_uuid":"ec3785fb-498b-443a-9dfe-1deb6ca38618","_cell_guid":"51222dd2-8e2e-4e42-b273-6b849d979280","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:24:04.859317Z","iopub.execute_input":"2024-03-18T15:24:04.859676Z","iopub.status.idle":"2024-03-18T15:24:04.866618Z","shell.execute_reply.started":"2024-03-18T15:24:04.859649Z","shell.execute_reply":"2024-03-18T15:24:04.865346Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/multimodal-misogyny-detection-mami-2022/MAMI_2022_images/training_images/1377.jpg'"},"metadata":{}}]},{"cell_type":"markdown","source":"### RUNNING AN INFERENCE ON THE TEST IMAGES using nlpconnect/vit-gpt2-image-captioning\nThis is to check the model behave on these images","metadata":{"_uuid":"1dbc31b0-402e-4ca6-b962-c35bb6806194","_cell_guid":"79ce5e03-9dab-401e-aa54-5da93411788d","trusted":true}},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n# from PIL import Image\n\n# # Load pre-trained model and tokenizer\n# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n# tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# # Move model to available device (GPU or CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# # Set generation parameters\n# max_length = 16\n# num_beams = 4\n# gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n# # Define function to predict captions for images with attention mask\n# def predict_caption(df):\n#     captions = []\n#     for index, row in df.iterrows():\n#         image_path = row['file_path']\n#         image = Image.open(image_path)\n#         if image.mode != \"RGB\":\n#             image = image.convert(mode=\"RGB\")\n        \n#         pixel_values = feature_extractor(images=[image], return_tensors=\"pt\").pixel_values\n#         pixel_values = pixel_values.to(device)\n        \n#         # Create attention mask to avoid padding tokens affecting the output\n#         attention_mask = torch.ones_like(pixel_values)\n#         attention_mask[pixel_values == 0] = 0\n        \n#         output_ids = model.generate(pixel_values, attention_mask=attention_mask, **gen_kwargs)\n#         preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n#         preds = [pred.strip() for pred in preds]\n        \n#         captions.append(preds[0])  # Assuming only one caption per image\n        \n#     return captions","metadata":{"_uuid":"541d5bfd-d586-42d2-8036-66757ad08c32","_cell_guid":"7fa36a55-e936-4a09-819d-2aee7edd4d47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:24:04.868541Z","iopub.execute_input":"2024-03-18T15:24:04.869312Z","iopub.status.idle":"2024-03-18T15:24:04.881820Z","shell.execute_reply.started":"2024-03-18T15:24:04.869280Z","shell.execute_reply":"2024-03-18T15:24:04.880738Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Predict and plot only 5 random images\n","metadata":{}},{"cell_type":"code","source":"# from PIL import Image\n# import random\n# import matplotlib.pyplot as plt\n\n# # Select 5 random images from the test set\n# random_indices = random.sample(range(len(df_test)), 5)\n# df_sampled = df_test.iloc[random_indices]\n\n# # Predict captions for the sampled images\n# predicted_captions_sampled = predict_caption(df_sampled)\n\n# # Plot the sampled images with their predicted captions\n# plt.figure(figsize=(15, 10))\n# for i, (index, row) in enumerate(df_sampled.iterrows()):\n#     image_path = row['file_path']\n#     image = Image.open(image_path)\n    \n#     plt.subplot(2, 3, i+1)\n#     plt.imshow(image)\n#     plt.title(predicted_captions_sampled[i])\n#     plt.axis('off')\n\n# plt.tight_layout()\n# plt.show()","metadata":{"_uuid":"43569682-d396-45c1-b023-c477b0693ed8","_cell_guid":"de35e185-bff3-45f3-b858-fd05ccc9d30f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:24:04.883459Z","iopub.execute_input":"2024-03-18T15:24:04.884505Z","iopub.status.idle":"2024-03-18T15:24:04.897209Z","shell.execute_reply.started":"2024-03-18T15:24:04.884460Z","shell.execute_reply":"2024-03-18T15:24:04.896012Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# # Print the first few rows of the DataFrame with predicted captions\n# df_pred.caption[6]","metadata":{"_uuid":"fb4df99b-b30e-4f37-be6a-a920d58b8c7d","_cell_guid":"bb2f9e0e-4149-4a7c-9dc6-9cb88d23b5bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:24:04.898618Z","iopub.execute_input":"2024-03-18T15:24:04.899116Z","iopub.status.idle":"2024-03-18T15:24:04.912634Z","shell.execute_reply.started":"2024-03-18T15:24:04.899080Z","shell.execute_reply":"2024-03-18T15:24:04.910961Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### USING OCR TO READ THE TEXT IN THE IMAGES","metadata":{}},{"cell_type":"code","source":"# from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n# import requests\n# from PIL import Image\n# import pandas as pd\n\n# def perform_ocr_on_images(df):\n#     # Load TrOCR processor and VisionEncoderDecoderModel\n#     processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n#     model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n    \n#     ocr_results = []\n    \n#     for index, row in df.iterrows():\n#         file_path = row['file_path']\n        \n#         # Load image from the file path\n#         image = Image.open(file_path).convert(\"RGB\")\n        \n#         # Process the image using the TrOCR processor\n#         pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n        \n#         # Generate text from the processed image using the VisionEncoderDecoderModel\n#         generated_ids = model.generate(pixel_values)\n#         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        \n#         ocr_results.append(generated_text)\n    \n#     return ocr_results\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:04.914487Z","iopub.execute_input":"2024-03-18T15:24:04.915257Z","iopub.status.idle":"2024-03-18T15:24:04.925111Z","shell.execute_reply.started":"2024-03-18T15:24:04.915219Z","shell.execute_reply":"2024-03-18T15:24:04.924068Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# import random\n# import matplotlib.pyplot as plt\n\n# # Select 5 random images from the test set\n# random_indices = random.sample(range(len(df_test)), 5)\n# df_sampled = df_test.iloc[random_indices]\n\n# # Predict captions for the sampled images\n# predicted_captions_sampled = perform_ocr_on_images(df_sampled)\n\n# # Plot the sampled images with their predicted captions\n# plt.figure(figsize=(15, 10))\n# for i, (index, row) in enumerate(df_sampled.iterrows()):\n#     image_path = row['file_path']\n#     image = Image.open(image_path)\n    \n#     plt.subplot(2, 3, i+1)\n#     plt.imshow(image)\n#     plt.title(predicted_captions_sampled[i])\n#     plt.axis('off')\n\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:04.926879Z","iopub.execute_input":"2024-03-18T15:24:04.927304Z","iopub.status.idle":"2024-03-18T15:24:04.938361Z","shell.execute_reply.started":"2024-03-18T15:24:04.927272Z","shell.execute_reply":"2024-03-18T15:24:04.937180Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# IMAGE BASELINE VGG16","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nepochs = 50\nimage_size = 224\nthreshold = 0.5\n","metadata":{"_uuid":"1ecdef79-3b5e-46ab-b9da-b263a7f9afc9","_cell_guid":"46b6e306-7eda-470a-bff6-e1bd6a74a8b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-18T15:24:04.942887Z","iopub.execute_input":"2024-03-18T15:24:04.943552Z","iopub.status.idle":"2024-03-18T15:24:04.953404Z","shell.execute_reply.started":"2024-03-18T15:24:04.943514Z","shell.execute_reply":"2024-03-18T15:24:04.952431Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# import tensorflow_hub as hub\nimport tensorflow as tf\nimport numpy as np\nimport keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras import regularizers\nimport os\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:25:12.863027Z","iopub.execute_input":"2024-03-18T15:25:12.864328Z","iopub.status.idle":"2024-03-18T15:25:12.881244Z","shell.execute_reply.started":"2024-03-18T15:25:12.864274Z","shell.execute_reply":"2024-03-18T15:25:12.879932Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Convert the label column to strings\ndf_train[\"label\"] = df_train[\"label\"].astype(str)\ndf_test[\"label\"] = df_test[\"label\"].astype(str)\ndf_valid[\"label\"] = df_valid[\"label\"].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:25:17.008547Z","iopub.execute_input":"2024-03-18T15:25:17.009023Z","iopub.status.idle":"2024-03-18T15:25:17.024616Z","shell.execute_reply.started":"2024-03-18T15:25:17.008971Z","shell.execute_reply":"2024-03-18T15:25:17.023689Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define the data generator for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=False,\n    vertical_flip=False,\n    fill_mode='nearest',\n    \n)\n\n# Define the data generator for validation images\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\n# Define the batch size\nbatch_size = 32\n\n# Create the training data generator\ntrain_dataset = train_datagen.flow_from_dataframe(\n    dataframe=df_train,\n    x_col='file_path',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=True\n)\n\n# Create the validation data generator\nval_dataset  = valid_datagen.flow_from_dataframe(\n    dataframe=df_valid,\n    x_col='file_path',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n)\n\n# Define the data generator for test images\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create the test data generator\ntest_dataset = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    x_col='file_path',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False,\n)\n\n\n# # Define the data generator for test images\n# test_datagen = ImageDataGenerator(rescale=1./255)\n\n# # Create the test data generator\n# test_generator = test_datagen.flow_from_dataframe(\n#     dataframe=df_test,\n#     x_col='roi_path',\n#     y_col='label',\n#     target_size=(224, 224),\n#     batch_size=batch_size,\n#     class_mode='binary',\n#     shuffle=False,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:28.012539Z","iopub.execute_input":"2024-03-18T15:27:28.013731Z","iopub.status.idle":"2024-03-18T15:27:36.440137Z","shell.execute_reply.started":"2024-03-18T15:27:28.013685Z","shell.execute_reply":"2024-03-18T15:27:36.439184Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Found 9000 validated image filenames belonging to 2 classes.\nFound 1000 validated image filenames belonging to 2 classes.\nFound 1000 validated image filenames belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset.labels\ntest_dataset.filepaths","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:33:32.271856Z","iopub.execute_input":"2024-03-18T15:33:32.272950Z","iopub.status.idle":"2024-03-18T15:33:32.318090Z","shell.execute_reply.started":"2024-03-18T15:33:32.272914Z","shell.execute_reply":"2024-03-18T15:33:32.315398Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset\u001b[38;5;241m.\u001b[39mlabels\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_paths\u001b[49m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrameIterator' object has no attribute 'file_paths'"],"ename":"AttributeError","evalue":"'DataFrameIterator' object has no attribute 'file_paths'","output_type":"error"}]},{"cell_type":"code","source":"metrics = [\n        tf.keras.metrics.TruePositives(name = 'tp'),\n        tf.keras.metrics.TrueNegatives(name = 'tn'),\n        tf.keras.metrics.FalsePositives(name = 'fp'),\n        tf.keras.metrics.FalseNegatives(name = 'fn'),\n        tf.keras.metrics.BinaryAccuracy(name = 'acc'),\n        tf.keras.metrics.Recall(name = 'r'),\n        tf.keras.metrics.Precision(name = 'p'),\n#         F1Score(threshold = 0.5, average='micro', name = 'F1'),\n#         CohenKappa(num_classes = 2, name = 'Kappa')\n    ]\nepochs=10","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.855498Z","iopub.status.idle":"2024-03-18T15:24:22.855893Z","shell.execute_reply.started":"2024-03-18T15:24:22.855698Z","shell.execute_reply":"2024-03-18T15:24:22.855714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2_strength = 1e-5\n\ninput_image = layers.Input(shape=(image_size,image_size,3))\nvgg_model = VGG16(input_tensor = input_image, weights = 'imagenet', include_top=False)\n\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\nx = vgg_model.output\nx = layers.Flatten(input_shape=vgg_model.output_shape[1:])(x)\nx = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_strength))(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_strength))(x)\nimage_output = layers.Dense(1, activation='sigmoid')(x)\n\n# Define the image model\nimage_model = Model(inputs=input_image, outputs=image_output)\n\nimage_model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=metrics)\nimage_model.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.857707Z","iopub.status.idle":"2024-03-18T15:24:22.858508Z","shell.execute_reply.started":"2024-03-18T15:24:22.858209Z","shell.execute_reply":"2024-03-18T15:24:22.858234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [tf.keras.callbacks.ReduceLROnPlateau(factor = 0.3)]","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.859843Z","iopub.status.idle":"2024-03-18T15:24:22.860795Z","shell.execute_reply.started":"2024-03-18T15:24:22.860591Z","shell.execute_reply":"2024-03-18T15:24:22.860609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_fit = image_model.fit(\n        train_dataset.filepaths,\n    train_dataset.labels\n        validation_data = val_dataset,\n        batch_size = 32, \n        epochs = epochs, \n        verbose = 1, \n        callbacks = callbacks,\n        shuffle = True,\n        initial_epoch = 0,)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.862067Z","iopub.status.idle":"2024-03-18T15:24:22.863229Z","shell.execute_reply.started":"2024-03-18T15:24:22.862891Z","shell.execute_reply":"2024-03-18T15:24:22.862922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_model.save('ImageModel/model_image.h5')\n# tf.keras.utils.plot_model(image_model, \"./ImageModel/model_image.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.864863Z","iopub.status.idle":"2024-03-18T15:24:22.865446Z","shell.execute_reply.started":"2024-03-18T15:24:22.865163Z","shell.execute_reply":"2024-03-18T15:24:22.865185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"0bff5585-1d2e-4815-a2f2-e455da05f165","_cell_guid":"41aadc85-f64d-4465-ac33-22a1058b72f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BILSTM ON THE DATASET","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.866973Z","iopub.status.idle":"2024-03-18T15:24:22.868161Z","shell.execute_reply.started":"2024-03-18T15:24:22.867844Z","shell.execute_reply":"2024-03-18T15:24:22.867868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the label column to strings\ndf_train[\"label\"] = df_train[\"label\"].astype(int)\ndf_test[\"label\"] = df_test[\"label\"].astype(int)\ndf_valid[\"label\"] = df_valid[\"label\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.869466Z","iopub.status.idle":"2024-03-18T15:24:22.870115Z","shell.execute_reply.started":"2024-03-18T15:24:22.869908Z","shell.execute_reply":"2024-03-18T15:24:22.869925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df_train contains 'text' and 'label' columns\n# Split the data into training and validation sets\n\nX_train = df_train['text'].values\ny_train = df_train['label'].values\n\nX_val = df_valid['text'].values\ny_val = df_valid['label'].values\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Tokenize the text data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(seq) for seq in X_train_seq])\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\nX_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n\n\n# Define the text input placeholder\ntext_input = layers.Input(shape=(max_length,))\n\n# Tokenize the text data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(seq) for seq in X_train_seq])\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\nX_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n\n\n# Define the text model\nembedding_layer = layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(text_input)\nlstm_layer = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(embedding_layer)\ntext_output = layers.Dense(1, activation='sigmoid')(lstm_layer)\n\ntext_model = Model(inputs=text_input, outputs=text_output)\n\n# Compile the model\ntext_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.871421Z","iopub.status.idle":"2024-03-18T15:24:22.872234Z","shell.execute_reply.started":"2024-03-18T15:24:22.871920Z","shell.execute_reply":"2024-03-18T15:24:22.871945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.873860Z","iopub.status.idle":"2024-03-18T15:24:22.874429Z","shell.execute_reply.started":"2024-03-18T15:24:22.874156Z","shell.execute_reply":"2024-03-18T15:24:22.874178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntext_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=epochs, batch_size=32)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.875582Z","iopub.status.idle":"2024-03-18T15:24:22.876166Z","shell.execute_reply.started":"2024-03-18T15:24:22.875868Z","shell.execute_reply":"2024-03-18T15:24:22.875890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### COMBINING THE IMAGE WITH THE TEXT MODEL","metadata":{}},{"cell_type":"code","source":"train_dataset.index_array","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.878122Z","iopub.status.idle":"2024-03-18T15:24:22.878653Z","shell.execute_reply.started":"2024-03-18T15:24:22.878381Z","shell.execute_reply":"2024-03-18T15:24:22.878403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Combine both models\ncombined_output = layers.concatenate([image_output, text_output])\n\n# Additional dense layers for combined processing\nx = layers.Dense(128, activation='relu')(combined_output)\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Define the combined model\ncombined_model = Model(inputs=[input_image, text_input], outputs=output)\n\n# Compile the combined model\ncombined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Summary of the combined model\ncombined_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.880902Z","iopub.status.idle":"2024-03-18T15:24:22.881462Z","shell.execute_reply.started":"2024-03-18T15:24:22.881186Z","shell.execute_reply":"2024-03-18T15:24:22.881209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## testing the combine model","metadata":{}},{"cell_type":"code","source":"# history_fit = image_model.fit(\n#         train_dataset, \n#         validation_data = val_dataset,\n#         batch_size = 32, \n#         epochs = epochs, \n#         verbose = 1, \n#         callbacks = callbacks,\n#         shuffle = True,\n#         initial_epoch = 0,)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.883330Z","iopub.status.idle":"2024-03-18T15:24:22.884115Z","shell.execute_reply.started":"2024-03-18T15:24:22.883881Z","shell.execute_reply":"2024-03-18T15:24:22.883900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"loading the test datat in the right form","metadata":{}},{"cell_type":"code","source":"X_test=df_test['text'].values\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.885413Z","iopub.status.idle":"2024-03-18T15:24:22.885789Z","shell.execute_reply.started":"2024-03-18T15:24:22.885600Z","shell.execute_reply":"2024-03-18T15:24:22.885615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = test_dataset.classes\ny_pred = image_model.predict(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.886877Z","iopub.status.idle":"2024-03-18T15:24:22.887339Z","shell.execute_reply.started":"2024-03-18T15:24:22.887078Z","shell.execute_reply":"2024-03-18T15:24:22.887092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Convert the predicted probabilities to binary predictions\ny_pred_binary = (y_pred > 0.5).astype(int)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_true, y_pred_binary)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No MISOGNY', 'mISOGYNY'], yticklabels=['No Glaucoma', 'Glaucoma'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.888772Z","iopub.status.idle":"2024-03-18T15:24:22.889161Z","shell.execute_reply.started":"2024-03-18T15:24:22.888962Z","shell.execute_reply":"2024-03-18T15:24:22.888989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.890186Z","iopub.status.idle":"2024-03-18T15:24:22.890550Z","shell.execute_reply.started":"2024-03-18T15:24:22.890370Z","shell.execute_reply":"2024-03-18T15:24:22.890385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredictions = combined_model.predict([test_dataset, X_test_pad], batch_size=batch_size)\npredictions = predictions.reshape(predictions.shape[0])\npred = predictions > 0.5\npred = list(map(int, pred)) #true/false to 1/0\n\npredictions_db = pd.DataFrame(data=test_df['file_name'])\npredictions_db['misogynist'] = pred\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:24:22.891678Z","iopub.status.idle":"2024-03-18T15:24:22.892157Z","shell.execute_reply.started":"2024-03-18T15:24:22.891950Z","shell.execute_reply":"2024-03-18T15:24:22.891966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}