{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260},{"sourceId":11049071,"sourceType":"datasetVersion","datasetId":6883180},{"sourceId":290798,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":249148,"modelId":270667}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch datasets sentence-transformers PyPDF2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T11:13:19.973211Z","iopub.execute_input":"2025-03-18T11:13:19.973614Z","iopub.status.idle":"2025-03-18T11:13:26.998551Z","shell.execute_reply.started":"2025-03-18T11:13:19.973580Z","shell.execute_reply":"2025-03-18T11:13:26.997442Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import os\n# import PyPDF2\n\n# def extract_text_from_pdf(pdf_path):\n#     \"\"\"Extract text from a PDF file.\"\"\"\n#     text = \"\"\n#     try:\n#         with open(pdf_path, \"rb\") as file:\n#             reader = PyPDF2.PdfReader(file)\n#             for page in reader.pages:\n#                 text += page.extract_text() + \"\\n\"\n#     except Exception as e:\n#         print(f\"Error extracting {pdf_path}: {e}\")\n#     return text\n\n# def parse_resume_dataset(root_dir):\n#     \"\"\"Parse through the dataset directory and extract text from PDFs.\"\"\"\n#     job_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n    \n#     resume_data = {}\n    \n#     for job in job_folders:\n#         job_path = os.path.join(root_dir, job)\n#         resume_data[job] = []\n        \n#         for filename in os.listdir(job_path):\n#             if filename.lower().endswith(\".pdf\"):\n#                 pdf_path = os.path.join(job_path, filename)\n#                 text = extract_text_from_pdf(pdf_path)\n#                 resume_data[job].append({\"filename\": filename, \"content\": text})\n    \n#     return resume_data\n\n# # Define dataset root directory\n# dataset_path = \"/kaggle/input/resume-dataset/data/data\"\n# resumes = parse_resume_dataset(dataset_path)\n\n# # Print sample output\n# for job, files in resumes.items():\n#     print(f\"Job Category: {job}\")\n#     for resume in files[:2]:  # Show only first two resumes per job\n#         print(f\"Filename: {resume['filename']}\")\n#         print(f\"Content Preview: {resume['content'][:500]}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.543422Z","iopub.execute_input":"2025-03-17T14:40:15.543716Z","iopub.status.idle":"2025-03-17T14:40:15.547717Z","shell.execute_reply.started":"2025-03-17T14:40:15.543692Z","shell.execute_reply":"2025-03-17T14:40:15.546801Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import os\n# import PyPDF2\n# import json\n\n# def extract_text_from_pdf(pdf_path):\n#     \"\"\"Extract text from a PDF file.\"\"\"\n#     text = \"\"\n#     try:\n#         with open(pdf_path, \"rb\") as file:\n#             reader = PyPDF2.PdfReader(file)\n#             for page in reader.pages:\n#                 text += page.extract_text() + \"\\n\"\n#     except Exception as e:\n#         print(f\"Error extracting {pdf_path}: {e}\")\n#     return text\n\n# def load_checkpoint(checkpoint_path):\n#     \"\"\"Load checkpoint if exists.\"\"\"\n#     if os.path.exists(checkpoint_path):\n#         with open(checkpoint_path, \"r\") as file:\n#             return json.load(file)\n#     return {}\n\n# def save_checkpoint(checkpoint_path, data):\n#     \"\"\"Save checkpoint to file.\"\"\"\n#     with open(checkpoint_path, \"w\") as file:\n#         json.dump(data, file)\n\n# def parse_resume_dataset(root_dir, output_dir, checkpoint_path):\n#     \"\"\"Parse through the dataset directory and extract text from PDFs.\"\"\"\n#     job_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n#     resume_data = load_checkpoint(checkpoint_path)\n    \n#     for job in job_folders:\n#         job_path = os.path.join(root_dir, job)\n#         if job not in resume_data:\n#             resume_data[job] = []\n        \n#         processed_files = {r[\"filename\"] for r in resume_data[job]}\n        \n#         for filename in os.listdir(job_path):\n#             if filename.lower().endswith(\".pdf\") and filename not in processed_files:\n#                 pdf_path = os.path.join(job_path, filename)\n#                 text = extract_text_from_pdf(pdf_path)\n#                 resume_data[job].append({\"filename\": filename, \"content\": text})\n                \n#                 # Save checkpoint after each file\n#                 save_checkpoint(checkpoint_path, resume_data)\n    \n#     output_file = os.path.join(output_dir, \"resume_texts.json\")\n#     with open(output_file, \"w\") as f:\n#         json.dump(resume_data, f, indent=4)\n    \n#     return resume_data\n\n# # Define dataset root directory and output directory\n# dataset_path = \"/kaggle/input/resume-dataset/data/data\"\n# output_path = \"/kaggle/working\"\n# checkpoint_file = os.path.join(output_path, \"checkpoint.json\")\n\n# resumes = parse_resume_dataset(dataset_path, output_path, checkpoint_file)\n\n# # Print sample output\n# for job, files in resumes.items():\n#     print(f\"Job Category: {job}\")\n#     for resume in files[:2]:  # Show only first two resumes per job\n#         print(f\"Filename: {resume['filename']}\")\n#         print(f\"Content Preview: {resume['content'][:500]}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.549173Z","iopub.execute_input":"2025-03-17T14:40:15.549434Z","iopub.status.idle":"2025-03-17T14:40:15.557759Z","shell.execute_reply.started":"2025-03-17T14:40:15.549411Z","shell.execute_reply":"2025-03-17T14:40:15.556960Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import torch\n# from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n# from datasets import Dataset\n\n# # Load the JSON file containing resume texts\n# def load_resume_data(json_path):\n#     with open(json_path, \"r\") as f:\n#         resume_data = json.load(f)\n#     return resume_data\n\n# # Preprocess the resume data for training\n# def preprocess_data(resume_data):\n#     inputs = []\n#     labels = []\n#     for job, resumes in resume_data.items():\n#         for resume in resumes:\n#             text = resume[\"content\"]\n#             # Tokenize the text\n#             tokens = tokenizer(\n#                 text,\n#                 truncation=True,\n#                 padding=\"max_length\",\n#                 max_length=512,  # Adjust based on your needs\n#                 return_tensors=\"pt\",\n#             )\n#             inputs.append(tokens)\n            \n#             # Create pseudo-labels (replace this with actual labels if available)\n#             # For now, we use a dummy label (0) for all tokens\n#             label = [0] * len(tokens[\"input_ids\"][0])  # Dummy label\n#             labels.append(label)\n    \n#     # Combine inputs and labels into a dataset\n#     dataset = {\n#         \"input_ids\": torch.cat([x[\"input_ids\"] for x in inputs]),\n#         \"attention_mask\": torch.cat([x[\"attention_mask\"] for x in inputs]),\n#         \"labels\": torch.tensor(labels),\n#     }\n#     return Dataset.from_dict(dataset)\n\n# # Load pre-trained model and tokenizer\n# model_name = \"bert-base-uncased\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=8)  # Adjust num_labels as needed\n\n# # Load the JSON file\n# json_path = os.path.join(output_path, \"resume_texts.json\")\n# resume_data = load_resume_data(json_path)\n\n# # Preprocess the data\n# dataset = preprocess_data(resume_data)\n\n# # Define training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"no\",  # Disable evaluation\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=16,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     save_total_limit=2,\n#     save_steps=500,\n#     logging_dir=\"./logs\",\n#     logging_steps=10,\n# )\n\n# # Define Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=dataset,\n# )\n\n# # Fine-tune the model\n# trainer.train()\n\n# # Save the fine-tuned model\n# trainer.save_model(\"./fine-tuned-resume-model\")\n# tokenizer.save_pretrained(\"./fine-tuned-resume-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.558589Z","iopub.execute_input":"2025-03-17T14:40:15.558828Z","iopub.status.idle":"2025-03-17T14:40:15.571251Z","shell.execute_reply.started":"2025-03-17T14:40:15.558799Z","shell.execute_reply":"2025-03-17T14:40:15.570601Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Enhanced patterns with better context awareness and coverage\nPATTERNS = {\n    \"name\": [\n        # Handles multi-part names, hyphens, and international characters\n        r\"(?i)\\b(?:[A-Z][a-z]+(?:[-'\\s][A-Z][a-z]+){1,3})\\b\",\n        # Matches name headers (e.g., \"Name:\", \"Full Name:\")\n        r\"(?i)(?:name|full name|contact name):?\\s*([A-Z][a-z]+(?: [A-Z][a-z]+){1,3})\"\n    ],\n    \n    \"email\": [\n        # Improved email pattern with common resume formats\n        r\"(?i)(?:email|e-mail|contact):?\\s*([a-z0-9_.+-]+@[a-z0-9-]+\\.[a-z0-9-.]{2,})\",\n        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\"\n    ],\n    \n    \"phone\": [\n        # Comprehensive phone number patterns with optional country codes\n        r\"(?i)(?:phone|mobile|tel):?\\s*(\\+?\\d[\\d\\s-]{7,15})\",\n        r\"\\b\\+\\d{1,3}[- ]?\\(?\\d{3}\\)?[- ]?\\d{3}[- ]?\\d{4}\\b\",\n        r\"\\b\\d{3}[- ]?\\d{3}[- ]?\\d{4}\\b\"\n    ],\n    \n    \"skill\": [\n        # Skills with context awareness (section headers and bullet points)\n        r\"(?i)(?:skills|technical skills|competencies):?[\\s\\S]*?([-\\•].*?)(?=\\n\\s*\\n)\",\n        # Programming languages with version numbers\n        r\"\\b(?:python|java|c\\+\\+)(?:\\s*\\d+\\.?\\d*)?\\b\",\n        # Cloud technologies with platforms\n        r\"\\b(?:aws|azure|gcp)(?:\\s*\\(?(?:ec2|s3|lambda)\\)?)?\\b\",\n        # Frameworks with version ranges\n        r\"\\b(?:react(?:\\.js)?|angular|vue)(?:\\s*\\d+\\.?\\d*[- ]?\\d*\\.?\\d*)?\\b\",\n        # Certifications in skills section\n        r\"\\b(?:certified|licensed)\\s+(?:aws|scrum|pmp)\\b\"\n    ],\n    \n    \"experience\": [\n        # Job title with company and duration pattern\n        r\"(?i)(\\b(?:senior|junior)?\\s*(?:software engineer|data scientist)\\b.*?\\b(?:at|@)\\b.*?\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b\\s*\\d{4}.*?–.*?(?:present|\\d{4}))\",\n        # Duration patterns with multiple formats\n        r\"\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\s*\\d{4}\\s*(?:–|-|to)\\s*(?:present|current|(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\s*\\d{4})\\b\",\n        # Achievement-based experience points\n        r\"-\\s*(?:led|developed|implemented|optimized)\\s+.*?\\b(?:using|with)\\b.*?\\b(?:python|machine learning|aws)\\b\"\n    ],\n    \n    \"certification\": [\n        # Certification with issuing authority and date\n        r\"(?i)(?:certifications|licenses):?[\\s\\S]*?-\\s*(.*?)\\s*(?:\\(?(?:aws|microsoft|google)\\)?)\\s*(?:\\(\\d{4}\\))?\",\n        # Specific certification patterns\n        r\"\\b(?:AWS Certified Solutions Architect|Google Cloud Professional|PMP®)\\b(?:\\s*–?\\s*\\d{4})?\",\n        # Certification IDs\n        r\"\\b(?:certification\\s+ID:?\\s*[A-Z0-9-]+)\\b\"\n    ]\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.572066Z","iopub.execute_input":"2025-03-17T14:40:15.572353Z","iopub.status.idle":"2025-03-17T14:40:15.584542Z","shell.execute_reply.started":"2025-03-17T14:40:15.572327Z","shell.execute_reply":"2025-03-17T14:40:15.583734Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# import re\n# import json\n# import random\n# import numpy as np\n# from collections import defaultdict\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Input, Concatenate\n# from tensorflow.keras.optimizers import Adam\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.base import BaseEstimator, TransformerMixin\n\n# # Enhanced patterns with context awareness\n# PATTERNS = {\n#     \"name\": [\n#         r\"(?i)\\b(?:[A-Z][a-z]+(?:[-'\\s][A-Z][a-z]+){1,3})\\b\",\n#         r\"(?i)(?:name|full name|contact name):?\\s*([A-Z][a-z]+(?: [A-Z][a-z]+){1,3})\"\n#     ],\n#     \"email\": [\n#         r\"(?i)(?:email|e-mail|contact):?\\s*([a-z0-9_.+-]+@[a-z0-9-]+\\.[a-z0-9-.]{2,})\",\n#         r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\"\n#     ],\n#     \"phone\": [\n#         r\"(?i)(?:phone|mobile|tel):?\\s*(\\+?\\d[\\d\\s-]{7,15})\",\n#         r\"\\b\\+\\d{1,3}[- ]?\\(?\\d{3}\\)?[- ]?\\d{3}[- ]?\\d{4}\\b\"\n#     ],\n#     \"skill\": [\n#         r\"(?i)(?:skills|technical skills|competencies):?[\\s\\S]*?([-\\•].*?)(?=\\n\\s*\\n)\",\n#         r\"\\b(?:python|java|c\\+\\+)(?:\\s*\\d+\\.?\\d*)?\\b\",\n#         r\"\\b(?:aws|azure|gcp)(?:\\s*\\(?(?:ec2|s3|lambda)\\)?)?\\b\"\n#     ],\n#     \"experience\": [\n#         r\"(?i)(\\b(?:senior|junior)?\\s*(?:software engineer|data scientist)\\b.*?\\b(?:at|@)\\b.*?\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b\\s*\\d{4}.*?–.*?(?:present|\\d{4}))\",\n#         r\"-\\s*(?:led|developed|implemented|optimized)\\s+.*?\\b(?:using|with)\\b.*?\\b(?:python|machine learning|aws)\\b\"\n#     ],\n#     \"certification\": [\n#         r\"(?i)(?:certifications|licenses):?[\\s\\S]*?-\\s*(.*?)\\s*(?:\\(?(?:aws|microsoft|google)\\)?)\\s*(?:\\(\\d{4}\\))?\",\n#         r\"\\b(?:AWS Certified Solutions Architect|Google Cloud Professional|PMP®)\\b(?:\\s*–?\\s*\\d{4})?\"\n#     ]\n# }\n\n# ACTIONS = list(PATTERNS.keys()) + [\"next\"]\n\n# REWARDS = {\n#     \"correct\": 2.0,\n#     \"context_match\": 1.5,\n#     \"partial\": 0.5,\n#     \"incorrect\": -1.0,\n#     \"neutral\": -0.1,\n#     \"redundant\": -0.7\n# }\n\n# class EnhancedVectorizer(BaseEstimator, TransformerMixin):\n#     def __init__(self, max_features=100):\n#         self.vectorizer = CountVectorizer(max_features=max_features)\n        \n#     def fit(self, X, y=None):\n#         text_data = [x.split(\"[FEATURES_START]\")[0].strip() for x in X]\n#         self.vectorizer.fit(text_data)\n#         return self\n        \n#     def transform(self, X):\n#         text_data = [x.split(\"[FEATURES_START]\")[0].strip() for x in X]\n#         features = [json.loads(x.split(\"[FEATURES_START]\")[1].split(\"[FEATURES_END]\")[0]) for x in X]\n#         text_vectors = self.vectorizer.transform(text_data).toarray()\n#         feature_vectors = np.array([[f['has_date'], f['has_company'], f['has_title'], f['list_item']] for f in features])\n#         return np.concatenate([text_vectors, feature_vectors], axis=1)\n\n# class EnhancedResumeEnv:\n#     def __init__(self, resume_text):\n#         self.lines = [line.strip() for line in resume_text.split(\"\\n\") if line.strip()]\n#         self.current_line = 0\n#         self.extracted_data = defaultdict(list)\n        \n#     def get_state_representation(self, line):\n#         features = {\n#             'has_date': bool(re.search(r'\\d{4}', line)),\n#             'has_company': bool(re.search(r'\\b(?:at|@)\\b', line)),\n#             'has_title': bool(re.search(r'\\b(?:experienced|senior|junior)\\b', line)),\n#             'list_item': bool(re.search(r'^[-\\•]', line))\n#         }\n#         return f\"{line} [FEATURES_START]{json.dumps(features)}[FEATURES_END]\"\n    \n#     def reset(self):\n#         self.current_line = 0\n#         self.extracted_data = defaultdict(list)\n#         if self.lines:\n#             return self.get_state_representation(self.lines[0])\n#         return None\n    \n#     def step(self, action):\n#         try:\n#             if self.current_line >= len(self.lines):\n#                 return None, 0, True, self.extracted_data\n                \n#             raw_line = self.lines[self.current_line]\n#             processed_line = self.get_state_representation(raw_line)\n#             reward = REWARDS[\"neutral\"]\n#             done = False\n#             extracted = False\n            \n#             if action in PATTERNS:\n#                 # Check for section headers\n#                 if any(re.search(rf\"(?i)^{key}:?\", raw_line) for key in PATTERNS.keys()):\n#                     reward = REWARDS[\"context_match\"]\n                    \n#                 # Pattern matching\n#                 for pattern in PATTERNS[action]:\n#                     match = re.search(pattern, raw_line, re.IGNORECASE)\n#                     if match:\n#                         content = match.group(1) if len(match.groups()) > 0 else match.group()\n#                         if content not in self.extracted_data[action]:\n#                             self.extracted_data[action].append(content.strip())\n#                             reward = REWARDS[\"correct\"]\n#                         else:\n#                             reward = REWARDS[\"redundant\"]\n#                         extracted = True\n#                         break\n#                 if not extracted:\n#                     reward = REWARDS[\"incorrect\"]\n\n#             self.current_line += 1\n#             next_state = self.get_state_representation(self.lines[self.current_line]) if self.current_line < len(self.lines) else None\n#             done = self.current_line >= len(self.lines)\n            \n#             return next_state, reward, done, self.extracted_data\n#         except IndexError:\n#             return None, 0, True, self.extracted_data\n\n# class PolicyNetwork:\n#     def __init__(self, state_size, action_size):\n#         self.state_size = state_size\n#         self.action_size = action_size\n#         self.model = self._build_model()\n#         self.optimizer = Adam(0.001)\n        \n#     def _build_model(self):\n#         model = Sequential([\n#             Dense(64, activation='relu', input_shape=(self.state_size,)),\n#             Dense(64, activation='relu'),\n#             Dense(self.action_size, activation='softmax')\n#         ])\n#         return model\n    \n#     def choose_action(self, state):\n#         probs = self.model.predict(state, verbose=0)[0]\n#         return np.random.choice(self.action_size, p=probs)\n    \n#     def train(self, states, actions, rewards):\n#         with tf.GradientTape() as tape:\n#             probs = self.model(states, training=True)\n#             selected_probs = tf.reduce_sum(probs * tf.one_hot(actions, self.action_size), axis=1)\n#             loss = -tf.reduce_mean(tf.math.log(selected_probs) * rewards)\n#         grads = tape.gradient(loss, self.model.trainable_variables)\n#         self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n\n# # Load resume data\n# resume_data = {\n#     \"DESIGNER\": [\n#         {\n#             \"filename\": \"22506245.pdf\",\n#             \"content\": \"PRE-PRESS GRAPHIC DESIGNER\\nSummary\\nCreative, hardworking designer seeking a full-time desktop job, educated as a graphic artist, past experience in business world as a desktop\\npublisher laying out designs for printed mail and advertisements, in local government designing new websites with graphics for different agencies\\nwithin the system, and later for the same government printing and reproduction center creating documents to be printed off a press or copiers.\\nSkills\\nAdobe InDesign, Photoshop, Illustrator, and Acrobat Professional\\nStrongly familiar with Microsoft Word, Excel, PowerPoint, and Publisher / also QuarkXPress\\nBasic knowledge of web development with Adobe Dreamweaver, HTML, WordPress\\nAble to perform graphic design and administrative functions\\nAble to work as a team player and independently\\nExperienced using phone, fax, email, copiers and printers\\nProvides excellent customer service (in-person, by phone, email, or interoffice mail)\\nPrioritizes and calmly handles multiple projects and requests\\nListens to directions, takes notes for later reference, follows procedures\\nKnowledge of design setup on computer for jobs to be printed by outside vender or in-office copiers\\nExperience\\n01/2008\\n \\nto \\nCurrent\\nPre-Press Graphic Designer\\n \\nCompany Name\\n \\n\\u00ef\\u00bc\\u200b \\nCity\\n \\n, \\nState\\nCreate new designs for variety of items like manuals, newsletters, and posters.\\nUse templates for updated documents like envelopes, letterheads, and business cards.\\nProof jobs for initial and final customer approval.\\nManufactures a high-quality PDF file digitally for proofing, photocopying and offset printing.\\nPerforms file backup and organizes system for easy recovery.\\nMaintains and monitors supply inventory and orders items when needed.\\nOperates photocopying equipment, includes sending approved documents to printer.\\nAssists in the bindery department, using the folder and manual paper cutter for small jobs.\\nAlso can use bindery equipment, like the fastback and GBC binding of spines.\\nMounts and laminates to foam boards, manually trims to size.\\nEnsures timely submission of files to production.\\n04/2000\\n \\nto \\n01/2008\\nWeb Designer\\n \\nCompany Name\\n \\n\\u00ef\\u00bc\\u200b \\nCity\\n \\n, \\nState\\nCreated new sites and made updates to current sites; created graphics to use on web pages; scanned documents and converted digital files\\nfor links on sites; maintained updates and corrections on sites; answered email and phone call requests from departments about site changes;\\nproofed pages with emails before sending live to internet.\\n06/1998\\n \\nto \\n02/2000\\nDesktop Publisher\\n \\nCompany Name\\n \\n\\u00ef\\u00bc\\u200b \\nCity\\n \\n, \\nState\\nPerformed set-up and conversion of documents from Mac to PC then to UNIX systems; used QuarkXPress on Mac for the set-up of many\\njobs; sent to network to be used by programmers for \\\"targeted\\\" direct mail printouts; trained new team staff members; helped with clean-up\\nwhen company shut down.\\n06/1997\\n \\nto \\n03/2000\\nGraphic Designer\\n \\nCompany Name\\n \\n\\u00ef\\u00bc\\u200b \\nCity\\n \\n, \\nState\\nTemporary office jobs using Macintosh computers to design files to be printed for various companies like:.\\nAlltel Publishing.\\nCleveland School District.\\nHKM Marketing Communications.\\nNationwide Advertising.\\nEducation and Training\\nMay 1997\\nBachelor of Fine Arts\\n \\nAlfred University\\n \\n\\u00ef\\u00bc\\u200b \\nCity\\n \\n, \\nState\\nWork History\\nCompany Name\\nSkills\\nadministrative functions, Acrobat, Adobe Dreamweaver, Photoshop, Advertising, backup, Basic, business cards, conversion, excellent customer\\nservice, direct mail, email, fax, graphic design, graphics, HTML, Illustrator, Adobe InDesign, Mac, Macintosh computers, Marketing\\nCommunications, Excel, mail, office, PowerPoint, Publisher, Microsoft Word, monitors, network, newsletters, takes notes, PDF, copiers, posters,\\nprinter, printers, proofing, quality, QuarkXPress, supply inventory, team player, phone, UNIX, web development, web pages\\n\"\n#         }\n#     ]\n# }\n\n# # Proper data access pattern\n# vectorizer = EnhancedVectorizer(max_features=100)\n# all_lines = []\n# for job_category in resume_data.values():\n#     for resume in job_category:\n#         content = resume[\"content\"]\n#         env = EnhancedResumeEnv(content)\n#         line = env.reset()\n#         while line:\n#             all_lines.append(line)\n#             line, _, _, _ = env.step(\"next\")\n\n# vectorizer.fit(all_lines)\n\n# state_size = vectorizer.transform([all_lines[0]]).shape[1]\n# action_size = len(ACTIONS)\n# policy_net = PolicyNetwork(state_size, action_size)\n\n# # Training loop\n# for job, resumes in resume_data.items():\n#     for resume in resumes:\n#         env = EnhancedResumeEnv(resume[\"content\"])\n#         state = env.reset()\n        \n#         states, actions, rewards = [], [], []\n        \n#         while True:\n#             if state is None:\n#                 break\n                \n#             state_vec = vectorizer.transform([state])\n#             action = policy_net.choose_action(state_vec)\n#             next_state, reward, done, _ = env.step(ACTIONS[action])\n            \n#             states.append(state_vec)\n#             actions.append(action)\n#             rewards.append(reward)\n            \n#             if done or len(rewards) > 50:  # Early stopping\n#                 break\n#             state = next_state\n        \n#         # Compute discounted rewards\n#         if rewards:\n#             discounted = []\n#             cumulative = 0\n#             for r in reversed(rewards):\n#                 cumulative = cumulative * 0.99 + r\n#                 discounted.append(cumulative)\n#             discounted = (discounted - np.mean(discounted)) / (np.std(discounted) + 1e-8)\n            \n#             # Train\n#             policy_net.train(\n#                 np.vstack(states),\n#                 np.array(actions),\n#                 discounted\n#             )\n\n# # Testing\n# test_env = EnhancedResumeEnv(resume_data[\"DESIGNER\"][0][\"content\"])\n# state = test_env.reset()\n# extracted = defaultdict(list)\n\n# while state is not None:\n#     state_vec = vectorizer.transform([state])\n#     action = policy_net.choose_action(state_vec)\n#     state, _, _, data = test_env.step(ACTIONS[action])\n#     for k, v in data.items():\n#         extracted[k] = list(set(v))  # Deduplicate\n\n# print(\"Extracted Data:\")\n# for category, items in extracted.items():\n#     print(f\"{category.upper()}:\")\n#     for item in items:\n#         print(f\" - {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.585500Z","iopub.execute_input":"2025-03-17T14:40:15.585761Z","iopub.status.idle":"2025-03-17T14:40:15.596905Z","shell.execute_reply.started":"2025-03-17T14:40:15.585743Z","shell.execute_reply":"2025-03-17T14:40:15.596224Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import gymnasium as gym\n# from gymnasium import spaces\n# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n# from stable_baselines3 import PPO\n# from stable_baselines3.common.vec_env import DummyVecEnv\n# from collections import defaultdict\n# import re\n# import numpy as np\n\n# # Load pre-trained GPT-2 model and tokenizer\n# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# # Add a padding token to the tokenizer\n# if tokenizer.pad_token is None:\n#     tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n#     model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings for the new token\n\n# # Define the Gym-compatible environment\n# class ResumeParsingEnv(gym.Env):\n#     def __init__(self, resume_text):\n#         super(ResumeParsingEnv, self).__init__()\n#         self.lines = resume_text.split(\"\\n\")\n#         self.current_line = 0\n#         self.extracted_data = defaultdict(list)\n        \n#         # Define observation and action spaces\n#         self.observation_space = spaces.Box(\n#             low=0, high=tokenizer.vocab_size, shape=(100,), dtype=np.int32  # 100 tokens\n#         )\n#         self.action_space = spaces.Discrete(2)  # Example: 2 actions (extract or skip)\n\n#     def reset(self, seed=None, options=None):\n#         self.current_line = 0\n#         self.extracted_data = defaultdict(list)\n#         observation = self._get_observation()\n#         return observation, {}\n\n#     def step(self, action):\n#         line = self.lines[self.current_line] if self.current_line < len(self.lines) else \"\"\n#         reward = self._compute_reward(action, line)\n    \n#         self.current_line += 1\n#         done = self.current_line >= len(self.lines)\n        \n#         next_state = self._get_observation()\n    \n#         info = {}  # Additional metadata (can be empty)\n        \n#         return next_state, reward, done, False, info  # **Now returns 5 values**\n\n\n#     def _compute_reward(self, action, line):\n#         if action == 1 and re.search(r\"^[A-Z][a-z]+\\s[A-Z][a-z]+$\", line):  # Example: Extract name\n#             return 1.0\n#         return -1.0\n\n#     def _get_observation(self):\n#         # Ensure observation has a fixed shape (e.g., 100 elements)\n#         obs = np.zeros(100)  # Default to zero if there's no data\n        \n#         if self.current_line < len(self.lines):\n#             line_embedding = self._encode_text(self.lines[self.current_line])\n            \n#             # Ensure the embedding has the correct shape\n#             obs[:min(len(line_embedding), 100)] = line_embedding[:100]\n    \n#         return obs\n\n        \n# # Initialize environment and agent\n# resume_text = \"\"\"PRE-PRESS GRAPHIC DESIGNER\n# Summary\n# Creative, hardworking designer seeking full-time position...\n# Skills\n# - Adobe InDesign\n# - Photoshop\n# Experience\n# 01/2008 to Current: Graphic Designer at Company\n# \"\"\"\n\n# # Wrap the environment for PPO compatibility\n# env = DummyVecEnv([lambda: ResumeParsingEnv(resume_text)])\n# rl_agent = PPO(\"MlpPolicy\", env, verbose=1)\n\n# # Train the agent\n# rl_agent.learn(total_timesteps=2000)\n\n# # Test the agent\n# test_env = ResumeParsingEnv(resume_text)\n# observation, _ = test_env.reset()\n# while True:\n#     action, _ = rl_agent.predict(observation)\n#     observation, reward, done, truncated, info = test_env.step(action)\n#     if done:\n#         break\n# print(\"Extracted Data:\", test_env.extracted_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.597645Z","iopub.execute_input":"2025-03-17T14:40:15.597902Z","iopub.status.idle":"2025-03-17T14:40:15.611940Z","shell.execute_reply.started":"2025-03-17T14:40:15.597879Z","shell.execute_reply":"2025-03-17T14:40:15.611244Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Replace with the dataset name or path\ndataset_name = \"ganchengguang/resume_seven_class\"  # Example: \"imdb\", \"glue\", etc.\n\n# Load the dataset (some datasets may require specifying a split)\ndataset = load_dataset(dataset_name)\n\n# Print dataset information\nprint(dataset)\n\n# Display the first few rows of the dataset\nprint(dataset['train'][2])  # Change 'train' to the relevant split (e.g., 'test', 'validation')\ndata = dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:15.613877Z","iopub.execute_input":"2025-03-17T14:40:15.614088Z","iopub.status.idle":"2025-03-17T14:40:19.169682Z","shell.execute_reply.started":"2025-03-17T14:40:15.614071Z","shell.execute_reply":"2025-03-17T14:40:19.168827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/528 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c855afb524e4019bb3027d30e2aaf71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"resume.txt:   0%|          | 0.00/5.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef75edbfbf744a3ea7a5b6e4dbb4ae22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/78670 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f58e84e6f347ec8f24f78cd37c6c5b"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 78670\n    })\n})\n{'text': 'PI\\tPhone: 940-242-3303'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import csv\n\n# Read the dataset from a text file\ninput_file = \"/kaggle/input/resume/resume.txt\"  # Change this to the actual filename\noutput_file = \"processed_data.csv\"\n\ndata = []\n\n# Open the text file and process each line\nwith open(input_file, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        line = line.strip()\n        if not line:\n            continue  # Skip empty lines\n\n        # Split at the first tab character\n        parts = line.split(\"\\t\", 1)\n        if len(parts) == 2:\n            label, text = parts\n            data.append([label, text])\n\n# Save to a CSV file\nwith open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Label\", \"Text\"])  # Write header\n    writer.writerows(data)\n\nprint(f\"Processed data saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:19.171130Z","iopub.execute_input":"2025-03-17T14:40:19.171616Z","iopub.status.idle":"2025-03-17T14:40:19.542134Z","shell.execute_reply.started":"2025-03-17T14:40:19.171592Z","shell.execute_reply":"2025-03-17T14:40:19.541391Z"}},"outputs":[{"name":"stdout","text":"Processed data saved to processed_data.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define the model and tokenizer\nmodel_checkpoint = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Load dataset from CSV\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/working/processed_data.csv\")\n\n# Let's first examine the dataset structure\nprint(\"Dataset structure:\", dataset)\nprint(\"Column names:\", dataset[\"train\"].column_names)\nprint(\"Sample row:\", dataset[\"train\"][0])\n\n# Create a label mapping dictionary\n# Assuming your Label column contains categorical values like \"Exp\", \"Loc\", etc.\n# First, get unique labels\nunique_labels = set()\nfor example in dataset[\"train\"]:\n    unique_labels.add(example[\"Label\"])\nunique_labels = sorted(list(unique_labels))\nprint(f\"Unique labels found: {unique_labels}\")\n\n# Create mapping dictionary\nlabel_to_id = {label: i for i, label in enumerate(unique_labels)}\nid_to_label = {i: label for i, label in enumerate(unique_labels)}\nprint(f\"Label mapping: {label_to_id}\")\n\n# Now update the number of labels\nnum_labels = len(unique_labels)\nprint(f\"Number of labels: {num_labels}\")\n\n# Split dataset into train and validation sets\ndataset = dataset['train'].train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:19.542896Z","iopub.execute_input":"2025-03-17T14:40:19.543165Z","iopub.status.idle":"2025-03-17T14:40:42.463959Z","shell.execute_reply.started":"2025-03-17T14:40:19.543133Z","shell.execute_reply":"2025-03-17T14:40:42.463058Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ff5fcf91944fdca6b78f3b2c380fdd"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69217e1fb53e4434a55e29d0f3798ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27366ccdd1c44cf3bd73825e7e38886d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922de648c22345669eea32608a3ee642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc12c7f533f4e84951ed0f864104d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b44d94b10da43798fb1d3d542fc9038"}},"metadata":{}},{"name":"stdout","text":"Dataset structure: DatasetDict({\n    train: Dataset({\n        features: ['Label', 'Text'],\n        num_rows: 78669\n    })\n})\nColumn names: ['Label', 'Text']\nSample row: {'Label': 'Exp', 'Text': 'Name: Abiral Pandey'}\nUnique labels found: ['Edu', 'Exp', 'Obj', 'PI', 'QC', 'Skill', 'Sum']\nLabel mapping: {'Edu': 0, 'Exp': 1, 'Obj': 2, 'PI': 3, 'QC': 4, 'Skill': 5, 'Sum': 6}\nNumber of labels: 7\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\ndef tokenize_and_prepare(examples):\n    # Tokenize the texts\n    tokenized_inputs = tokenizer(\n        examples[\"Text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    \n    # Convert categorical labels to numeric labels using our mapping\n    tokenized_inputs[\"labels\"] = [label_to_id[label] for label in examples[\"Label\"]]\n    \n    return tokenized_inputs\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_and_prepare, batched=True)\n\n# Define compute_metrics function for evaluation\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n    acc = accuracy_score(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:40:42.464746Z","iopub.execute_input":"2025-03-17T14:40:42.465263Z","iopub.status.idle":"2025-03-17T14:41:06.300186Z","shell.execute_reply.started":"2025-03-17T14:40:42.465241Z","shell.execute_reply":"2025-03-17T14:41:06.299476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/70802 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf09783b940a421bb5856aad413620ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7867 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca0220a31b841f39ac183ee492231b8"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Define model - using SequenceClassification instead of TokenClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint, \n    num_labels=num_labels,\n    id2label=id_to_label,\n    label2id=label_to_id\n)\n\n# Move model to GPU if available\nmodel.to(device)\n\n# Training arguments with progress display\ntraining_args = TrainingArguments(\n    output_dir=\"./classification_model\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:41:06.301068Z","iopub.execute_input":"2025-03-17T14:41:06.301371Z","iopub.status.idle":"2025-03-17T14:41:09.001632Z","shell.execute_reply.started":"2025-03-17T14:41:06.301333Z","shell.execute_reply":"2025-03-17T14:41:09.000764Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0033fffa99664adba1ee8d27e5afc2e5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Custom callback to print progress during training\nfrom transformers import TrainerCallback\n\nclass EpochProgressCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if state.is_local_process_zero:\n            print(f\"\\n{'='*50}\")\n            print(f\"End of epoch {state.epoch}\")\n            print(f\"Steps completed: {state.global_step}\")\n            print(f\"{'='*50}\\n\")\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if state.is_local_process_zero and logs:\n            loss = logs.get('loss', None)\n            lr = logs.get('learning_rate', None)\n            step = state.global_step\n            \n            if loss is not None:\n                print(f\"Step {step}: Loss: {loss:.4f}\", end=\" \")\n            if lr is not None:\n                print(f\"LR: {lr:.2e}\", end=\" \")\n            if 'eval_loss' in logs:\n                print(f\"\\nEvaluation metrics: {logs}\")\n            else:\n                print()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:41:09.002509Z","iopub.execute_input":"2025-03-17T14:41:09.002868Z","iopub.status.idle":"2025-03-17T14:41:09.008783Z","shell.execute_reply.started":"2025-03-17T14:41:09.002805Z","shell.execute_reply":"2025-03-17T14:41:09.007947Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EpochProgressCallback()]\n)\n\n# Train model\nprint(\"Starting training...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving the model...\")\ntrainer.save_model(\"./classification_model_final\")\nprint(\"Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:41:09.009747Z","iopub.execute_input":"2025-03-17T14:41:09.010082Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-f177cf927f4d>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10659' max='13278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10659/13278 2:50:15 < 41:50, 1.04 it/s, Epoch 2.41/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.571500</td>\n      <td>0.450999</td>\n      <td>0.862082</td>\n      <td>0.858335</td>\n      <td>0.858262</td>\n      <td>0.862082</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.272000</td>\n      <td>0.446739</td>\n      <td>0.868056</td>\n      <td>0.867570</td>\n      <td>0.867355</td>\n      <td>0.868056</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step 10: Loss: 1.7597 LR: 2.00e-05 \nStep 20: Loss: 1.4818 LR: 2.00e-05 \nStep 30: Loss: 1.3016 LR: 2.00e-05 \nStep 40: Loss: 1.2815 LR: 1.99e-05 \nStep 50: Loss: 1.1281 LR: 1.99e-05 \nStep 60: Loss: 1.0653 LR: 1.99e-05 \nStep 70: Loss: 1.1103 LR: 1.99e-05 \nStep 80: Loss: 0.9353 LR: 1.99e-05 \nStep 90: Loss: 0.9740 LR: 1.99e-05 \nStep 100: Loss: 0.9767 LR: 1.98e-05 \nStep 110: Loss: 0.8084 LR: 1.98e-05 \nStep 120: Loss: 0.8760 LR: 1.98e-05 \nStep 130: Loss: 0.8447 LR: 1.98e-05 \nStep 140: Loss: 0.7957 LR: 1.98e-05 \nStep 150: Loss: 0.8862 LR: 1.98e-05 \nStep 160: Loss: 0.7822 LR: 1.98e-05 \nStep 170: Loss: 0.7690 LR: 1.97e-05 \nStep 180: Loss: 0.6229 LR: 1.97e-05 \nStep 190: Loss: 0.8209 LR: 1.97e-05 \nStep 200: Loss: 0.8482 LR: 1.97e-05 \nStep 210: Loss: 0.7628 LR: 1.97e-05 \nStep 220: Loss: 0.6699 LR: 1.97e-05 \nStep 230: Loss: 0.6547 LR: 1.97e-05 \nStep 240: Loss: 0.7764 LR: 1.96e-05 \nStep 250: Loss: 0.7434 LR: 1.96e-05 \nStep 260: Loss: 0.6963 LR: 1.96e-05 \nStep 270: Loss: 0.9167 LR: 1.96e-05 \nStep 280: Loss: 0.6249 LR: 1.96e-05 \nStep 290: Loss: 0.6791 LR: 1.96e-05 \nStep 300: Loss: 0.6665 LR: 1.95e-05 \nStep 310: Loss: 0.5647 LR: 1.95e-05 \nStep 320: Loss: 0.7037 LR: 1.95e-05 \nStep 330: Loss: 0.7507 LR: 1.95e-05 \nStep 340: Loss: 0.7879 LR: 1.95e-05 \nStep 350: Loss: 0.6608 LR: 1.95e-05 \nStep 360: Loss: 0.6230 LR: 1.95e-05 \nStep 370: Loss: 0.6336 LR: 1.94e-05 \nStep 380: Loss: 0.5824 LR: 1.94e-05 \nStep 390: Loss: 0.8244 LR: 1.94e-05 \nStep 400: Loss: 0.6960 LR: 1.94e-05 \nStep 410: Loss: 0.5113 LR: 1.94e-05 \nStep 420: Loss: 0.5615 LR: 1.94e-05 \nStep 430: Loss: 0.5086 LR: 1.94e-05 \nStep 440: Loss: 0.6175 LR: 1.93e-05 \nStep 450: Loss: 0.6821 LR: 1.93e-05 \nStep 460: Loss: 0.6250 LR: 1.93e-05 \nStep 470: Loss: 0.7754 LR: 1.93e-05 \nStep 480: Loss: 0.4587 LR: 1.93e-05 \nStep 490: Loss: 0.6447 LR: 1.93e-05 \nStep 500: Loss: 0.5945 LR: 1.92e-05 \nStep 510: Loss: 0.6819 LR: 1.92e-05 \nStep 520: Loss: 0.4560 LR: 1.92e-05 \nStep 530: Loss: 0.5690 LR: 1.92e-05 \nStep 540: Loss: 0.6787 LR: 1.92e-05 \nStep 550: Loss: 0.6267 LR: 1.92e-05 \nStep 560: Loss: 0.6089 LR: 1.92e-05 \nStep 570: Loss: 0.4876 LR: 1.91e-05 \nStep 580: Loss: 0.6137 LR: 1.91e-05 \nStep 590: Loss: 0.7689 LR: 1.91e-05 \nStep 600: Loss: 0.6324 LR: 1.91e-05 \nStep 610: Loss: 0.6919 LR: 1.91e-05 \nStep 620: Loss: 0.5303 LR: 1.91e-05 \nStep 630: Loss: 0.5704 LR: 1.91e-05 \nStep 640: Loss: 0.5928 LR: 1.90e-05 \nStep 650: Loss: 0.6254 LR: 1.90e-05 \nStep 660: Loss: 0.5342 LR: 1.90e-05 \nStep 670: Loss: 0.4884 LR: 1.90e-05 \nStep 680: Loss: 0.6613 LR: 1.90e-05 \nStep 690: Loss: 0.5329 LR: 1.90e-05 \nStep 700: Loss: 0.6022 LR: 1.89e-05 \nStep 710: Loss: 0.5016 LR: 1.89e-05 \nStep 720: Loss: 0.4446 LR: 1.89e-05 \nStep 730: Loss: 0.4788 LR: 1.89e-05 \nStep 740: Loss: 0.6662 LR: 1.89e-05 \nStep 750: Loss: 0.5034 LR: 1.89e-05 \nStep 760: Loss: 0.6373 LR: 1.89e-05 \nStep 770: Loss: 0.5443 LR: 1.88e-05 \nStep 780: Loss: 0.6025 LR: 1.88e-05 \nStep 790: Loss: 0.4881 LR: 1.88e-05 \nStep 800: Loss: 0.5716 LR: 1.88e-05 \nStep 810: Loss: 0.5244 LR: 1.88e-05 \nStep 820: Loss: 0.6633 LR: 1.88e-05 \nStep 830: Loss: 0.6765 LR: 1.87e-05 \nStep 840: Loss: 0.5806 LR: 1.87e-05 \nStep 850: Loss: 0.5324 LR: 1.87e-05 \nStep 860: Loss: 0.6327 LR: 1.87e-05 \nStep 870: Loss: 0.5504 LR: 1.87e-05 \nStep 880: Loss: 0.5259 LR: 1.87e-05 \nStep 890: Loss: 0.6179 LR: 1.87e-05 \nStep 900: Loss: 0.5697 LR: 1.86e-05 \nStep 910: Loss: 0.4945 LR: 1.86e-05 \nStep 920: Loss: 0.7231 LR: 1.86e-05 \nStep 930: Loss: 0.6027 LR: 1.86e-05 \nStep 940: Loss: 0.4908 LR: 1.86e-05 \nStep 950: Loss: 0.4421 LR: 1.86e-05 \nStep 960: Loss: 0.7070 LR: 1.86e-05 \nStep 970: Loss: 0.5451 LR: 1.85e-05 \nStep 980: Loss: 0.4542 LR: 1.85e-05 \nStep 990: Loss: 0.5927 LR: 1.85e-05 \nStep 1000: Loss: 0.6378 LR: 1.85e-05 \nStep 1010: Loss: 0.6396 LR: 1.85e-05 \nStep 1020: Loss: 0.5454 LR: 1.85e-05 \nStep 1030: Loss: 0.5593 LR: 1.84e-05 \nStep 1040: Loss: 0.6236 LR: 1.84e-05 \nStep 1050: Loss: 0.5465 LR: 1.84e-05 \nStep 1060: Loss: 0.5477 LR: 1.84e-05 \nStep 1070: Loss: 0.5983 LR: 1.84e-05 \nStep 1080: Loss: 0.4455 LR: 1.84e-05 \nStep 1090: Loss: 0.3970 LR: 1.84e-05 \nStep 1100: Loss: 0.5201 LR: 1.83e-05 \nStep 1110: Loss: 0.6306 LR: 1.83e-05 \nStep 1120: Loss: 0.5568 LR: 1.83e-05 \nStep 1130: Loss: 0.5908 LR: 1.83e-05 \nStep 1140: Loss: 0.5653 LR: 1.83e-05 \nStep 1150: Loss: 0.6115 LR: 1.83e-05 \nStep 1160: Loss: 0.5111 LR: 1.83e-05 \nStep 1170: Loss: 0.6790 LR: 1.82e-05 \nStep 1180: Loss: 0.4796 LR: 1.82e-05 \nStep 1190: Loss: 0.6554 LR: 1.82e-05 \nStep 1200: Loss: 0.4503 LR: 1.82e-05 \nStep 1210: Loss: 0.6185 LR: 1.82e-05 \nStep 1220: Loss: 0.4913 LR: 1.82e-05 \nStep 1230: Loss: 0.5840 LR: 1.81e-05 \nStep 1240: Loss: 0.6701 LR: 1.81e-05 \nStep 1250: Loss: 0.4826 LR: 1.81e-05 \nStep 1260: Loss: 0.5838 LR: 1.81e-05 \nStep 1270: Loss: 0.3486 LR: 1.81e-05 \nStep 1280: Loss: 0.5257 LR: 1.81e-05 \nStep 1290: Loss: 0.5122 LR: 1.81e-05 \nStep 1300: Loss: 0.5344 LR: 1.80e-05 \nStep 1310: Loss: 0.4504 LR: 1.80e-05 \nStep 1320: Loss: 0.4301 LR: 1.80e-05 \nStep 1330: Loss: 0.7377 LR: 1.80e-05 \nStep 1340: Loss: 0.5219 LR: 1.80e-05 \nStep 1350: Loss: 0.6014 LR: 1.80e-05 \nStep 1360: Loss: 0.5380 LR: 1.80e-05 \nStep 1370: Loss: 0.6334 LR: 1.79e-05 \nStep 1380: Loss: 0.5150 LR: 1.79e-05 \nStep 1390: Loss: 0.4652 LR: 1.79e-05 \nStep 1400: Loss: 0.6169 LR: 1.79e-05 \nStep 1410: Loss: 0.4349 LR: 1.79e-05 \nStep 1420: Loss: 0.4458 LR: 1.79e-05 \nStep 1430: Loss: 0.5848 LR: 1.78e-05 \nStep 1440: Loss: 0.5261 LR: 1.78e-05 \nStep 1450: Loss: 0.3586 LR: 1.78e-05 \nStep 1460: Loss: 0.6124 LR: 1.78e-05 \nStep 1470: Loss: 0.4960 LR: 1.78e-05 \nStep 1480: Loss: 0.5514 LR: 1.78e-05 \nStep 1490: Loss: 0.5287 LR: 1.78e-05 \nStep 1500: Loss: 0.6066 LR: 1.77e-05 \nStep 1510: Loss: 0.6207 LR: 1.77e-05 \nStep 1520: Loss: 0.5676 LR: 1.77e-05 \nStep 1530: Loss: 0.5223 LR: 1.77e-05 \nStep 1540: Loss: 0.6843 LR: 1.77e-05 \nStep 1550: Loss: 0.3831 LR: 1.77e-05 \nStep 1560: Loss: 0.4855 LR: 1.77e-05 \nStep 1570: Loss: 0.4073 LR: 1.76e-05 \nStep 1580: Loss: 0.5048 LR: 1.76e-05 \nStep 1590: Loss: 0.5756 LR: 1.76e-05 \nStep 1600: Loss: 0.4998 LR: 1.76e-05 \nStep 1610: Loss: 0.5208 LR: 1.76e-05 \nStep 1620: Loss: 0.6528 LR: 1.76e-05 \nStep 1630: Loss: 0.4865 LR: 1.75e-05 \nStep 1640: Loss: 0.5101 LR: 1.75e-05 \nStep 1650: Loss: 0.6437 LR: 1.75e-05 \nStep 1660: Loss: 0.5523 LR: 1.75e-05 \nStep 1670: Loss: 0.4156 LR: 1.75e-05 \nStep 1680: Loss: 0.5594 LR: 1.75e-05 \nStep 1690: Loss: 0.6083 LR: 1.75e-05 \nStep 1700: Loss: 0.5087 LR: 1.74e-05 \nStep 1710: Loss: 0.5479 LR: 1.74e-05 \nStep 1720: Loss: 0.6387 LR: 1.74e-05 \nStep 1730: Loss: 0.4941 LR: 1.74e-05 \nStep 1740: Loss: 0.5152 LR: 1.74e-05 \nStep 1750: Loss: 0.6104 LR: 1.74e-05 \nStep 1760: Loss: 0.6211 LR: 1.73e-05 \nStep 1770: Loss: 0.3891 LR: 1.73e-05 \nStep 1780: Loss: 0.6168 LR: 1.73e-05 \nStep 1790: Loss: 0.5788 LR: 1.73e-05 \nStep 1800: Loss: 0.5498 LR: 1.73e-05 \nStep 1810: Loss: 0.6950 LR: 1.73e-05 \nStep 1820: Loss: 0.4086 LR: 1.73e-05 \nStep 1830: Loss: 0.5395 LR: 1.72e-05 \nStep 1840: Loss: 0.7137 LR: 1.72e-05 \nStep 1850: Loss: 0.4640 LR: 1.72e-05 \nStep 1860: Loss: 0.6157 LR: 1.72e-05 \nStep 1870: Loss: 0.5844 LR: 1.72e-05 \nStep 1880: Loss: 0.4605 LR: 1.72e-05 \nStep 1890: Loss: 0.5065 LR: 1.72e-05 \nStep 1900: Loss: 0.5310 LR: 1.71e-05 \nStep 1910: Loss: 0.4862 LR: 1.71e-05 \nStep 1920: Loss: 0.4853 LR: 1.71e-05 \nStep 1930: Loss: 0.4686 LR: 1.71e-05 \nStep 1940: Loss: 0.4568 LR: 1.71e-05 \nStep 1950: Loss: 0.5319 LR: 1.71e-05 \nStep 1960: Loss: 0.5805 LR: 1.70e-05 \nStep 1970: Loss: 0.5047 LR: 1.70e-05 \nStep 1980: Loss: 0.6120 LR: 1.70e-05 \nStep 1990: Loss: 0.5092 LR: 1.70e-05 \nStep 2000: Loss: 0.4772 LR: 1.70e-05 \nStep 2010: Loss: 0.3952 LR: 1.70e-05 \nStep 2020: Loss: 0.4160 LR: 1.70e-05 \nStep 2030: Loss: 0.4942 LR: 1.69e-05 \nStep 2040: Loss: 0.4197 LR: 1.69e-05 \nStep 2050: Loss: 0.4154 LR: 1.69e-05 \nStep 2060: Loss: 0.5660 LR: 1.69e-05 \nStep 2070: Loss: 0.5253 LR: 1.69e-05 \nStep 2080: Loss: 0.4518 LR: 1.69e-05 \nStep 2090: Loss: 0.4055 LR: 1.69e-05 \nStep 2100: Loss: 0.4557 LR: 1.68e-05 \nStep 2110: Loss: 0.4862 LR: 1.68e-05 \nStep 2120: Loss: 0.4212 LR: 1.68e-05 \nStep 2130: Loss: 0.4263 LR: 1.68e-05 \nStep 2140: Loss: 0.5357 LR: 1.68e-05 \nStep 2150: Loss: 0.5802 LR: 1.68e-05 \nStep 2160: Loss: 0.4568 LR: 1.67e-05 \nStep 2170: Loss: 0.4553 LR: 1.67e-05 \nStep 2180: Loss: 0.4619 LR: 1.67e-05 \nStep 2190: Loss: 0.4897 LR: 1.67e-05 \nStep 2200: Loss: 0.4146 LR: 1.67e-05 \nStep 2210: Loss: 0.4671 LR: 1.67e-05 \nStep 2220: Loss: 0.6514 LR: 1.67e-05 \nStep 2230: Loss: 0.5627 LR: 1.66e-05 \nStep 2240: Loss: 0.4846 LR: 1.66e-05 \nStep 2250: Loss: 0.4405 LR: 1.66e-05 \nStep 2260: Loss: 0.5649 LR: 1.66e-05 \nStep 2270: Loss: 0.5557 LR: 1.66e-05 \nStep 2280: Loss: 0.5526 LR: 1.66e-05 \nStep 2290: Loss: 0.4360 LR: 1.66e-05 \nStep 2300: Loss: 0.4015 LR: 1.65e-05 \nStep 2310: Loss: 0.4753 LR: 1.65e-05 \nStep 2320: Loss: 0.5084 LR: 1.65e-05 \nStep 2330: Loss: 0.6407 LR: 1.65e-05 \nStep 2340: Loss: 0.5915 LR: 1.65e-05 \nStep 2350: Loss: 0.5128 LR: 1.65e-05 \nStep 2360: Loss: 0.6628 LR: 1.64e-05 \nStep 2370: Loss: 0.6420 LR: 1.64e-05 \nStep 2380: Loss: 0.5269 LR: 1.64e-05 \nStep 2390: Loss: 0.5872 LR: 1.64e-05 \nStep 2400: Loss: 0.5294 LR: 1.64e-05 \nStep 2410: Loss: 0.4030 LR: 1.64e-05 \nStep 2420: Loss: 0.4614 LR: 1.64e-05 \nStep 2430: Loss: 0.4793 LR: 1.63e-05 \nStep 2440: Loss: 0.4787 LR: 1.63e-05 \nStep 2450: Loss: 0.4738 LR: 1.63e-05 \nStep 2460: Loss: 0.3973 LR: 1.63e-05 \nStep 2470: Loss: 0.4418 LR: 1.63e-05 \nStep 2480: Loss: 0.6341 LR: 1.63e-05 \nStep 2490: Loss: 0.4884 LR: 1.62e-05 \nStep 2500: Loss: 0.4352 LR: 1.62e-05 \nStep 2510: Loss: 0.4024 LR: 1.62e-05 \nStep 2520: Loss: 0.4076 LR: 1.62e-05 \nStep 2530: Loss: 0.4438 LR: 1.62e-05 \nStep 2540: Loss: 0.3931 LR: 1.62e-05 \nStep 2550: Loss: 0.4972 LR: 1.62e-05 \nStep 2560: Loss: 0.5946 LR: 1.61e-05 \nStep 2570: Loss: 0.4816 LR: 1.61e-05 \nStep 2580: Loss: 0.4540 LR: 1.61e-05 \nStep 2590: Loss: 0.5926 LR: 1.61e-05 \nStep 2600: Loss: 0.5135 LR: 1.61e-05 \nStep 2610: Loss: 0.4171 LR: 1.61e-05 \nStep 2620: Loss: 0.5305 LR: 1.61e-05 \nStep 2630: Loss: 0.4191 LR: 1.60e-05 \nStep 2640: Loss: 0.4811 LR: 1.60e-05 \nStep 2650: Loss: 0.3594 LR: 1.60e-05 \nStep 2660: Loss: 0.5121 LR: 1.60e-05 \nStep 2670: Loss: 0.4925 LR: 1.60e-05 \nStep 2680: Loss: 0.5912 LR: 1.60e-05 \nStep 2690: Loss: 0.4035 LR: 1.59e-05 \nStep 2700: Loss: 0.5181 LR: 1.59e-05 \nStep 2710: Loss: 0.5677 LR: 1.59e-05 \nStep 2720: Loss: 0.4135 LR: 1.59e-05 \nStep 2730: Loss: 0.4880 LR: 1.59e-05 \nStep 2740: Loss: 0.4303 LR: 1.59e-05 \nStep 2750: Loss: 0.5013 LR: 1.59e-05 \nStep 2760: Loss: 0.2405 LR: 1.58e-05 \nStep 2770: Loss: 0.5990 LR: 1.58e-05 \nStep 2780: Loss: 0.6175 LR: 1.58e-05 \nStep 2790: Loss: 0.4590 LR: 1.58e-05 \nStep 2800: Loss: 0.4751 LR: 1.58e-05 \nStep 2810: Loss: 0.6482 LR: 1.58e-05 \nStep 2820: Loss: 0.3869 LR: 1.58e-05 \nStep 2830: Loss: 0.3849 LR: 1.57e-05 \nStep 2840: Loss: 0.5790 LR: 1.57e-05 \nStep 2850: Loss: 0.5206 LR: 1.57e-05 \nStep 2860: Loss: 0.5186 LR: 1.57e-05 \nStep 2870: Loss: 0.6418 LR: 1.57e-05 \nStep 2880: Loss: 0.4451 LR: 1.57e-05 \nStep 2890: Loss: 0.4219 LR: 1.56e-05 \nStep 2900: Loss: 0.5570 LR: 1.56e-05 \nStep 2910: Loss: 0.4243 LR: 1.56e-05 \nStep 2920: Loss: 0.4652 LR: 1.56e-05 \nStep 2930: Loss: 0.4923 LR: 1.56e-05 \nStep 2940: Loss: 0.5664 LR: 1.56e-05 \nStep 2950: Loss: 0.6025 LR: 1.56e-05 \nStep 2960: Loss: 0.4621 LR: 1.55e-05 \nStep 2970: Loss: 0.4497 LR: 1.55e-05 \nStep 2980: Loss: 0.4364 LR: 1.55e-05 \nStep 2990: Loss: 0.5669 LR: 1.55e-05 \nStep 3000: Loss: 0.4380 LR: 1.55e-05 \nStep 3010: Loss: 0.4483 LR: 1.55e-05 \nStep 3020: Loss: 0.5055 LR: 1.55e-05 \nStep 3030: Loss: 0.5398 LR: 1.54e-05 \nStep 3040: Loss: 0.4870 LR: 1.54e-05 \nStep 3050: Loss: 0.3362 LR: 1.54e-05 \nStep 3060: Loss: 0.4258 LR: 1.54e-05 \nStep 3070: Loss: 0.4518 LR: 1.54e-05 \nStep 3080: Loss: 0.5670 LR: 1.54e-05 \nStep 3090: Loss: 0.4678 LR: 1.53e-05 \nStep 3100: Loss: 0.6712 LR: 1.53e-05 \nStep 3110: Loss: 0.5031 LR: 1.53e-05 \nStep 3120: Loss: 0.5552 LR: 1.53e-05 \nStep 3130: Loss: 0.5011 LR: 1.53e-05 \nStep 3140: Loss: 0.6342 LR: 1.53e-05 \nStep 3150: Loss: 0.5074 LR: 1.53e-05 \nStep 3160: Loss: 0.5377 LR: 1.52e-05 \nStep 3170: Loss: 0.4413 LR: 1.52e-05 \nStep 3180: Loss: 0.3999 LR: 1.52e-05 \nStep 3190: Loss: 0.5389 LR: 1.52e-05 \nStep 3200: Loss: 0.6298 LR: 1.52e-05 \nStep 3210: Loss: 0.4845 LR: 1.52e-05 \nStep 3220: Loss: 0.5875 LR: 1.51e-05 \nStep 3230: Loss: 0.4585 LR: 1.51e-05 \nStep 3240: Loss: 0.4468 LR: 1.51e-05 \nStep 3250: Loss: 0.5179 LR: 1.51e-05 \nStep 3260: Loss: 0.4365 LR: 1.51e-05 \nStep 3270: Loss: 0.6058 LR: 1.51e-05 \nStep 3280: Loss: 0.5578 LR: 1.51e-05 \nStep 3290: Loss: 0.4484 LR: 1.50e-05 \nStep 3300: Loss: 0.4248 LR: 1.50e-05 \nStep 3310: Loss: 0.4432 LR: 1.50e-05 \nStep 3320: Loss: 0.4196 LR: 1.50e-05 \nStep 3330: Loss: 0.6210 LR: 1.50e-05 \nStep 3340: Loss: 0.4508 LR: 1.50e-05 \nStep 3350: Loss: 0.5925 LR: 1.50e-05 \nStep 3360: Loss: 0.4462 LR: 1.49e-05 \nStep 3370: Loss: 0.4792 LR: 1.49e-05 \nStep 3380: Loss: 0.3795 LR: 1.49e-05 \nStep 3390: Loss: 0.4554 LR: 1.49e-05 \nStep 3400: Loss: 0.4738 LR: 1.49e-05 \nStep 3410: Loss: 0.5256 LR: 1.49e-05 \nStep 3420: Loss: 0.4242 LR: 1.48e-05 \nStep 3430: Loss: 0.4349 LR: 1.48e-05 \nStep 3440: Loss: 0.3653 LR: 1.48e-05 \nStep 3450: Loss: 0.4863 LR: 1.48e-05 \nStep 3460: Loss: 0.5988 LR: 1.48e-05 \nStep 3470: Loss: 0.5139 LR: 1.48e-05 \nStep 3480: Loss: 0.3409 LR: 1.48e-05 \nStep 3490: Loss: 0.5175 LR: 1.47e-05 \nStep 3500: Loss: 0.4561 LR: 1.47e-05 \nStep 3510: Loss: 0.3709 LR: 1.47e-05 \nStep 3520: Loss: 0.7132 LR: 1.47e-05 \nStep 3530: Loss: 0.4424 LR: 1.47e-05 \nStep 3540: Loss: 0.4156 LR: 1.47e-05 \nStep 3550: Loss: 0.4961 LR: 1.47e-05 \nStep 3560: Loss: 0.5683 LR: 1.46e-05 \nStep 3570: Loss: 0.4264 LR: 1.46e-05 \nStep 3580: Loss: 0.5086 LR: 1.46e-05 \nStep 3590: Loss: 0.4419 LR: 1.46e-05 \nStep 3600: Loss: 0.4308 LR: 1.46e-05 \nStep 3610: Loss: 0.6161 LR: 1.46e-05 \nStep 3620: Loss: 0.3560 LR: 1.45e-05 \nStep 3630: Loss: 0.5645 LR: 1.45e-05 \nStep 3640: Loss: 0.3925 LR: 1.45e-05 \nStep 3650: Loss: 0.2895 LR: 1.45e-05 \nStep 3660: Loss: 0.4300 LR: 1.45e-05 \nStep 3670: Loss: 0.4798 LR: 1.45e-05 \nStep 3680: Loss: 0.3469 LR: 1.45e-05 \nStep 3690: Loss: 0.4415 LR: 1.44e-05 \nStep 3700: Loss: 0.4838 LR: 1.44e-05 \nStep 3710: Loss: 0.3823 LR: 1.44e-05 \nStep 3720: Loss: 0.5670 LR: 1.44e-05 \nStep 3730: Loss: 0.4074 LR: 1.44e-05 \nStep 3740: Loss: 0.4543 LR: 1.44e-05 \nStep 3750: Loss: 0.4035 LR: 1.44e-05 \nStep 3760: Loss: 0.4956 LR: 1.43e-05 \nStep 3770: Loss: 0.4676 LR: 1.43e-05 \nStep 3780: Loss: 0.3754 LR: 1.43e-05 \nStep 3790: Loss: 0.6014 LR: 1.43e-05 \nStep 3800: Loss: 0.5098 LR: 1.43e-05 \nStep 3810: Loss: 0.5661 LR: 1.43e-05 \nStep 3820: Loss: 0.4698 LR: 1.42e-05 \nStep 3830: Loss: 0.4341 LR: 1.42e-05 \nStep 3840: Loss: 0.5462 LR: 1.42e-05 \nStep 3850: Loss: 0.4236 LR: 1.42e-05 \nStep 3860: Loss: 0.5548 LR: 1.42e-05 \nStep 3870: Loss: 0.4653 LR: 1.42e-05 \nStep 3880: Loss: 0.5676 LR: 1.42e-05 \nStep 3890: Loss: 0.5644 LR: 1.41e-05 \nStep 3900: Loss: 0.3480 LR: 1.41e-05 \nStep 3910: Loss: 0.7159 LR: 1.41e-05 \nStep 3920: Loss: 0.4348 LR: 1.41e-05 \nStep 3930: Loss: 0.4542 LR: 1.41e-05 \nStep 3940: Loss: 0.6197 LR: 1.41e-05 \nStep 3950: Loss: 0.5775 LR: 1.41e-05 \nStep 3960: Loss: 0.6001 LR: 1.40e-05 \nStep 3970: Loss: 0.4906 LR: 1.40e-05 \nStep 3980: Loss: 0.3344 LR: 1.40e-05 \nStep 3990: Loss: 0.3460 LR: 1.40e-05 \nStep 4000: Loss: 0.4456 LR: 1.40e-05 \nStep 4010: Loss: 0.3275 LR: 1.40e-05 \nStep 4020: Loss: 0.4695 LR: 1.39e-05 \nStep 4030: Loss: 0.4726 LR: 1.39e-05 \nStep 4040: Loss: 0.5637 LR: 1.39e-05 \nStep 4050: Loss: 0.5393 LR: 1.39e-05 \nStep 4060: Loss: 0.4726 LR: 1.39e-05 \nStep 4070: Loss: 0.6327 LR: 1.39e-05 \nStep 4080: Loss: 0.5075 LR: 1.39e-05 \nStep 4090: Loss: 0.5592 LR: 1.38e-05 \nStep 4100: Loss: 0.4436 LR: 1.38e-05 \nStep 4110: Loss: 0.4382 LR: 1.38e-05 \nStep 4120: Loss: 0.3560 LR: 1.38e-05 \nStep 4130: Loss: 0.6553 LR: 1.38e-05 \nStep 4140: Loss: 0.4805 LR: 1.38e-05 \nStep 4150: Loss: 0.4974 LR: 1.37e-05 \nStep 4160: Loss: 0.3329 LR: 1.37e-05 \nStep 4170: Loss: 0.5125 LR: 1.37e-05 \nStep 4180: Loss: 0.4597 LR: 1.37e-05 \nStep 4190: Loss: 0.4073 LR: 1.37e-05 \nStep 4200: Loss: 0.4895 LR: 1.37e-05 \nStep 4210: Loss: 0.4892 LR: 1.37e-05 \nStep 4220: Loss: 0.6077 LR: 1.36e-05 \nStep 4230: Loss: 0.5567 LR: 1.36e-05 \nStep 4240: Loss: 0.3825 LR: 1.36e-05 \nStep 4250: Loss: 0.4818 LR: 1.36e-05 \nStep 4260: Loss: 0.5191 LR: 1.36e-05 \nStep 4270: Loss: 0.5335 LR: 1.36e-05 \nStep 4280: Loss: 0.4936 LR: 1.36e-05 \nStep 4290: Loss: 0.3925 LR: 1.35e-05 \nStep 4300: Loss: 0.4601 LR: 1.35e-05 \nStep 4310: Loss: 0.5211 LR: 1.35e-05 \nStep 4320: Loss: 0.4146 LR: 1.35e-05 \nStep 4330: Loss: 0.4521 LR: 1.35e-05 \nStep 4340: Loss: 0.4551 LR: 1.35e-05 \nStep 4350: Loss: 0.5501 LR: 1.34e-05 \nStep 4360: Loss: 0.4240 LR: 1.34e-05 \nStep 4370: Loss: 0.5750 LR: 1.34e-05 \nStep 4380: Loss: 0.6957 LR: 1.34e-05 \nStep 4390: Loss: 0.5130 LR: 1.34e-05 \nStep 4400: Loss: 0.5831 LR: 1.34e-05 \nStep 4410: Loss: 0.4306 LR: 1.34e-05 \nStep 4420: Loss: 0.5715 LR: 1.33e-05 \n\n==================================================\nEnd of epoch 1.0\nSteps completed: 4426\n==================================================\n\n\nEvaluation metrics: {'eval_loss': 0.45099860429763794, 'eval_accuracy': 0.8620821151646116, 'eval_f1': 0.8583345352965058, 'eval_precision': 0.8582620737617493, 'eval_recall': 0.8620821151646116, 'eval_runtime': 152.0554, 'eval_samples_per_second': 51.738, 'eval_steps_per_second': 3.236, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 4430: Loss: 0.3571 LR: 1.33e-05 \nStep 4440: Loss: 0.4932 LR: 1.33e-05 \nStep 4450: Loss: 0.4606 LR: 1.33e-05 \nStep 4460: Loss: 0.4001 LR: 1.33e-05 \nStep 4470: Loss: 0.3518 LR: 1.33e-05 \nStep 4480: Loss: 0.3432 LR: 1.33e-05 \nStep 4490: Loss: 0.4420 LR: 1.32e-05 \nStep 4500: Loss: 0.4406 LR: 1.32e-05 \nStep 4510: Loss: 0.5070 LR: 1.32e-05 \nStep 4520: Loss: 0.3711 LR: 1.32e-05 \nStep 4530: Loss: 0.4716 LR: 1.32e-05 \nStep 4540: Loss: 0.2981 LR: 1.32e-05 \nStep 4550: Loss: 0.3158 LR: 1.31e-05 \nStep 4560: Loss: 0.3180 LR: 1.31e-05 \nStep 4570: Loss: 0.4814 LR: 1.31e-05 \nStep 4580: Loss: 0.4412 LR: 1.31e-05 \nStep 4590: Loss: 0.3783 LR: 1.31e-05 \nStep 4600: Loss: 0.3767 LR: 1.31e-05 \nStep 4610: Loss: 0.3995 LR: 1.31e-05 \nStep 4620: Loss: 0.3194 LR: 1.30e-05 \nStep 4630: Loss: 0.3817 LR: 1.30e-05 \nStep 4640: Loss: 0.4399 LR: 1.30e-05 \nStep 4650: Loss: 0.4004 LR: 1.30e-05 \nStep 4660: Loss: 0.4237 LR: 1.30e-05 \nStep 4670: Loss: 0.3945 LR: 1.30e-05 \nStep 4680: Loss: 0.3528 LR: 1.30e-05 \nStep 4690: Loss: 0.4526 LR: 1.29e-05 \nStep 4700: Loss: 0.4411 LR: 1.29e-05 \nStep 4710: Loss: 0.3148 LR: 1.29e-05 \nStep 4720: Loss: 0.5182 LR: 1.29e-05 \nStep 4730: Loss: 0.3901 LR: 1.29e-05 \nStep 4740: Loss: 0.4007 LR: 1.29e-05 \nStep 4750: Loss: 0.5367 LR: 1.28e-05 \nStep 4760: Loss: 0.4049 LR: 1.28e-05 \nStep 4770: Loss: 0.5109 LR: 1.28e-05 \nStep 4780: Loss: 0.5887 LR: 1.28e-05 \nStep 4790: Loss: 0.3082 LR: 1.28e-05 \nStep 4800: Loss: 0.4701 LR: 1.28e-05 \nStep 4810: Loss: 0.3842 LR: 1.28e-05 \nStep 4820: Loss: 0.3274 LR: 1.27e-05 \nStep 4830: Loss: 0.3539 LR: 1.27e-05 \nStep 4840: Loss: 0.4611 LR: 1.27e-05 \nStep 4850: Loss: 0.4132 LR: 1.27e-05 \nStep 4860: Loss: 0.4008 LR: 1.27e-05 \nStep 4870: Loss: 0.4306 LR: 1.27e-05 \nStep 4880: Loss: 0.3170 LR: 1.26e-05 \nStep 4890: Loss: 0.3994 LR: 1.26e-05 \nStep 4900: Loss: 0.4744 LR: 1.26e-05 \nStep 4910: Loss: 0.4879 LR: 1.26e-05 \nStep 4920: Loss: 0.3727 LR: 1.26e-05 \nStep 4930: Loss: 0.5022 LR: 1.26e-05 \nStep 4940: Loss: 0.2808 LR: 1.26e-05 \nStep 4950: Loss: 0.3461 LR: 1.25e-05 \nStep 4960: Loss: 0.5403 LR: 1.25e-05 \nStep 4970: Loss: 0.3287 LR: 1.25e-05 \nStep 4980: Loss: 0.3352 LR: 1.25e-05 \nStep 4990: Loss: 0.4394 LR: 1.25e-05 \nStep 5000: Loss: 0.3780 LR: 1.25e-05 \nStep 5010: Loss: 0.3392 LR: 1.25e-05 \nStep 5020: Loss: 0.4579 LR: 1.24e-05 \nStep 5030: Loss: 0.3149 LR: 1.24e-05 \nStep 5040: Loss: 0.4547 LR: 1.24e-05 \nStep 5050: Loss: 0.3773 LR: 1.24e-05 \nStep 5060: Loss: 0.5658 LR: 1.24e-05 \nStep 5070: Loss: 0.4586 LR: 1.24e-05 \nStep 5080: Loss: 0.2594 LR: 1.23e-05 \nStep 5090: Loss: 0.5545 LR: 1.23e-05 \nStep 5100: Loss: 0.3116 LR: 1.23e-05 \nStep 5110: Loss: 0.4982 LR: 1.23e-05 \nStep 5120: Loss: 0.4498 LR: 1.23e-05 \nStep 5130: Loss: 0.3803 LR: 1.23e-05 \nStep 5140: Loss: 0.4547 LR: 1.23e-05 \nStep 5150: Loss: 0.3366 LR: 1.22e-05 \nStep 5160: Loss: 0.3167 LR: 1.22e-05 \nStep 5170: Loss: 0.5626 LR: 1.22e-05 \nStep 5180: Loss: 0.3806 LR: 1.22e-05 \nStep 5190: Loss: 0.2617 LR: 1.22e-05 \nStep 5200: Loss: 0.3371 LR: 1.22e-05 \nStep 5210: Loss: 0.3364 LR: 1.22e-05 \nStep 5220: Loss: 0.3114 LR: 1.21e-05 \nStep 5230: Loss: 0.3718 LR: 1.21e-05 \nStep 5240: Loss: 0.4419 LR: 1.21e-05 \nStep 5250: Loss: 0.4109 LR: 1.21e-05 \nStep 5260: Loss: 0.2921 LR: 1.21e-05 \nStep 5270: Loss: 0.3692 LR: 1.21e-05 \nStep 5280: Loss: 0.2434 LR: 1.20e-05 \nStep 5290: Loss: 0.4114 LR: 1.20e-05 \nStep 5300: Loss: 0.5775 LR: 1.20e-05 \nStep 5310: Loss: 0.2196 LR: 1.20e-05 \nStep 5320: Loss: 0.4613 LR: 1.20e-05 \nStep 5330: Loss: 0.4526 LR: 1.20e-05 \nStep 5340: Loss: 0.3258 LR: 1.20e-05 \nStep 5350: Loss: 0.3024 LR: 1.19e-05 \nStep 5360: Loss: 0.3561 LR: 1.19e-05 \nStep 5370: Loss: 0.2895 LR: 1.19e-05 \nStep 5380: Loss: 0.3966 LR: 1.19e-05 \nStep 5390: Loss: 0.4054 LR: 1.19e-05 \nStep 5400: Loss: 0.3292 LR: 1.19e-05 \nStep 5410: Loss: 0.4558 LR: 1.19e-05 \nStep 5420: Loss: 0.3734 LR: 1.18e-05 \nStep 5430: Loss: 0.3190 LR: 1.18e-05 \nStep 5440: Loss: 0.4053 LR: 1.18e-05 \nStep 5450: Loss: 0.3560 LR: 1.18e-05 \nStep 5460: Loss: 0.3730 LR: 1.18e-05 \nStep 5470: Loss: 0.3133 LR: 1.18e-05 \nStep 5480: Loss: 0.4585 LR: 1.17e-05 \nStep 5490: Loss: 0.3972 LR: 1.17e-05 \nStep 5500: Loss: 0.2469 LR: 1.17e-05 \nStep 5510: Loss: 0.4529 LR: 1.17e-05 \nStep 5520: Loss: 0.3646 LR: 1.17e-05 \nStep 5530: Loss: 0.3691 LR: 1.17e-05 \nStep 5540: Loss: 0.4914 LR: 1.17e-05 \nStep 5550: Loss: 0.4538 LR: 1.16e-05 \nStep 5560: Loss: 0.4322 LR: 1.16e-05 \nStep 5570: Loss: 0.4563 LR: 1.16e-05 \nStep 5580: Loss: 0.3541 LR: 1.16e-05 \nStep 5590: Loss: 0.5176 LR: 1.16e-05 \nStep 5600: Loss: 0.4306 LR: 1.16e-05 \nStep 5610: Loss: 0.2859 LR: 1.15e-05 \nStep 5620: Loss: 0.4132 LR: 1.15e-05 \nStep 5630: Loss: 0.3142 LR: 1.15e-05 \nStep 5640: Loss: 0.3238 LR: 1.15e-05 \nStep 5650: Loss: 0.4281 LR: 1.15e-05 \nStep 5660: Loss: 0.3668 LR: 1.15e-05 \nStep 5670: Loss: 0.3807 LR: 1.15e-05 \nStep 5680: Loss: 0.4126 LR: 1.14e-05 \nStep 5690: Loss: 0.4106 LR: 1.14e-05 \nStep 5700: Loss: 0.3823 LR: 1.14e-05 \nStep 5710: Loss: 0.4985 LR: 1.14e-05 \nStep 5720: Loss: 0.4069 LR: 1.14e-05 \nStep 5730: Loss: 0.3224 LR: 1.14e-05 \nStep 5740: Loss: 0.3347 LR: 1.14e-05 \nStep 5750: Loss: 0.4247 LR: 1.13e-05 \nStep 5760: Loss: 0.3651 LR: 1.13e-05 \nStep 5770: Loss: 0.5008 LR: 1.13e-05 \nStep 5780: Loss: 0.4034 LR: 1.13e-05 \nStep 5790: Loss: 0.4378 LR: 1.13e-05 \nStep 5800: Loss: 0.3212 LR: 1.13e-05 \nStep 5810: Loss: 0.3751 LR: 1.12e-05 \nStep 5820: Loss: 0.3078 LR: 1.12e-05 \nStep 5830: Loss: 0.5017 LR: 1.12e-05 \nStep 5840: Loss: 0.2978 LR: 1.12e-05 \nStep 5850: Loss: 0.4579 LR: 1.12e-05 \nStep 5860: Loss: 0.3617 LR: 1.12e-05 \nStep 5870: Loss: 0.4565 LR: 1.12e-05 \nStep 5880: Loss: 0.3896 LR: 1.11e-05 \nStep 5890: Loss: 0.2786 LR: 1.11e-05 \nStep 5900: Loss: 0.3289 LR: 1.11e-05 \nStep 5910: Loss: 0.4728 LR: 1.11e-05 \nStep 5920: Loss: 0.3545 LR: 1.11e-05 \nStep 5930: Loss: 0.2867 LR: 1.11e-05 \nStep 5940: Loss: 0.4783 LR: 1.11e-05 \nStep 5950: Loss: 0.4369 LR: 1.10e-05 \nStep 5960: Loss: 0.3605 LR: 1.10e-05 \nStep 5970: Loss: 0.3223 LR: 1.10e-05 \nStep 5980: Loss: 0.3242 LR: 1.10e-05 \nStep 5990: Loss: 0.3420 LR: 1.10e-05 \nStep 6000: Loss: 0.4186 LR: 1.10e-05 \nStep 6010: Loss: 0.3060 LR: 1.09e-05 \nStep 6020: Loss: 0.4176 LR: 1.09e-05 \nStep 6030: Loss: 0.2843 LR: 1.09e-05 \nStep 6040: Loss: 0.5199 LR: 1.09e-05 \nStep 6050: Loss: 0.3215 LR: 1.09e-05 \nStep 6060: Loss: 0.3790 LR: 1.09e-05 \nStep 6070: Loss: 0.5617 LR: 1.09e-05 \nStep 6080: Loss: 0.4664 LR: 1.08e-05 \nStep 6090: Loss: 0.3020 LR: 1.08e-05 \nStep 6100: Loss: 0.3976 LR: 1.08e-05 \nStep 6110: Loss: 0.4137 LR: 1.08e-05 \nStep 6120: Loss: 0.4396 LR: 1.08e-05 \nStep 6130: Loss: 0.3770 LR: 1.08e-05 \nStep 6140: Loss: 0.4609 LR: 1.08e-05 \nStep 6150: Loss: 0.4484 LR: 1.07e-05 \nStep 6160: Loss: 0.3538 LR: 1.07e-05 \nStep 6170: Loss: 0.3249 LR: 1.07e-05 \nStep 6180: Loss: 0.6271 LR: 1.07e-05 \nStep 6190: Loss: 0.5815 LR: 1.07e-05 \nStep 6200: Loss: 0.4754 LR: 1.07e-05 \nStep 6210: Loss: 0.5921 LR: 1.06e-05 \nStep 6220: Loss: 0.4962 LR: 1.06e-05 \nStep 6230: Loss: 0.3745 LR: 1.06e-05 \nStep 6240: Loss: 0.3207 LR: 1.06e-05 \nStep 6250: Loss: 0.5177 LR: 1.06e-05 \nStep 6260: Loss: 0.4326 LR: 1.06e-05 \nStep 6270: Loss: 0.4457 LR: 1.06e-05 \nStep 6280: Loss: 0.3619 LR: 1.05e-05 \nStep 6290: Loss: 0.3466 LR: 1.05e-05 \nStep 6300: Loss: 0.3199 LR: 1.05e-05 \nStep 6310: Loss: 0.4370 LR: 1.05e-05 \nStep 6320: Loss: 0.3996 LR: 1.05e-05 \nStep 6330: Loss: 0.4791 LR: 1.05e-05 \nStep 6340: Loss: 0.3583 LR: 1.05e-05 \nStep 6350: Loss: 0.4380 LR: 1.04e-05 \nStep 6360: Loss: 0.3463 LR: 1.04e-05 \nStep 6370: Loss: 0.4218 LR: 1.04e-05 \nStep 6380: Loss: 0.6257 LR: 1.04e-05 \nStep 6390: Loss: 0.4085 LR: 1.04e-05 \nStep 6400: Loss: 0.3787 LR: 1.04e-05 \nStep 6410: Loss: 0.3329 LR: 1.03e-05 \nStep 6420: Loss: 0.7348 LR: 1.03e-05 \nStep 6430: Loss: 0.4347 LR: 1.03e-05 \nStep 6440: Loss: 0.4612 LR: 1.03e-05 \nStep 6450: Loss: 0.3677 LR: 1.03e-05 \nStep 6460: Loss: 0.4029 LR: 1.03e-05 \nStep 6470: Loss: 0.4216 LR: 1.03e-05 \nStep 6480: Loss: 0.4535 LR: 1.02e-05 \nStep 6490: Loss: 0.4628 LR: 1.02e-05 \nStep 6500: Loss: 0.4156 LR: 1.02e-05 \nStep 6510: Loss: 0.3288 LR: 1.02e-05 \nStep 6520: Loss: 0.4054 LR: 1.02e-05 \nStep 6530: Loss: 0.3332 LR: 1.02e-05 \nStep 6540: Loss: 0.3718 LR: 1.01e-05 \nStep 6550: Loss: 0.3537 LR: 1.01e-05 \nStep 6560: Loss: 0.3443 LR: 1.01e-05 \nStep 6570: Loss: 0.4796 LR: 1.01e-05 \nStep 6580: Loss: 0.3163 LR: 1.01e-05 \nStep 6590: Loss: 0.3852 LR: 1.01e-05 \nStep 6600: Loss: 0.4832 LR: 1.01e-05 \nStep 6610: Loss: 0.2977 LR: 1.00e-05 \nStep 6620: Loss: 0.5350 LR: 1.00e-05 \nStep 6630: Loss: 0.4839 LR: 1.00e-05 \nStep 6640: Loss: 0.3338 LR: 1.00e-05 \nStep 6650: Loss: 0.3863 LR: 9.98e-06 \nStep 6660: Loss: 0.5156 LR: 9.97e-06 \nStep 6670: Loss: 0.3051 LR: 9.95e-06 \nStep 6680: Loss: 0.4068 LR: 9.94e-06 \nStep 6690: Loss: 0.4125 LR: 9.92e-06 \nStep 6700: Loss: 0.3942 LR: 9.91e-06 \nStep 6710: Loss: 0.4132 LR: 9.89e-06 \nStep 6720: Loss: 0.3893 LR: 9.88e-06 \nStep 6730: Loss: 0.5308 LR: 9.86e-06 \nStep 6740: Loss: 0.4417 LR: 9.85e-06 \nStep 6750: Loss: 0.4544 LR: 9.83e-06 \nStep 6760: Loss: 0.2701 LR: 9.82e-06 \nStep 6770: Loss: 0.3404 LR: 9.80e-06 \nStep 6780: Loss: 0.4886 LR: 9.79e-06 \nStep 6790: Loss: 0.4139 LR: 9.77e-06 \nStep 6800: Loss: 0.3802 LR: 9.76e-06 \nStep 6810: Loss: 0.3805 LR: 9.74e-06 \nStep 6820: Loss: 0.4310 LR: 9.73e-06 \nStep 6830: Loss: 0.3472 LR: 9.71e-06 \nStep 6840: Loss: 0.4153 LR: 9.70e-06 \nStep 6850: Loss: 0.4551 LR: 9.68e-06 \nStep 6860: Loss: 0.3746 LR: 9.67e-06 \nStep 6870: Loss: 0.3938 LR: 9.65e-06 \nStep 6880: Loss: 0.3119 LR: 9.64e-06 \nStep 6890: Loss: 0.4173 LR: 9.62e-06 \nStep 6900: Loss: 0.4149 LR: 9.61e-06 \nStep 6910: Loss: 0.5230 LR: 9.59e-06 \nStep 6920: Loss: 0.3422 LR: 9.58e-06 \nStep 6930: Loss: 0.3925 LR: 9.56e-06 \nStep 6940: Loss: 0.4355 LR: 9.55e-06 \nStep 6950: Loss: 0.3481 LR: 9.53e-06 \nStep 6960: Loss: 0.3958 LR: 9.52e-06 \nStep 6970: Loss: 0.4439 LR: 9.50e-06 \nStep 6980: Loss: 0.5022 LR: 9.49e-06 \nStep 6990: Loss: 0.2745 LR: 9.47e-06 \nStep 7000: Loss: 0.4098 LR: 9.46e-06 \nStep 7010: Loss: 0.4117 LR: 9.44e-06 \nStep 7020: Loss: 0.4171 LR: 9.43e-06 \nStep 7030: Loss: 0.6251 LR: 9.41e-06 \nStep 7040: Loss: 0.5018 LR: 9.40e-06 \nStep 7050: Loss: 0.4236 LR: 9.38e-06 \nStep 7060: Loss: 0.4341 LR: 9.37e-06 \nStep 7070: Loss: 0.5898 LR: 9.35e-06 \nStep 7080: Loss: 0.4537 LR: 9.34e-06 \nStep 7090: Loss: 0.4444 LR: 9.32e-06 \nStep 7100: Loss: 0.3965 LR: 9.31e-06 \nStep 7110: Loss: 0.4298 LR: 9.29e-06 \nStep 7120: Loss: 0.3178 LR: 9.28e-06 \nStep 7130: Loss: 0.4977 LR: 9.26e-06 \nStep 7140: Loss: 0.2812 LR: 9.25e-06 \nStep 7150: Loss: 0.3724 LR: 9.23e-06 \nStep 7160: Loss: 0.4956 LR: 9.22e-06 \nStep 7170: Loss: 0.2966 LR: 9.20e-06 \nStep 7180: Loss: 0.4237 LR: 9.19e-06 \nStep 7190: Loss: 0.3888 LR: 9.17e-06 \nStep 7200: Loss: 0.4704 LR: 9.15e-06 \nStep 7210: Loss: 0.3677 LR: 9.14e-06 \nStep 7220: Loss: 0.2587 LR: 9.12e-06 \nStep 7230: Loss: 0.5382 LR: 9.11e-06 \nStep 7240: Loss: 0.3615 LR: 9.09e-06 \nStep 7250: Loss: 0.3575 LR: 9.08e-06 \nStep 7260: Loss: 0.3845 LR: 9.06e-06 \nStep 7270: Loss: 0.3434 LR: 9.05e-06 \nStep 7280: Loss: 0.4027 LR: 9.03e-06 \nStep 7290: Loss: 0.3604 LR: 9.02e-06 \nStep 7300: Loss: 0.5720 LR: 9.00e-06 \nStep 7310: Loss: 0.4944 LR: 8.99e-06 \nStep 7320: Loss: 0.5016 LR: 8.97e-06 \nStep 7330: Loss: 0.4023 LR: 8.96e-06 \nStep 7340: Loss: 0.4656 LR: 8.94e-06 \nStep 7350: Loss: 0.3700 LR: 8.93e-06 \nStep 7360: Loss: 0.4869 LR: 8.91e-06 \nStep 7370: Loss: 0.3954 LR: 8.90e-06 \nStep 7380: Loss: 0.3410 LR: 8.88e-06 \nStep 7390: Loss: 0.3770 LR: 8.87e-06 \nStep 7400: Loss: 0.3469 LR: 8.85e-06 \nStep 7410: Loss: 0.2440 LR: 8.84e-06 \nStep 7420: Loss: 0.4089 LR: 8.82e-06 \nStep 7430: Loss: 0.3695 LR: 8.81e-06 \nStep 7440: Loss: 0.5167 LR: 8.79e-06 \nStep 7450: Loss: 0.2621 LR: 8.78e-06 \nStep 7460: Loss: 0.4200 LR: 8.76e-06 \nStep 7470: Loss: 0.4088 LR: 8.75e-06 \nStep 7480: Loss: 0.2863 LR: 8.73e-06 \nStep 7490: Loss: 0.3734 LR: 8.72e-06 \nStep 7500: Loss: 0.3117 LR: 8.70e-06 \nStep 7510: Loss: 0.4121 LR: 8.69e-06 \nStep 7520: Loss: 0.5266 LR: 8.67e-06 \nStep 7530: Loss: 0.3610 LR: 8.66e-06 \nStep 7540: Loss: 0.2635 LR: 8.64e-06 \nStep 7550: Loss: 0.3718 LR: 8.63e-06 \nStep 7560: Loss: 0.5024 LR: 8.61e-06 \nStep 7570: Loss: 0.4056 LR: 8.60e-06 \nStep 7580: Loss: 0.5101 LR: 8.58e-06 \nStep 7590: Loss: 0.2887 LR: 8.57e-06 \nStep 7600: Loss: 0.3361 LR: 8.55e-06 \nStep 7610: Loss: 0.4869 LR: 8.54e-06 \nStep 7620: Loss: 0.2667 LR: 8.52e-06 \nStep 7630: Loss: 0.3611 LR: 8.51e-06 \nStep 7640: Loss: 0.4178 LR: 8.49e-06 \nStep 7650: Loss: 0.2789 LR: 8.48e-06 \nStep 7660: Loss: 0.2975 LR: 8.46e-06 \nStep 7670: Loss: 0.5041 LR: 8.45e-06 \nStep 7680: Loss: 0.3849 LR: 8.43e-06 \nStep 7690: Loss: 0.4479 LR: 8.42e-06 \nStep 7700: Loss: 0.4981 LR: 8.40e-06 \nStep 7710: Loss: 0.5027 LR: 8.39e-06 \nStep 7720: Loss: 0.4445 LR: 8.37e-06 \nStep 7730: Loss: 0.5289 LR: 8.36e-06 \nStep 7740: Loss: 0.3987 LR: 8.34e-06 \nStep 7750: Loss: 0.3670 LR: 8.33e-06 \nStep 7760: Loss: 0.4744 LR: 8.31e-06 \nStep 7770: Loss: 0.3736 LR: 8.30e-06 \nStep 7780: Loss: 0.4025 LR: 8.28e-06 \nStep 7790: Loss: 0.4306 LR: 8.27e-06 \nStep 7800: Loss: 0.3233 LR: 8.25e-06 \nStep 7810: Loss: 0.3442 LR: 8.24e-06 \nStep 7820: Loss: 0.1915 LR: 8.22e-06 \nStep 7830: Loss: 0.3956 LR: 8.21e-06 \nStep 7840: Loss: 0.4747 LR: 8.19e-06 \nStep 7850: Loss: 0.4618 LR: 8.18e-06 \nStep 7860: Loss: 0.4354 LR: 8.16e-06 \nStep 7870: Loss: 0.4492 LR: 8.15e-06 \nStep 7880: Loss: 0.4530 LR: 8.13e-06 \nStep 7890: Loss: 0.4593 LR: 8.12e-06 \nStep 7900: Loss: 0.3610 LR: 8.10e-06 \nStep 7910: Loss: 0.4369 LR: 8.09e-06 \nStep 7920: Loss: 0.3675 LR: 8.07e-06 \nStep 7930: Loss: 0.3104 LR: 8.06e-06 \nStep 7940: Loss: 0.3039 LR: 8.04e-06 \nStep 7950: Loss: 0.4743 LR: 8.03e-06 \nStep 7960: Loss: 0.3778 LR: 8.01e-06 \nStep 7970: Loss: 0.5086 LR: 8.00e-06 \nStep 7980: Loss: 0.2902 LR: 7.98e-06 \nStep 7990: Loss: 0.3618 LR: 7.97e-06 \nStep 8000: Loss: 0.3774 LR: 7.95e-06 \nStep 8010: Loss: 0.4844 LR: 7.93e-06 \nStep 8020: Loss: 0.3861 LR: 7.92e-06 \nStep 8030: Loss: 0.4359 LR: 7.90e-06 \nStep 8040: Loss: 0.3879 LR: 7.89e-06 \nStep 8050: Loss: 0.4841 LR: 7.87e-06 \nStep 8060: Loss: 0.5151 LR: 7.86e-06 \nStep 8070: Loss: 0.3911 LR: 7.84e-06 \nStep 8080: Loss: 0.3523 LR: 7.83e-06 \nStep 8090: Loss: 0.3374 LR: 7.81e-06 \nStep 8100: Loss: 0.2625 LR: 7.80e-06 \nStep 8110: Loss: 0.3855 LR: 7.78e-06 \nStep 8120: Loss: 0.4835 LR: 7.77e-06 \nStep 8130: Loss: 0.5027 LR: 7.75e-06 \nStep 8140: Loss: 0.6503 LR: 7.74e-06 \nStep 8150: Loss: 0.4256 LR: 7.72e-06 \nStep 8160: Loss: 0.3546 LR: 7.71e-06 \nStep 8170: Loss: 0.3661 LR: 7.69e-06 \nStep 8180: Loss: 0.3124 LR: 7.68e-06 \nStep 8190: Loss: 0.3343 LR: 7.66e-06 \nStep 8200: Loss: 0.4109 LR: 7.65e-06 \nStep 8210: Loss: 0.3901 LR: 7.63e-06 \nStep 8220: Loss: 0.3234 LR: 7.62e-06 \nStep 8230: Loss: 0.4100 LR: 7.60e-06 \nStep 8240: Loss: 0.4237 LR: 7.59e-06 \nStep 8250: Loss: 0.4767 LR: 7.57e-06 \nStep 8260: Loss: 0.3901 LR: 7.56e-06 \nStep 8270: Loss: 0.3513 LR: 7.54e-06 \nStep 8280: Loss: 0.4359 LR: 7.53e-06 \nStep 8290: Loss: 0.3890 LR: 7.51e-06 \nStep 8300: Loss: 0.3587 LR: 7.50e-06 \nStep 8310: Loss: 0.6634 LR: 7.48e-06 \nStep 8320: Loss: 0.4026 LR: 7.47e-06 \nStep 8330: Loss: 0.3220 LR: 7.45e-06 \nStep 8340: Loss: 0.5178 LR: 7.44e-06 \nStep 8350: Loss: 0.4174 LR: 7.42e-06 \nStep 8360: Loss: 0.4264 LR: 7.41e-06 \nStep 8370: Loss: 0.4379 LR: 7.39e-06 \nStep 8380: Loss: 0.3728 LR: 7.38e-06 \nStep 8390: Loss: 0.3927 LR: 7.36e-06 \nStep 8400: Loss: 0.3425 LR: 7.35e-06 \nStep 8410: Loss: 0.3542 LR: 7.33e-06 \nStep 8420: Loss: 0.2437 LR: 7.32e-06 \nStep 8430: Loss: 0.2383 LR: 7.30e-06 \nStep 8440: Loss: 0.4099 LR: 7.29e-06 \nStep 8450: Loss: 0.3678 LR: 7.27e-06 \nStep 8460: Loss: 0.3562 LR: 7.26e-06 \nStep 8470: Loss: 0.4385 LR: 7.24e-06 \nStep 8480: Loss: 0.4938 LR: 7.23e-06 \nStep 8490: Loss: 0.4437 LR: 7.21e-06 \nStep 8500: Loss: 0.3550 LR: 7.20e-06 \nStep 8510: Loss: 0.4355 LR: 7.18e-06 \nStep 8520: Loss: 0.5473 LR: 7.17e-06 \nStep 8530: Loss: 0.3079 LR: 7.15e-06 \nStep 8540: Loss: 0.3374 LR: 7.14e-06 \nStep 8550: Loss: 0.4642 LR: 7.12e-06 \nStep 8560: Loss: 0.3683 LR: 7.11e-06 \nStep 8570: Loss: 0.5247 LR: 7.09e-06 \nStep 8580: Loss: 0.2760 LR: 7.08e-06 \nStep 8590: Loss: 0.3284 LR: 7.06e-06 \nStep 8600: Loss: 0.3258 LR: 7.05e-06 \nStep 8610: Loss: 0.4455 LR: 7.03e-06 \nStep 8620: Loss: 0.3675 LR: 7.02e-06 \nStep 8630: Loss: 0.4154 LR: 7.00e-06 \nStep 8640: Loss: 0.3439 LR: 6.99e-06 \nStep 8650: Loss: 0.4606 LR: 6.97e-06 \nStep 8660: Loss: 0.4658 LR: 6.96e-06 \nStep 8670: Loss: 0.4025 LR: 6.94e-06 \nStep 8680: Loss: 0.2671 LR: 6.93e-06 \nStep 8690: Loss: 0.3056 LR: 6.91e-06 \nStep 8700: Loss: 0.3023 LR: 6.90e-06 \nStep 8710: Loss: 0.4053 LR: 6.88e-06 \nStep 8720: Loss: 0.3867 LR: 6.87e-06 \nStep 8730: Loss: 0.3905 LR: 6.85e-06 \nStep 8740: Loss: 0.3573 LR: 6.84e-06 \nStep 8750: Loss: 0.3815 LR: 6.82e-06 \nStep 8760: Loss: 0.3236 LR: 6.81e-06 \nStep 8770: Loss: 0.5155 LR: 6.79e-06 \nStep 8780: Loss: 0.3872 LR: 6.78e-06 \nStep 8790: Loss: 0.4314 LR: 6.76e-06 \nStep 8800: Loss: 0.6191 LR: 6.74e-06 \nStep 8810: Loss: 0.3825 LR: 6.73e-06 \nStep 8820: Loss: 0.3477 LR: 6.71e-06 \nStep 8830: Loss: 0.3371 LR: 6.70e-06 \nStep 8840: Loss: 0.5249 LR: 6.68e-06 \nStep 8850: Loss: 0.2720 LR: 6.67e-06 \n\n==================================================\nEnd of epoch 2.0\nSteps completed: 8852\n==================================================\n\n\nEvaluation metrics: {'eval_loss': 0.44673916697502136, 'eval_accuracy': 0.8680564382865132, 'eval_f1': 0.8675700144776649, 'eval_precision': 0.8673546170776372, 'eval_recall': 0.8680564382865132, 'eval_runtime': 152.214, 'eval_samples_per_second': 51.684, 'eval_steps_per_second': 3.232, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 8860: Loss: 0.2411 LR: 6.65e-06 \nStep 8870: Loss: 0.3847 LR: 6.64e-06 \nStep 8880: Loss: 0.3806 LR: 6.62e-06 \nStep 8890: Loss: 0.3777 LR: 6.61e-06 \nStep 8900: Loss: 0.3444 LR: 6.59e-06 \nStep 8910: Loss: 0.2954 LR: 6.58e-06 \nStep 8920: Loss: 0.2506 LR: 6.56e-06 \nStep 8930: Loss: 0.4206 LR: 6.55e-06 \nStep 8940: Loss: 0.3081 LR: 6.53e-06 \nStep 8950: Loss: 0.2221 LR: 6.52e-06 \nStep 8960: Loss: 0.3777 LR: 6.50e-06 \nStep 8970: Loss: 0.3609 LR: 6.49e-06 \nStep 8980: Loss: 0.2302 LR: 6.47e-06 \nStep 8990: Loss: 0.3755 LR: 6.46e-06 \nStep 9000: Loss: 0.2881 LR: 6.44e-06 \nStep 9010: Loss: 0.3152 LR: 6.43e-06 \nStep 9020: Loss: 0.3093 LR: 6.41e-06 \nStep 9030: Loss: 0.3434 LR: 6.40e-06 \nStep 9040: Loss: 0.3238 LR: 6.38e-06 \nStep 9050: Loss: 0.3580 LR: 6.37e-06 \nStep 9060: Loss: 0.2608 LR: 6.35e-06 \nStep 9070: Loss: 0.3329 LR: 6.34e-06 \nStep 9080: Loss: 0.2525 LR: 6.32e-06 \nStep 9090: Loss: 0.2171 LR: 6.31e-06 \nStep 9100: Loss: 0.5496 LR: 6.29e-06 \nStep 9110: Loss: 0.4484 LR: 6.28e-06 \nStep 9120: Loss: 0.2286 LR: 6.26e-06 \nStep 9130: Loss: 0.3776 LR: 6.25e-06 \nStep 9140: Loss: 0.2774 LR: 6.23e-06 \nStep 9150: Loss: 0.3208 LR: 6.22e-06 \nStep 9160: Loss: 0.3431 LR: 6.20e-06 \nStep 9170: Loss: 0.2725 LR: 6.19e-06 \nStep 9180: Loss: 0.4044 LR: 6.17e-06 \nStep 9190: Loss: 0.5685 LR: 6.16e-06 \nStep 9200: Loss: 0.2294 LR: 6.14e-06 \nStep 9210: Loss: 0.2712 LR: 6.13e-06 \nStep 9220: Loss: 0.3983 LR: 6.11e-06 \nStep 9230: Loss: 0.3008 LR: 6.10e-06 \nStep 9240: Loss: 0.3422 LR: 6.08e-06 \nStep 9250: Loss: 0.4118 LR: 6.07e-06 \nStep 9260: Loss: 0.3066 LR: 6.05e-06 \nStep 9270: Loss: 0.3174 LR: 6.04e-06 \nStep 9280: Loss: 0.3143 LR: 6.02e-06 \nStep 9290: Loss: 0.3496 LR: 6.01e-06 \nStep 9300: Loss: 0.2381 LR: 5.99e-06 \nStep 9310: Loss: 0.3604 LR: 5.98e-06 \nStep 9320: Loss: 0.2298 LR: 5.96e-06 \nStep 9330: Loss: 0.2786 LR: 5.95e-06 \nStep 9340: Loss: 0.3709 LR: 5.93e-06 \nStep 9350: Loss: 0.2891 LR: 5.92e-06 \nStep 9360: Loss: 0.2425 LR: 5.90e-06 \nStep 9370: Loss: 0.2686 LR: 5.89e-06 \nStep 9380: Loss: 0.3796 LR: 5.87e-06 \nStep 9390: Loss: 0.3414 LR: 5.86e-06 \nStep 9400: Loss: 0.3248 LR: 5.84e-06 \nStep 9410: Loss: 0.3654 LR: 5.83e-06 \nStep 9420: Loss: 0.3680 LR: 5.81e-06 \nStep 9430: Loss: 0.2490 LR: 5.80e-06 \nStep 9440: Loss: 0.1684 LR: 5.78e-06 \nStep 9450: Loss: 0.3257 LR: 5.77e-06 \nStep 9460: Loss: 0.2871 LR: 5.75e-06 \nStep 9470: Loss: 0.2527 LR: 5.74e-06 \nStep 9480: Loss: 0.4097 LR: 5.72e-06 \nStep 9490: Loss: 0.3132 LR: 5.71e-06 \nStep 9500: Loss: 0.3488 LR: 5.69e-06 \nStep 9510: Loss: 0.3307 LR: 5.68e-06 \nStep 9520: Loss: 0.3863 LR: 5.66e-06 \nStep 9530: Loss: 0.2817 LR: 5.65e-06 \nStep 9540: Loss: 0.3697 LR: 5.63e-06 \nStep 9550: Loss: 0.2958 LR: 5.62e-06 \nStep 9560: Loss: 0.2792 LR: 5.60e-06 \nStep 9570: Loss: 0.4017 LR: 5.59e-06 \nStep 9580: Loss: 0.4131 LR: 5.57e-06 \nStep 9590: Loss: 0.2055 LR: 5.56e-06 \nStep 9600: Loss: 0.2511 LR: 5.54e-06 \nStep 9610: Loss: 0.3565 LR: 5.52e-06 \nStep 9620: Loss: 0.3444 LR: 5.51e-06 \nStep 9630: Loss: 0.2468 LR: 5.49e-06 \nStep 9640: Loss: 0.2742 LR: 5.48e-06 \nStep 9650: Loss: 0.3377 LR: 5.46e-06 \nStep 9660: Loss: 0.3073 LR: 5.45e-06 \nStep 9670: Loss: 0.2976 LR: 5.43e-06 \nStep 9680: Loss: 0.2309 LR: 5.42e-06 \nStep 9690: Loss: 0.3783 LR: 5.40e-06 \nStep 9700: Loss: 0.3817 LR: 5.39e-06 \nStep 9710: Loss: 0.4372 LR: 5.37e-06 \nStep 9720: Loss: 0.3337 LR: 5.36e-06 \nStep 9730: Loss: 0.2658 LR: 5.34e-06 \nStep 9740: Loss: 0.3356 LR: 5.33e-06 \nStep 9750: Loss: 0.2542 LR: 5.31e-06 \nStep 9760: Loss: 0.3064 LR: 5.30e-06 \nStep 9770: Loss: 0.3465 LR: 5.28e-06 \nStep 9780: Loss: 0.3603 LR: 5.27e-06 \nStep 9790: Loss: 0.2838 LR: 5.25e-06 \nStep 9800: Loss: 0.3680 LR: 5.24e-06 \nStep 9810: Loss: 0.3840 LR: 5.22e-06 \nStep 9820: Loss: 0.2896 LR: 5.21e-06 \nStep 9830: Loss: 0.3938 LR: 5.19e-06 \nStep 9840: Loss: 0.4261 LR: 5.18e-06 \nStep 9850: Loss: 0.3799 LR: 5.16e-06 \nStep 9860: Loss: 0.2991 LR: 5.15e-06 \nStep 9870: Loss: 0.3886 LR: 5.13e-06 \nStep 9880: Loss: 0.2959 LR: 5.12e-06 \nStep 9890: Loss: 0.3634 LR: 5.10e-06 \nStep 9900: Loss: 0.3045 LR: 5.09e-06 \nStep 9910: Loss: 0.2023 LR: 5.07e-06 \nStep 9920: Loss: 0.2699 LR: 5.06e-06 \nStep 9930: Loss: 0.2557 LR: 5.04e-06 \nStep 9940: Loss: 0.3889 LR: 5.03e-06 \nStep 9950: Loss: 0.2360 LR: 5.01e-06 \nStep 9960: Loss: 0.4266 LR: 5.00e-06 \nStep 9970: Loss: 0.3486 LR: 4.98e-06 \nStep 9980: Loss: 0.4707 LR: 4.97e-06 \nStep 9990: Loss: 0.3344 LR: 4.95e-06 \nStep 10000: Loss: 0.2915 LR: 4.94e-06 \nStep 10010: Loss: 0.3722 LR: 4.92e-06 \nStep 10020: Loss: 0.4734 LR: 4.91e-06 \nStep 10030: Loss: 0.2836 LR: 4.89e-06 \nStep 10040: Loss: 0.3549 LR: 4.88e-06 \nStep 10050: Loss: 0.3348 LR: 4.86e-06 \nStep 10060: Loss: 0.2410 LR: 4.85e-06 \nStep 10070: Loss: 0.2364 LR: 4.83e-06 \nStep 10080: Loss: 0.3799 LR: 4.82e-06 \nStep 10090: Loss: 0.3659 LR: 4.80e-06 \nStep 10100: Loss: 0.2164 LR: 4.79e-06 \nStep 10110: Loss: 0.2698 LR: 4.77e-06 \nStep 10120: Loss: 0.2752 LR: 4.76e-06 \nStep 10130: Loss: 0.2807 LR: 4.74e-06 \nStep 10140: Loss: 0.2793 LR: 4.73e-06 \nStep 10150: Loss: 0.4972 LR: 4.71e-06 \nStep 10160: Loss: 0.2377 LR: 4.70e-06 \nStep 10170: Loss: 0.3812 LR: 4.68e-06 \nStep 10180: Loss: 0.3173 LR: 4.67e-06 \nStep 10190: Loss: 0.5539 LR: 4.65e-06 \nStep 10200: Loss: 0.3606 LR: 4.64e-06 \nStep 10210: Loss: 0.4703 LR: 4.62e-06 \nStep 10220: Loss: 0.3751 LR: 4.61e-06 \nStep 10230: Loss: 0.2969 LR: 4.59e-06 \nStep 10240: Loss: 0.2961 LR: 4.58e-06 \nStep 10250: Loss: 0.3016 LR: 4.56e-06 \nStep 10260: Loss: 0.3465 LR: 4.55e-06 \nStep 10270: Loss: 0.2509 LR: 4.53e-06 \nStep 10280: Loss: 0.2899 LR: 4.52e-06 \nStep 10290: Loss: 0.2027 LR: 4.50e-06 \nStep 10300: Loss: 0.2603 LR: 4.49e-06 \nStep 10310: Loss: 0.3880 LR: 4.47e-06 \nStep 10320: Loss: 0.4880 LR: 4.46e-06 \nStep 10330: Loss: 0.3198 LR: 4.44e-06 \nStep 10340: Loss: 0.2308 LR: 4.43e-06 \nStep 10350: Loss: 0.3816 LR: 4.41e-06 \nStep 10360: Loss: 0.3985 LR: 4.40e-06 \nStep 10370: Loss: 0.4640 LR: 4.38e-06 \nStep 10380: Loss: 0.3207 LR: 4.37e-06 \nStep 10390: Loss: 0.2519 LR: 4.35e-06 \nStep 10400: Loss: 0.3710 LR: 4.33e-06 \nStep 10410: Loss: 0.3296 LR: 4.32e-06 \nStep 10420: Loss: 0.3614 LR: 4.30e-06 \nStep 10430: Loss: 0.3025 LR: 4.29e-06 \nStep 10440: Loss: 0.2653 LR: 4.27e-06 \nStep 10450: Loss: 0.3721 LR: 4.26e-06 \nStep 10460: Loss: 0.3048 LR: 4.24e-06 \nStep 10470: Loss: 0.2453 LR: 4.23e-06 \nStep 10480: Loss: 0.3641 LR: 4.21e-06 \nStep 10490: Loss: 0.2896 LR: 4.20e-06 \nStep 10500: Loss: 0.3331 LR: 4.18e-06 \nStep 10510: Loss: 0.3783 LR: 4.17e-06 \nStep 10520: Loss: 0.2712 LR: 4.15e-06 \nStep 10530: Loss: 0.3653 LR: 4.14e-06 \nStep 10540: Loss: 0.2468 LR: 4.12e-06 \nStep 10550: Loss: 0.3228 LR: 4.11e-06 \nStep 10560: Loss: 0.3169 LR: 4.09e-06 \nStep 10570: Loss: 0.3211 LR: 4.08e-06 \nStep 10580: Loss: 0.2327 LR: 4.06e-06 \nStep 10590: Loss: 0.3840 LR: 4.05e-06 \nStep 10600: Loss: 0.3289 LR: 4.03e-06 \nStep 10610: Loss: 0.2815 LR: 4.02e-06 \nStep 10620: Loss: 0.2630 LR: 4.00e-06 \nStep 10630: Loss: 0.4208 LR: 3.99e-06 \nStep 10640: Loss: 0.4150 LR: 3.97e-06 \nStep 10650: Loss: 0.2912 LR: 3.96e-06 \n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"markdown","source":"Process a pdf","metadata":{}},{"cell_type":"code","source":"import PyPDF2\n\ndef pdf_to_text(pdf_path, txt_path):\n    # Open the PDF file\n    with open(pdf_path, 'rb') as pdf_file:\n        # Create a PDF reader object\n        pdf_reader = PyPDF2.PdfReader(pdf_file)\n        \n        # Initialize an empty string to hold the text\n        text = ''\n        \n        # Loop through each page in the PDF\n        for page_num in range(len(pdf_reader.pages)):\n            # Get the page object\n            page = pdf_reader.pages[page_num]\n            \n            # Extract the text from the page\n            text += page.extract_text()\n        \n        # Write the extracted text to a .txt file\n        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n            txt_file.write(text)\n\n# Example usage\npdf_path = '/kaggle/input/resume-dataset/data/data/ENGINEERING/10030015.pdf'  # Path to the input PDF file\ntxt_path = 'output.txt'   # Path to the output text file\n\npdf_to_text(pdf_path, txt_path)\nprint(f\"Text extracted from {pdf_path} and saved to {txt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:03:44.190551Z","iopub.execute_input":"2025-03-18T12:03:44.191115Z","iopub.status.idle":"2025-03-18T12:03:44.316227Z","shell.execute_reply.started":"2025-03-18T12:03:44.191052Z","shell.execute_reply":"2025-03-18T12:03:44.315035Z"}},"outputs":[{"name":"stdout","text":"Text extracted from /kaggle/input/resume-dataset/data/data/ENGINEERING/10030015.pdf and saved to output.txt\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Code for prediction","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n# import torch\n# import numpy as np\n\n# # Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Define the path to the fine-tuned model checkpoint\n# model_checkpoint = \"/kaggle/input/bert_fintuned_resume/pytorch/v1/1\"\n\n# # Load the fine-tuned model and tokenizer\n# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# # Move the model to the appropriate device (GPU or CPU)\n# model.to(device)\n\n# # Function to preprocess and tokenize input text\n# def preprocess_and_tokenize(texts):\n#     return tokenizer(\n#         texts,\n#         padding=\"max_length\",\n#         truncation=True,\n#         max_length=512,\n#         return_tensors=\"pt\"\n#     )\n\n# # Function to make predictions\n# def predict(texts):\n#     # Preprocess and tokenize the input texts\n#     inputs = preprocess_and_tokenize(texts)\n    \n#     # Move inputs to the same device as the model\n#     inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n#     # Perform inference\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n    \n#     # Get predicted class probabilities\n#     logits = outputs.logits\n#     probabilities = torch.softmax(logits, dim=-1)\n    \n#     # Get predicted class labels\n#     predictions = torch.argmax(probabilities, dim=-1).cpu().numpy()\n    \n#     return predictions, probabilities.cpu().numpy()\n\n# # Example input texts for prediction\n# input_texts = [\n#     \"This is an example text for prediction.\",\n#     \"Another example text to classify.\",\n#     \"This is a third example.\"\n# ]\n\n# # Make predictions\n# predictions, probabilities = predict(input_texts)\n\n# # Print results\n# for i, (text, pred, prob) in enumerate(zip(input_texts, predictions, probabilities)):\n#     print(f\"Text {i + 1}: {text}\")\n#     print(f\"Predicted Label: {pred}\")\n#     print(f\"Class Probabilities: {prob}\")\n#     print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:03:57.572789Z","iopub.execute_input":"2025-03-18T12:03:57.573335Z","iopub.status.idle":"2025-03-18T12:03:57.579494Z","shell.execute_reply.started":"2025-03-18T12:03:57.573294Z","shell.execute_reply":"2025-03-18T12:03:57.577825Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define the path to the fine-tuned model checkpoint\nmodel_checkpoint = \"/kaggle/input/bert_fintuned_resume/pytorch/v1/1/checkpoint\"\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Move the model to the appropriate device (GPU or CPU)\nmodel.to(device)\n\n# Define the label mapping\nid_to_label = {\n    0: \"Education\",\n    1: \"Experience\",\n    2: \"Objectives\",\n    3: \"Personal Information\",\n    4: \"QC\",\n    5: \"Skill\",\n    6: \"Summary\"\n}\n\n# Function to preprocess and tokenize input text\ndef preprocess_and_tokenize(texts):\n    return tokenizer(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\"\n    )\n\n# Function to make predictions\ndef predict(texts):\n    # Preprocess and tokenize the input texts\n    inputs = preprocess_and_tokenize(texts)\n    \n    # Move inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Perform inference\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get predicted class probabilities\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=-1)\n    \n    # Get predicted class labels\n    predictions = torch.argmax(probabilities, dim=-1).cpu().numpy()\n    \n    return predictions, probabilities.cpu().numpy()\n\n# Load text data from a .txt file\ndef load_text_data(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        texts = file.readlines()\n    # Remove newline characters and strip whitespace\n    texts = [text.strip() for text in texts]\n    return texts\n\n# Path to the .txt file\ntxt_file_path = \"/kaggle/working/output.txt\"\n\n# Load and preprocess the text data\ntexts = load_text_data(txt_file_path)\n\n# Make predictions\npredictions, probabilities = predict(texts)\n\n# Decode predictions using id_to_label mapping\ndecoded_predictions = [id_to_label[pred] for pred in predictions]\n\n# Print results\nfor i, (text, pred, decoded_pred, prob) in enumerate(zip(texts, predictions, decoded_predictions, probabilities)):\n    print(f\"Text {i + 1}: {text}\")\n    print(f\"Predicted Label ID: {pred}\")\n    print(f\"Decoded Label: {decoded_pred}\")\n    print(f\"Class Probabilities: {prob}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:03:57.977695Z","iopub.execute_input":"2025-03-18T12:03:57.978186Z","iopub.status.idle":"2025-03-18T12:06:08.401624Z","shell.execute_reply.started":"2025-03-18T12:03:57.978139Z","shell.execute_reply":"2025-03-18T12:06:08.400260Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nText 1: ENGINEERING LAB TECHNICIAN\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.06409964 0.6951933  0.00362999 0.09245677 0.01657899 0.09101989\n 0.03702145]\n--------------------------------------------------\nText 2: Career Focus\nPredicted Label ID: 2\nDecoded Label: Objectives\nClass Probabilities: [0.01421985 0.15456252 0.35557592 0.1947772  0.00694555 0.03790122\n 0.23601773]\n--------------------------------------------------\nText 3: My main objective in seeking employment with Triumph Actuation Systems Inc. is to work in a professional atmosphere where I can utilize my\nPredicted Label ID: 2\nDecoded Label: Objectives\nClass Probabilities: [0.00118166 0.0143166  0.90888274 0.02801012 0.00164228 0.00335065\n 0.04261587]\n--------------------------------------------------\nText 4: skills and continue to gain experience in the aerospace industry to advance in my career.\nPredicted Label ID: 2\nDecoded Label: Objectives\nClass Probabilities: [0.0014273  0.01355015 0.90986437 0.02123657 0.00139211 0.00405468\n 0.04847481]\n--------------------------------------------------\nText 5: Professional Experience\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.0934738  0.55738163 0.01558715 0.04905323 0.0263366  0.11550678\n 0.1426608 ]\n--------------------------------------------------\nText 6: Engineering Lab Technician\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01544176 0.77602226 0.00133236 0.13438068 0.01328561 0.03380181\n 0.02573557]\n--------------------------------------------------\nText 7: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 8: Oct 2016\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [2.0080755e-02 9.5661408e-01 5.5560406e-04 1.3848808e-02 1.9638743e-03\n 4.5628557e-03 2.3741338e-03]\n--------------------------------------------------\nText 9: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 10: to\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01831359 0.9391257  0.00372914 0.01875566 0.00283845 0.0067814\n 0.01045601]\n--------------------------------------------------\nText 11: Current\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01632239 0.7170674  0.0049361  0.17115186 0.00334449 0.04831106\n 0.03886662]\n--------------------------------------------------\nText 12: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 13: Company Name\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01270692 0.95806444 0.00105046 0.00911112 0.00252981 0.00943148\n 0.00710581]\n--------------------------------------------------\nText 14: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 15: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 16: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 17: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 18: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 19: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 20: Responsible for testing various seat structures to meet specific certification requirements. Â\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [9.4253075e-04 9.8626673e-01 2.0173789e-04 6.1236380e-04 4.4116838e-04\n 1.8031772e-03 9.7322650e-03]\n--------------------------------------------------\nText 21: Maintain and calibrate test instruments to ensure testing capabilities are maintained.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [4.9567246e-04 9.4547319e-01 3.1034116e-04 4.9949315e-04 3.4255945e-04\n 2.1762270e-03 5.0702576e-02]\n--------------------------------------------------\nText 22: Ensure data is captured and recorded correctly for certification test reports.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [3.7847439e-04 9.8368222e-01 2.7118251e-04 4.3842394e-04 3.1172071e-04\n 6.9520168e-04 1.4222619e-02]\n--------------------------------------------------\nText 23: Duties also dynamic test set-up and static suite testing.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [1.1175583e-03 9.5091897e-01 2.0373301e-04 5.6736922e-04 4.5576206e-04\n 3.1417515e-03 4.3594740e-02]\n--------------------------------------------------\nText 24: Engineering Lab Technician, Sr. Specialist\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.00722746 0.60029334 0.00132761 0.3396655  0.00715418 0.02491567\n 0.01941615]\n--------------------------------------------------\nText 25: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 26: Apr 2012\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.3797556  0.55673236 0.00172724 0.04152395 0.01001775 0.00786421\n 0.00237897]\n--------------------------------------------------\nText 27: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 28: to\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01831359 0.9391257  0.00372914 0.01875566 0.00283845 0.0067814\n 0.01045601]\n--------------------------------------------------\nText 29: Oct 2016\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [2.0080755e-02 9.5661408e-01 5.5560406e-04 1.3848808e-02 1.9638743e-03\n 4.5628557e-03 2.3741338e-03]\n--------------------------------------------------\nText 30: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 31: Company Name\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01270692 0.95806444 0.00105046 0.00911112 0.00252981 0.00943148\n 0.00710581]\n--------------------------------------------------\nText 32: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 33: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 34: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 35: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 36: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 37: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 38: Utilized skills learned from LabView Course 1 training to construct and maintain LabView VI programs.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.6973740e-04 9.5166063e-01 3.1606152e-04 5.0404045e-04 6.3454657e-04\n 2.4797686e-03 4.3735262e-02]\n--------------------------------------------------\nText 39: Responsible for fabricating and maintaining hydraulic/electrical test equipment to complete development and qualification programs.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.3585595e-04 9.4031024e-01 2.7753942e-04 4.7039229e-04 3.5276060e-04\n 1.6829243e-03 5.6270279e-02]\n--------------------------------------------------\nText 40: Apply engineering principles to operate electrical, mechanical, and hydraulic systems to test pumps, motors, and actuators for aircraft.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.00103088 0.76715153 0.00119073 0.00111764 0.00253887 0.02773621\n 0.19923414]\n--------------------------------------------------\nText 41: Work closely with Design Engineers and Lab Management to investigate performance/design issues.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [1.1411625e-03 8.7928355e-01 4.6408104e-04 1.0487767e-03 4.8183309e-04\n 2.6393088e-03 1.1494125e-01]\n--------------------------------------------------\nText 42: Completed Lean Six Sigma/5s training seminar.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.0225099  0.5696341  0.00296098 0.01395948 0.19477816 0.05188289\n 0.1442744 ]\n--------------------------------------------------\nText 43: Assembly/ Test Technician\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.1221621e-03 9.1264683e-01 6.2763260e-04 4.9017038e-02 6.0737082e-03\n 1.1274198e-02 1.4238296e-02]\n--------------------------------------------------\nText 44: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 45: Mar 2007\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.3542325e-01 4.8799708e-02 5.8544736e-04 5.2310224e-03 7.0124432e-03\n 1.9228566e-03 1.0252302e-03]\n--------------------------------------------------\nText 46: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 47: to\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01831359 0.9391257  0.00372914 0.01875566 0.00283845 0.0067814\n 0.01045601]\n--------------------------------------------------\nText 48: Mar 2012\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.84433174 0.12603667 0.00097663 0.01580658 0.00770157 0.00385842\n 0.00128835]\n--------------------------------------------------\nText 49: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 50: Company Name\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01270692 0.95806444 0.00105046 0.00911112 0.00252981 0.00943148\n 0.00710581]\n--------------------------------------------------\nText 51: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 52: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 53: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 54: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 55: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 56: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 57: Lead technician overseeing three member crew responsible for maintaining high quality standards in a high volume production environment.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.7312719e-04 9.7423720e-01 2.8087918e-04 7.6517957e-04 3.6711508e-04\n 7.4395526e-04 2.2932477e-02]\n--------------------------------------------------\nText 58: Responsible for cargo door system actuators for Boeing 787 aircraft.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.4718636e-04 9.4872934e-01 2.3222987e-04 6.7588314e-04 4.4896995e-04\n 3.2645136e-03 4.6001889e-02]\n--------------------------------------------------\nText 59: Experience with hydraulic components that meet military and FAA regulations.\nPredicted Label ID: 6\nDecoded Label: Summary\nClass Probabilities: [6.8847393e-04 1.8482706e-01 5.7635509e-04 1.0038065e-03 1.0356243e-03\n 1.4624721e-02 7.9724401e-01]\n--------------------------------------------------\nText 60: Work closely with Engineering Group and Quality Assurance inspectors to resolve problems with development actuators.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.3795247e-04 9.2526489e-01 2.7796393e-04 5.6140090e-04 3.6279828e-04\n 1.7475239e-03 7.1147457e-02]\n--------------------------------------------------\nText 61: Munitions Systems Journeyman\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01407992 0.73135024 0.0011056  0.03202569 0.01804511 0.16257755\n 0.04081589]\n--------------------------------------------------\nText 62: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 63: Mar 2003\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.0915424e-01 7.1499057e-02 6.4689259e-04 7.4769221e-03 7.3555545e-03\n 2.7524414e-03 1.1150625e-03]\n--------------------------------------------------\nText 64: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 65: to\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01831359 0.9391257  0.00372914 0.01875566 0.00283845 0.0067814\n 0.01045601]\n--------------------------------------------------\nText 66: Mar 2007\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.3542325e-01 4.8799708e-02 5.8544736e-04 5.2310224e-03 7.0124432e-03\n 1.9228566e-03 1.0252302e-03]\n--------------------------------------------------\nText 67: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 68: Company Name\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.01270692 0.95806444 0.00105046 0.00911112 0.00252981 0.00943148\n 0.00710581]\n--------------------------------------------------\nText 69: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 70: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 71: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 72: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 73: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 74: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 75: Responsible for certifying small arms, actuation devices, bombs, rockets and other munitions items.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [5.4547878e-04 9.7164243e-01 2.8127685e-04 6.3642941e-04 3.0097333e-04\n 2.2434278e-03 2.4350146e-02]\n--------------------------------------------------\nText 76: Supervised work crews of 3-5 men while supporting USAF aircraft during Operation Enduring Freedom.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [1.9556070e-03 7.7658927e-01 7.5629272e-04 2.0302376e-03 8.5048901e-04\n 5.1987590e-03 2.1261942e-01]\n--------------------------------------------------\nText 77: Assembled rockets, bombs, and small arms for Operation Enduring Freedom.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.0029693  0.77247316 0.00143887 0.00359895 0.00180153 0.01379616\n 0.20392211]\n--------------------------------------------------\nText 78: Train newly assigned personnel at Line Delivery section on transporting, loading, and delivering munitions to aircraft.\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [6.9669809e-04 9.5401740e-01 2.5182034e-04 4.8688927e-04 3.6228704e-04\n 1.2660807e-03 4.2918850e-02]\n--------------------------------------------------\nText 79: Skills\nPredicted Label ID: 5\nDecoded Label: Skill\nClass Probabilities: [0.00473303 0.02536669 0.00168533 0.00959052 0.00629337 0.9470038\n 0.00532724]\n--------------------------------------------------\nText 80: DasyLab, Labview, Mechanical, Electrical, Lean/5S, Hydraulic Testing, Test Stand Fabrication\nPredicted Label ID: 5\nDecoded Label: Skill\nClass Probabilities: [0.00988755 0.4215174  0.00088757 0.01947702 0.01299766 0.5145322\n 0.02070055]\n--------------------------------------------------\nText 81: Education and Training\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.87896556 0.07565852 0.00166353 0.01063113 0.01701221 0.00806984\n 0.0079993 ]\n--------------------------------------------------\nText 82: Associates\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [4.6831463e-03 9.7082597e-01 4.5399941e-04 1.0329881e-02 1.2475475e-03\n 6.7814430e-03 5.6780707e-03]\n--------------------------------------------------\nText 83: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 84: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 85: Applied Science Electronics Engineering\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.87461686 0.04712218 0.00112184 0.05271745 0.01339082 0.00607993\n 0.00495076]\n--------------------------------------------------\nText 86: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 87: 2011\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.5976806e-01 2.6374549e-02 6.6982256e-04 4.0330836e-03 6.4075245e-03\n 1.3748972e-03 1.3721582e-03]\n--------------------------------------------------\nText 88: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 89: Forsyth Technical Community College\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [8.9816153e-01 6.6237502e-02 8.5111300e-04 6.8339827e-03 1.8626291e-02\n 4.3340288e-03 4.9554589e-03]\n--------------------------------------------------\nText 90: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 91: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 92: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 93: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 94: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 95: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 96: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 97: Applied Science Electronics\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.81314737 0.08556431 0.00192986 0.0493523  0.01922277 0.02118663\n 0.00959679]\n--------------------------------------------------\nText 98: Engineering\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.91060126 0.04131812 0.00168272 0.01788379 0.0157173  0.00835895\n 0.00443794]\n--------------------------------------------------\nText 99: Associates\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [4.6831463e-03 9.7082597e-01 4.5399941e-04 1.0329881e-02 1.2475475e-03\n 6.7814430e-03 5.6780707e-03]\n--------------------------------------------------\nText 100: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 101: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 102: Applied Science, Munitions Systems Technology\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.7610506  0.10153338 0.00285356 0.0433486  0.04401355 0.02882166\n 0.01837868]\n--------------------------------------------------\nText 103: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 104: 2007\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.6149784e-01 2.5137365e-02 6.4133049e-04 2.8065071e-03 6.9516553e-03\n 1.3355013e-03 1.6298662e-03]\n--------------------------------------------------\nText 105: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 106: Community College of the Air Force\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [8.9768010e-01 6.0743313e-02 7.3763408e-04 7.5671435e-03 2.4063921e-02\n 4.3344172e-03 4.8734206e-03]\n--------------------------------------------------\nText 107: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 108: ï¼​\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.37288913 0.08616045 0.00058748 0.5311037  0.00605998 0.00197412\n 0.00122514]\n--------------------------------------------------\nText 109: City\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.02726869 0.6359367  0.00240805 0.2889312  0.00374669 0.03033928\n 0.01136949]\n--------------------------------------------------\nText 110: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 111: ,\nPredicted Label ID: 3\nDecoded Label: Personal Information\nClass Probabilities: [0.19829933 0.24213411 0.02047474 0.44263127 0.00855331 0.06262012\n 0.02528715]\n--------------------------------------------------\nText 112: State\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [9.7534639e-01 1.0595319e-02 3.8036625e-04 4.4588204e-03 7.0439503e-03\n 1.1862409e-03 9.8885328e-04]\n--------------------------------------------------\nText 113: \nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.15948449 0.47373638 0.02319561 0.10478382 0.0218796  0.17723185\n 0.0396883 ]\n--------------------------------------------------\nText 114: Applied Science,\nPredicted Label ID: 0\nDecoded Label: Education\nClass Probabilities: [0.86954856 0.0547187  0.0030485  0.03764591 0.01643408 0.01117079\n 0.00743344]\n--------------------------------------------------\nText 115: Munitions Systems Technology\nPredicted Label ID: 1\nDecoded Label: Experience\nClass Probabilities: [0.06228716 0.6133794  0.00273286 0.05915123 0.02161038 0.17822395\n 0.06261491]\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Sample row: {'Label': 'Exp', 'Text': 'Name: Abiral Pandey'}\nUnique labels found: ['Edu', 'Exp', 'Obj', 'PI', 'QC', 'Skill', 'Sum']\nLabel mapping: {'Edu': 0, 'Exp': 1, 'Obj': 2, 'PI': 3, 'QC': 4, 'Skill': 5, 'Sum': 6}\nNumber of labels: 7\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}